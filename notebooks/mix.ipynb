{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8e55322f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# core\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# data + plotting\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "from scipy.stats import zscore\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "# sklearn (preprocessing / pipeline / model selection / metrics)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures, PowerTransformer,RobustScaler\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# classical models (if you use them elsewhere)\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# gradient boosting / lightgbm / xgboost\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# utilities\n",
    "import joblib   # optional: save/load pipeline\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "067243b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hospital_Id</th>\n",
       "      <th>Supplier_Name</th>\n",
       "      <th>Supplier_Reliability</th>\n",
       "      <th>Equipment_Height</th>\n",
       "      <th>Equipment_Width</th>\n",
       "      <th>Equipment_Weight</th>\n",
       "      <th>Equipment_Type</th>\n",
       "      <th>Equipment_Value</th>\n",
       "      <th>Base_Transport_Fee</th>\n",
       "      <th>CrossBorder_Shipping</th>\n",
       "      <th>Urgent_Shipping</th>\n",
       "      <th>Installation_Service</th>\n",
       "      <th>Transport_Method</th>\n",
       "      <th>Fragile_Equipment</th>\n",
       "      <th>Hospital_Info</th>\n",
       "      <th>Rural_Hospital</th>\n",
       "      <th>Order_Placed_Date</th>\n",
       "      <th>Delivery_Date</th>\n",
       "      <th>Hospital_Location</th>\n",
       "      <th>Transport_Cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fffe3200360030003700</td>\n",
       "      <td>Jo Valencia</td>\n",
       "      <td>0.44</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.62</td>\n",
       "      <td>17.13</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Roadways</td>\n",
       "      <td>No</td>\n",
       "      <td>Working Class</td>\n",
       "      <td>No</td>\n",
       "      <td>10/20/17</td>\n",
       "      <td>10/20/17</td>\n",
       "      <td>APO AA 33776</td>\n",
       "      <td>179.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fffe3400380037003400</td>\n",
       "      <td>Wanda Warren</td>\n",
       "      <td>0.58</td>\n",
       "      <td>29.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1210684.0</td>\n",
       "      <td>Marble</td>\n",
       "      <td>9703.37</td>\n",
       "      <td>35.42</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Roadways</td>\n",
       "      <td>No</td>\n",
       "      <td>Working Class</td>\n",
       "      <td>No</td>\n",
       "      <td>02/22/16</td>\n",
       "      <td>02/24/16</td>\n",
       "      <td>South Kevin, VT 84493</td>\n",
       "      <td>627732.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fffe3200350036003700</td>\n",
       "      <td>Robert Ackies</td>\n",
       "      <td>0.97</td>\n",
       "      <td>39.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3305.0</td>\n",
       "      <td>Aluminium</td>\n",
       "      <td>40.21</td>\n",
       "      <td>18.54</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Roadways</td>\n",
       "      <td>No</td>\n",
       "      <td>Working Class</td>\n",
       "      <td>No</td>\n",
       "      <td>01/11/18</td>\n",
       "      <td>01/10/18</td>\n",
       "      <td>Kevinshire, NE 31279</td>\n",
       "      <td>1565.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fffe3800320034003400</td>\n",
       "      <td>Charlotte Membreno</td>\n",
       "      <td>0.70</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>606.0</td>\n",
       "      <td>Brass</td>\n",
       "      <td>4.55</td>\n",
       "      <td>17.48</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Roadways</td>\n",
       "      <td>No</td>\n",
       "      <td>Working Class</td>\n",
       "      <td>No</td>\n",
       "      <td>08/06/16</td>\n",
       "      <td>08/06/16</td>\n",
       "      <td>DPO AP 61572</td>\n",
       "      <td>257.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fffe3600340033003000</td>\n",
       "      <td>Nena Silva</td>\n",
       "      <td>0.66</td>\n",
       "      <td>27.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Marble</td>\n",
       "      <td>2726.80</td>\n",
       "      <td>30.23</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Roadways</td>\n",
       "      <td>No</td>\n",
       "      <td>Working Class</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12/15/16</td>\n",
       "      <td>12/17/16</td>\n",
       "      <td>Joshuamouth, AK 01550</td>\n",
       "      <td>8553.52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Hospital_Id       Supplier_Name  Supplier_Reliability  \\\n",
       "0  fffe3200360030003700         Jo Valencia                  0.44   \n",
       "1  fffe3400380037003400        Wanda Warren                  0.58   \n",
       "2  fffe3200350036003700       Robert Ackies                  0.97   \n",
       "3  fffe3800320034003400  Charlotte Membreno                  0.70   \n",
       "4  fffe3600340033003000          Nena Silva                  0.66   \n",
       "\n",
       "   Equipment_Height  Equipment_Width  Equipment_Weight Equipment_Type  \\\n",
       "0              21.0              6.0               NaN            NaN   \n",
       "1              29.0             20.0         1210684.0         Marble   \n",
       "2              39.0             15.0            3305.0      Aluminium   \n",
       "3               8.0              5.0             606.0          Brass   \n",
       "4              27.0             13.0               NaN         Marble   \n",
       "\n",
       "   Equipment_Value  Base_Transport_Fee CrossBorder_Shipping Urgent_Shipping  \\\n",
       "0             3.62               17.13                   No              No   \n",
       "1          9703.37               35.42                   No             Yes   \n",
       "2            40.21               18.54                   No              No   \n",
       "3             4.55               17.48                   No              No   \n",
       "4          2726.80               30.23                  Yes              No   \n",
       "\n",
       "  Installation_Service Transport_Method Fragile_Equipment  Hospital_Info  \\\n",
       "0                   No         Roadways                No  Working Class   \n",
       "1                  Yes         Roadways                No  Working Class   \n",
       "2                   No         Roadways                No  Working Class   \n",
       "3                   No         Roadways                No  Working Class   \n",
       "4                   No         Roadways                No  Working Class   \n",
       "\n",
       "  Rural_Hospital Order_Placed_Date Delivery_Date      Hospital_Location  \\\n",
       "0             No          10/20/17      10/20/17           APO AA 33776   \n",
       "1             No          02/22/16      02/24/16  South Kevin, VT 84493   \n",
       "2             No          01/11/18      01/10/18   Kevinshire, NE 31279   \n",
       "3             No          08/06/16      08/06/16           DPO AP 61572   \n",
       "4            NaN          12/15/16      12/17/16  Joshuamouth, AK 01550   \n",
       "\n",
       "   Transport_Cost  \n",
       "0          179.50  \n",
       "1       627732.45  \n",
       "2         1565.92  \n",
       "3          257.71  \n",
       "4         8553.52  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial data shape: (5000, 20)\n",
      "Cleaning string columns...\n",
      "Normalizing Yes/No columns...\n",
      "Converting date columns...\n",
      "Engineering Delivery_Days feature...\n",
      "Engineering more date features...\n",
      "Dropping duplicates...\n",
      "Dropped 0 duplicate rows.\n",
      "\n",
      "==============================\n",
      " CLEANING & FEATURE ENGINEERING COMPLETE \n",
      "==============================\n",
      "After basic cleaning shape: (5000, 24)\n",
      "\n",
      "Missing values (raw count):\n",
      "Hospital_Id                0\n",
      "Supplier_Name              0\n",
      "Supplier_Reliability     587\n",
      "Equipment_Height         283\n",
      "Equipment_Width          443\n",
      "Equipment_Weight         460\n",
      "Equipment_Type           599\n",
      "Equipment_Value            0\n",
      "Base_Transport_Fee         0\n",
      "CrossBorder_Shipping       0\n",
      "Urgent_Shipping            0\n",
      "Installation_Service       0\n",
      "Transport_Method        1071\n",
      "Fragile_Equipment          0\n",
      "Hospital_Info              0\n",
      "Rural_Hospital           586\n",
      "Order_Placed_Date          0\n",
      "Delivery_Date              0\n",
      "Hospital_Location          0\n",
      "Transport_Cost             0\n",
      "Delivery_Days              0\n",
      "Order_Month                0\n",
      "Order_Day_of_Week          0\n",
      "Order_Is_Weekend           0\n",
      "dtype: int64\n",
      "\n",
      "Missing values (percentage):\n",
      "Transport_Method        21.42\n",
      "Equipment_Type          11.98\n",
      "Supplier_Reliability    11.74\n",
      "Rural_Hospital          11.72\n",
      "Equipment_Weight         9.20\n",
      "Equipment_Width          8.86\n",
      "Equipment_Height         5.66\n",
      "dtype: float64\n",
      "\n",
      "DataFrame head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hospital_Id</th>\n",
       "      <th>Supplier_Name</th>\n",
       "      <th>Supplier_Reliability</th>\n",
       "      <th>Equipment_Height</th>\n",
       "      <th>Equipment_Width</th>\n",
       "      <th>Equipment_Weight</th>\n",
       "      <th>Equipment_Type</th>\n",
       "      <th>Equipment_Value</th>\n",
       "      <th>Base_Transport_Fee</th>\n",
       "      <th>CrossBorder_Shipping</th>\n",
       "      <th>...</th>\n",
       "      <th>Hospital_Info</th>\n",
       "      <th>Rural_Hospital</th>\n",
       "      <th>Order_Placed_Date</th>\n",
       "      <th>Delivery_Date</th>\n",
       "      <th>Hospital_Location</th>\n",
       "      <th>Transport_Cost</th>\n",
       "      <th>Delivery_Days</th>\n",
       "      <th>Order_Month</th>\n",
       "      <th>Order_Day_of_Week</th>\n",
       "      <th>Order_Is_Weekend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fffe3200360030003700</td>\n",
       "      <td>Jo Valencia</td>\n",
       "      <td>0.44</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.62</td>\n",
       "      <td>17.13</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>Working Class</td>\n",
       "      <td>No</td>\n",
       "      <td>2017-10-20</td>\n",
       "      <td>2017-10-20</td>\n",
       "      <td>APO AA 33776</td>\n",
       "      <td>179.50</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fffe3400380037003400</td>\n",
       "      <td>Wanda Warren</td>\n",
       "      <td>0.58</td>\n",
       "      <td>29.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1210684.0</td>\n",
       "      <td>Marble</td>\n",
       "      <td>9703.37</td>\n",
       "      <td>35.42</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>Working Class</td>\n",
       "      <td>No</td>\n",
       "      <td>2016-02-22</td>\n",
       "      <td>2016-02-24</td>\n",
       "      <td>South Kevin, VT 84493</td>\n",
       "      <td>627732.45</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fffe3200350036003700</td>\n",
       "      <td>Robert Ackies</td>\n",
       "      <td>0.97</td>\n",
       "      <td>39.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3305.0</td>\n",
       "      <td>Aluminium</td>\n",
       "      <td>40.21</td>\n",
       "      <td>18.54</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>Working Class</td>\n",
       "      <td>No</td>\n",
       "      <td>2018-01-11</td>\n",
       "      <td>2018-01-10</td>\n",
       "      <td>Kevinshire, NE 31279</td>\n",
       "      <td>1565.92</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fffe3800320034003400</td>\n",
       "      <td>Charlotte Membreno</td>\n",
       "      <td>0.70</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>606.0</td>\n",
       "      <td>Brass</td>\n",
       "      <td>4.55</td>\n",
       "      <td>17.48</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>Working Class</td>\n",
       "      <td>No</td>\n",
       "      <td>2016-08-06</td>\n",
       "      <td>2016-08-06</td>\n",
       "      <td>DPO AP 61572</td>\n",
       "      <td>257.71</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fffe3600340033003000</td>\n",
       "      <td>Nena Silva</td>\n",
       "      <td>0.66</td>\n",
       "      <td>27.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Marble</td>\n",
       "      <td>2726.80</td>\n",
       "      <td>30.23</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>Working Class</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-12-15</td>\n",
       "      <td>2016-12-17</td>\n",
       "      <td>Joshuamouth, AK 01550</td>\n",
       "      <td>8553.52</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Hospital_Id       Supplier_Name  Supplier_Reliability  \\\n",
       "0  fffe3200360030003700         Jo Valencia                  0.44   \n",
       "1  fffe3400380037003400        Wanda Warren                  0.58   \n",
       "2  fffe3200350036003700       Robert Ackies                  0.97   \n",
       "3  fffe3800320034003400  Charlotte Membreno                  0.70   \n",
       "4  fffe3600340033003000          Nena Silva                  0.66   \n",
       "\n",
       "   Equipment_Height  Equipment_Width  Equipment_Weight Equipment_Type  \\\n",
       "0              21.0              6.0               NaN            NaN   \n",
       "1              29.0             20.0         1210684.0         Marble   \n",
       "2              39.0             15.0            3305.0      Aluminium   \n",
       "3               8.0              5.0             606.0          Brass   \n",
       "4              27.0             13.0               NaN         Marble   \n",
       "\n",
       "   Equipment_Value  Base_Transport_Fee CrossBorder_Shipping  ...  \\\n",
       "0             3.62               17.13                   No  ...   \n",
       "1          9703.37               35.42                   No  ...   \n",
       "2            40.21               18.54                   No  ...   \n",
       "3             4.55               17.48                   No  ...   \n",
       "4          2726.80               30.23                  Yes  ...   \n",
       "\n",
       "   Hospital_Info Rural_Hospital Order_Placed_Date Delivery_Date  \\\n",
       "0  Working Class             No        2017-10-20    2017-10-20   \n",
       "1  Working Class             No        2016-02-22    2016-02-24   \n",
       "2  Working Class             No        2018-01-11    2018-01-10   \n",
       "3  Working Class             No        2016-08-06    2016-08-06   \n",
       "4  Working Class            NaN        2016-12-15    2016-12-17   \n",
       "\n",
       "       Hospital_Location Transport_Cost Delivery_Days Order_Month  \\\n",
       "0           APO AA 33776         179.50             0          10   \n",
       "1  South Kevin, VT 84493      627732.45             2           2   \n",
       "2   Kevinshire, NE 31279        1565.92            -1           1   \n",
       "3           DPO AP 61572         257.71             0           8   \n",
       "4  Joshuamouth, AK 01550        8553.52             2          12   \n",
       "\n",
       "  Order_Day_of_Week  Order_Is_Weekend  \n",
       "0                 4             False  \n",
       "1                 0             False  \n",
       "2                 3             False  \n",
       "3                 5              True  \n",
       "4                 3             False  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set plot style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "\n",
    "# 1️⃣ Load Data\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv('../data/train.csv')\n",
    "df.columns = df.columns.str.strip()\n",
    "display(df.head())\n",
    "print(f\"Initial data shape: {df.shape}\")\n",
    "\n",
    "# 2️⃣ Clean all string/object columns: strip spaces, replace blanks with NaN\n",
    "print(\"Cleaning string columns...\")\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    df[col] = df[col].astype(str).str.strip()\n",
    "    df[col] = df[col].replace({'': np.nan, 'nan': np.nan, 'NaN': np.nan})\n",
    "\n",
    "# 3️⃣ Normalize Yes/No columns to consistent \"Yes\"/\"No\"\n",
    "print(\"Normalizing Yes/No columns...\")\n",
    "yes_no_cols = ['CrossBorder_Shipping', 'Urgent_Shipping', 'Installation_Service',\n",
    "               'Fragile_Equipment', 'Rural_Hospital']\n",
    "for col in yes_no_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace({\n",
    "            'YES': 'Yes', 'yes': 'Yes', 'Y': 'Yes', 'y': 'Yes',\n",
    "            'NO': 'No', 'no': 'No', 'N': 'No', 'n': 'No'\n",
    "        })\n",
    "\n",
    "# 4️⃣ Convert date columns to datetime\n",
    "print(\"Converting date columns...\")\n",
    "df['Order_Placed_Date'] = pd.to_datetime(df['Order_Placed_Date'], errors='coerce')\n",
    "df['Delivery_Date'] = pd.to_datetime(df['Delivery_Date'], errors='coerce')\n",
    "\n",
    "# 5️⃣ Create new feature: Delivery_Days (difference in days)\n",
    "print(\"Engineering Delivery_Days feature...\")\n",
    "df['Delivery_Days'] = (df['Delivery_Date'] - df['Order_Placed_Date']).dt.days\n",
    "df['Delivery_Days'] = pd.to_numeric(df['Delivery_Days'], errors='coerce')\n",
    "\n",
    "# === ADDED: Date Feature Engineering ===\n",
    "print(\"Engineering more date features...\")\n",
    "df['Order_Month'] = df['Order_Placed_Date'].dt.month\n",
    "df['Order_Day_of_Week'] = df['Order_Placed_Date'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "df['Order_Is_Weekend'] = df['Order_Day_of_Week'].isin([5, 6])\n",
    "# === END ADDED ===\n",
    "\n",
    "# 6️⃣ (Original) delete initial date rows\n",
    "# df = df.dropna(subset=['Order_Placed_Date', 'Delivery_Date'])\n",
    "\n",
    "# 7️⃣ Drop exact duplicate rows\n",
    "print(\"Dropping duplicates...\")\n",
    "before = len(df)\n",
    "df = df.drop_duplicates()\n",
    "after = len(df)\n",
    "print(f\"Dropped {before - after} duplicate rows.\")\n",
    "\n",
    "# 8️⃣ Quick check after cleaning\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\" CLEANING & FEATURE ENGINEERING COMPLETE \")\n",
    "print(\"=\"*30)\n",
    "print(f\"After basic cleaning shape: {df.shape}\")\n",
    "\n",
    "print(\"\\nMissing values (raw count):\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "# === ADDED: Missing Value Percentage View ===\n",
    "print(\"\\nMissing values (percentage):\")\n",
    "missing_pct = (df.isna().sum() / len(df) * 100).sort_values(ascending=False)\n",
    "print(missing_pct[missing_pct > 0])\n",
    "# === END ADDED ===\n",
    "\n",
    "print(\"\\nDataFrame head:\")\n",
    "display(df.head())\n",
    "# print(df['Delivery_Days'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8560ee97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing script started...\n",
      "5000\n",
      "Filtered 0 rows with negative cost.\n",
      "Created 'Is_Negative_Delivery' flag.\n",
      "Log-transformed skewed numeric features.\n",
      "Applied log1p to target variable 'Transport_Cost'.\n",
      "Total features for modeling: 17\n",
      "\n",
      "Calculating baseline...\n",
      "Baseline RMSE (predicting mean cost): $48,764.21\n",
      "Training set shape: (4000, 17)\n",
      "Test set shape: (1000, 17)\n",
      "\n",
      "Fitting preprocessor on X_train...\n",
      "Preprocessing complete.\n",
      "Processed X_train shape: (4000, 49)\n",
      "Processed X_test shape: (1000, 49)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Preprocessing script started...\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: PRE-SPLIT (Final Cleaning & Feature Engineering)\n",
    "# ==============================================================================\n",
    "\n",
    "# 1. Filter Bad Data\n",
    "initial_rows = len(df)\n",
    "print(initial_rows)\n",
    "# df = df[df['Transport_Cost'] >= 0] instead of this we will make negative as value to 1\n",
    "df['Transport_Cost'] = np.where(df['Transport_Cost'] < 0, 1, df['Transport_Cost'])\n",
    "print(f\"Filtered {initial_rows - len(df)} rows with negative cost.\")\n",
    "\n",
    "# 2. Equipment Feature Engineering\n",
    "df['Equipment_Volume'] = df['Equipment_Height'] * df['Equipment_Width']\n",
    "\n",
    "# 3. Delivery_Days Feature Engineering (THE FIX)\n",
    "# (!!! THIS IS THE FIX !!!)\n",
    "# Create the flag column *before* modifying Delivery_Days\n",
    "print(\"Created 'Is_Negative_Delivery' flag.\")\n",
    "\n",
    "# Now, modify Delivery_Days (your suggestion)\n",
    "df['Delivery_Days'] = df['Delivery_Days'].abs()\n",
    "# print(\"Set negative Delivery_Days to -1.\")\n",
    "# no change to Delivery_Days as per latest discussion\n",
    "\n",
    "# 4. Log-Transform Skewed *Features*\n",
    "df['Equipment_Value'] = np.log1p(df['Equipment_Value'])\n",
    "df['Equipment_Volume'] = np.log1p(df['Equipment_Volume'])\n",
    "df['Equipment_Weight'] = np.log1p(df['Equipment_Weight'])\n",
    "df['Base_Transport_Fee'] = np.log1p(df['Base_Transport_Fee'])\n",
    "print(\"Log-transformed skewed numeric features.\")\n",
    "\n",
    "# 5. Define Target (y) and Features (X)\n",
    "y_original = df['Transport_Cost'] # Keep original for baseline\n",
    "y = np.log1p(y_original)         # This is the target we will train on\n",
    "print(\"Applied log1p to target variable 'Transport_Cost'.\")\n",
    "\n",
    "# Define X by dropping the target and all redundant/ID/replaced columns.\n",
    "X = df.drop(columns=[\n",
    "    'Transport_Cost',       # Target\n",
    "    'Equipment_Height',     # Replaced by Volume\n",
    "    'Equipment_Width',      # Replaced by Volume\n",
    "    'Hospital_Id',          # ID\n",
    "    'Supplier_Name',        # ID\n",
    "    'Hospital_Location',    # ID\n",
    "    'Order_Placed_Date',    # Replaced by date features\n",
    "    'Delivery_Date'         # Replaced by date features\n",
    "])\n",
    "\n",
    "print(f\"Total features for modeling: {len(X.columns)}\")\n",
    "\n",
    "# 6. Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 7. Calculate Baseline RMSE (in DOLLARS)\n",
    "print(\"\\nCalculating baseline...\")\n",
    "y_train_original_mean = y_original.loc[y_train.index].mean()\n",
    "y_test_original = y_original.loc[y_test.index]\n",
    "\n",
    "y_test_pred_baseline = np.full_like(y_test_original, y_train_original_mean)\n",
    "baseline_rmse = np.sqrt(mean_squared_error(y_test_original, y_test_pred_baseline))\n",
    "print(f\"Baseline RMSE (predicting mean cost): ${baseline_rmse:,.2f}\")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: POST-SPLIT (Pipelines & ColumnTransformer)\n",
    "# ==============================================================================\n",
    "\n",
    "# 1. Define Feature Lists\n",
    "numeric_features = [\n",
    "    'Supplier_Reliability',\n",
    "    'Equipment_Value',      # Log-transformed\n",
    "    'Base_Transport_Fee',   # Log-transformed\n",
    "    'Delivery_Days',        # Clipped at -1\n",
    "    'Equipment_Volume',     # Log-transformed\n",
    "    'Equipment_Weight'      # Log-transformed\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'Equipment_Type',\n",
    "    'CrossBorder_Shipping',\n",
    "    'Urgent_Shipping',\n",
    "    'Installation_Service',\n",
    "    'Transport_Method',\n",
    "    'Fragile_Equipment',\n",
    "    'Hospital_Info',\n",
    "    'Rural_Hospital',\n",
    "    'Order_Month',\n",
    "    'Order_Day_of_Week',\n",
    "    'Order_Is_Weekend'\n",
    "]\n",
    "\n",
    "# 2. Create the Numeric Pipeline\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "# 3. Create the Categorical Pipeline\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# 4. Create the Full Preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop' \n",
    ")\n",
    "\n",
    "# 5. Apply the Preprocessor\n",
    "print(\"\\nFitting preprocessor on X_train...\")\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test) \n",
    "\n",
    "print(\"Preprocessing complete.\")\n",
    "print(f\"Processed X_train shape: {X_train_processed.shape}\")\n",
    "print(f\"Processed X_test shape: {X_test_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2581b9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 1️⃣ Create full pipeline\n",
    "# ==============================\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),        # handle numeric + categorical\n",
    "    ('poly', PolynomialFeatures()),        # now safe\n",
    "    ('regressor', Lasso(max_iter=10000, random_state=42))\n",
    "])\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Define hyperparameter grid\n",
    "# ==============================\n",
    "param_grid = {\n",
    "    'poly__degree': [1,2],             # polynomial degree\n",
    "    'regressor__alpha': [0.01, 0.1, 1, 10]  # Lasso regularization\n",
    "}\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Setup GridSearchCV with K-Fold\n",
    "# ==============================\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=kf,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Fit GridSearch on training data\n",
    "# ==============================\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Best parameters and CV RMSE\n",
    "# ==============================\n",
    "best_params = grid_search.best_params_\n",
    "best_rmse = np.sqrt(-grid_search.best_score_)\n",
    "\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(f\"Best CV RMSE: {best_rmse:.4f}\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Evaluate on test set\n",
    "# ==============================\n",
    "y_test_pred = grid_search.predict(X_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "print(f\"Test RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Feature importance: Top 30 by absolute coefficient\n",
    "# ==============================\n",
    "best_model = grid_search.best_estimator_.named_steps['regressor']\n",
    "preprocessor = grid_search.best_estimator_.named_steps['preprocessor']\n",
    "poly = grid_search.best_estimator_.named_steps['poly']\n",
    "\n",
    "# numeric + categorical feature names\n",
    "num_features = numeric_features\n",
    "cat_features = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_features)\n",
    "all_features = np.concatenate([num_features, cat_features])\n",
    "\n",
    "# polynomial feature names\n",
    "poly_feature_names = poly.get_feature_names_out(all_features)\n",
    "\n",
    "# match coefficients to feature names\n",
    "coef = best_model.coef_\n",
    "feature_coef_df = pd.DataFrame({\n",
    "    'feature': poly_feature_names,\n",
    "    'coefficient': coef\n",
    "})\n",
    "\n",
    "# Top 30 features by absolute value\n",
    "top_30 = feature_coef_df.reindex(feature_coef_df['coefficient'].abs().sort_values(ascending=False).index).head(30)\n",
    "display(top_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf9f3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Get the BEST Lasso Alpha (for the selector)\n",
    "# ==============================\n",
    "try:\n",
    "    # We get the best alpha from your *first* grid search\n",
    "    best_lasso_alpha = grid_search.best_params_['regressor__alpha']\n",
    "    print(f\"--- Using fixed Lasso Alpha for selector: {best_lasso_alpha} ---\")\n",
    "except NameError:\n",
    "    print(\"ERROR: 'grid_search' object (from Lasso) not found.\")\n",
    "    raise\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Build the FULL Hybrid Pipeline\n",
    "# ==============================\n",
    "# This pipeline will be tuned by GridSearchCV\n",
    "\n",
    "hybrid_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor), # Your original, fitted preprocessor\n",
    "    \n",
    "    ('poly', PolynomialFeatures(include_bias=False)), # This is now a TUNABLE step\n",
    "    \n",
    "    ('selector', SelectFromModel(\n",
    "        Lasso(alpha=best_lasso_alpha, max_iter=10000, random_state=42),\n",
    "        # threshold=1e-5, <-- Removed! This will now be set by the grid search.\n",
    "        prefit=False  # This re-fits Lasso for each 'degree'\n",
    "    )),\n",
    "    \n",
    "    ('regressor', Ridge(random_state=42)) # This is also a TUNABLE step\n",
    "])\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Define the NEW Hyperparameter Grid\n",
    "# ==============================\n",
    "# We will tune poly__degree, regressor__alpha, AND the selector__threshold\n",
    "\n",
    "hybrid_param_grid = {\n",
    "    'poly__degree': [1, 2, 3],  # Test different degrees\n",
    "    'selector__threshold': ['mean', 'median', '1.25*mean'], # <-- DYNAMIC THRESHOLD\n",
    "    'regressor__alpha': [0.1, 1, 10, 100] # Test different Ridge alphas\n",
    "}\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Setup and Run the new GridSearchCV\n",
    "# ==============================\n",
    "# We re-use 'kf' from your previous cell\n",
    "# kf = KFold(n_splits=5, shuffle=True, random_state=42) \n",
    "\n",
    "hybrid_grid_search = GridSearchCV(\n",
    "    hybrid_pipeline,\n",
    "    hybrid_param_grid,\n",
    "    cv=kf,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(\"\\n--- Fitting Hybrid Model (Tuning Degree, Threshold, AND Ridge Alpha) ---\")\n",
    "hybrid_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Best parameters and CV RMSE\n",
    "# ==============================\n",
    "hybrid_best_params = hybrid_grid_search.best_params_\n",
    "hybrid_best_rmse = np.sqrt(-hybrid_grid_search.best_score_)\n",
    "\n",
    "print(\"\\n--- New Hybrid Model Results ---\")\n",
    "print(f\"Best parameters: {hybrid_best_params}\")\n",
    "print(f\"Best CV RMSE: {hybrid_best_rmse:.4f}\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Evaluate on test set\n",
    "# ==============================\n",
    "y_test_pred_hybrid = hybrid_grid_search.predict(X_test)\n",
    "test_rmse_hybrid = np.sqrt(mean_squared_error(y_test, y_test_pred_hybrid))\n",
    "\n",
    "print(f\"\\n--- Final Model Comparison (Test Set) ---\")\n",
    "try:\n",
    "    print(f\"Original Lasso RMSE:      {test_rmse:.4f}\")\n",
    "except NameError:\n",
    "    print(\"Original Lasso RMSE:      (variable 'test_rmse' not found)\")\n",
    "    \n",
    "try:\n",
    "    print(f\"Ridge (w/ Lasso FS) RMSE: {test_rmse_ridge_fs:.4f} (Old)\")\n",
    "except NameError:\n",
    "    print(\"Ridge (w/ Lasso FS) RMSE: (variable 'test_rmse_ridge_fs' not found)\")\n",
    "\n",
    "print(f\"New Hybrid RMSE:          {test_rmse_hybrid:.4f} (New)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b49bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, RobustScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "# ===================================================================\n",
    "# 🚀 Retraining final Polynomial Lasso model on ALL available data...\n",
    "# ===================================================================\n",
    "\n",
    "# === 1. Get Best Hyperparameters (DYNAMICALLY) ===\n",
    "try:\n",
    "    # Get the best params from the 'grid_search' object (from your previous cell)\n",
    "    best_params_lasso = grid_search.best_params_\n",
    "    best_degree_lasso = best_params_lasso['poly__degree']\n",
    "    best_alpha_lasso = best_params_lasso['regressor__alpha']\n",
    "    print(f\"   Using best hyperparameters: {best_params_lasso}\")\n",
    "\n",
    "except (NameError, AttributeError, NotFittedError) as e:\n",
    "    print(\"\\nERROR: Could not run. 'grid_search' object not found or not fitted.\")\n",
    "    print(\"Please make sure you have run the 'Polynomial Lasso GridSearchCV' cell successfully first.\")\n",
    "    print(f\"Details: {e}\")\n",
    "    # Stop the script if the params aren't found\n",
    "    raise\n",
    "\n",
    "# === 2. Re-define the Preprocessor (for clarity and safety) ===\n",
    "# Using the same feature lists from your reference \"V4\" script\n",
    "numeric_features = [\n",
    "    'Supplier_Reliability', 'Equipment_Value', 'Base_Transport_Fee',\n",
    "    'Delivery_Days', 'Equipment_Volume', 'Equipment_Weight'\n",
    "]\n",
    "categorical_features = [\n",
    "    'Equipment_Type', 'CrossBorder_Shipping', 'Urgent_Shipping',\n",
    "    'Installation_Service', 'Transport_Method', 'Fragile_Equipment',\n",
    "    'Hospital_Info', 'Rural_Hospital', 'Order_Month',\n",
    "    'Order_Day_of_Week', 'Order_Is_Weekend'\n",
    "]\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# This is the final preprocessor that will be used in the pipeline\n",
    "final_preprocessor_lasso = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop' \n",
    ")\n",
    "\n",
    "# === 3. Build the Final Model and Pipeline ===\n",
    "# Create the final pipeline with the 'poly' step and best params\n",
    "final_model_pipeline_lasso = Pipeline(steps=[\n",
    "    ('preprocessor', final_preprocessor_lasso),\n",
    "    ('poly', PolynomialFeatures(degree=best_degree_lasso, include_bias=False)), # <-- Uses best_degree\n",
    "    ('model', Lasso(alpha=best_alpha_lasso, max_iter=10000, random_state=42))    # <-- Uses best_alpha\n",
    "])\n",
    "\n",
    "# === 4. Fit on the ENTIRE dataset ===\n",
    "# We use 'X' and 'y' (the full, pre-split data) to train the final model\n",
    "print(f\"\\nFitting pipeline on {X.shape[0]} rows of data...\")\n",
    "final_model_pipeline_lasso.fit(X, y)\n",
    "\n",
    "print(\"\\n✅ Final Polynomial Lasso model trained on entire dataset!\")\n",
    "print(\"You can now use this 'final_model_pipeline_lasso' for all predictions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ce6d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, RobustScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "# =====================================================================\n",
    "# 🚀 Retraining final Hybrid (Lasso-Select + Ridge) model on ALL data...\n",
    "# =====================================================================\n",
    "\n",
    "# === 1. Get Best Hyperparameters (DYNAMICALLY) ===\n",
    "\n",
    "# --- 1a. Get params from the NEW Hybrid grid search ---\n",
    "try:\n",
    "    best_params_hybrid = hybrid_grid_search.best_params_\n",
    "    best_degree_hybrid = best_params_hybrid['poly__degree']\n",
    "    best_alpha_ridge = best_params_hybrid['regressor__alpha']\n",
    "    best_threshold_hybrid = best_params_hybrid['selector__threshold']\n",
    "    \n",
    "    print(\"   Using best HYBRID hyperparameters:\")\n",
    "    print(f\"   - {best_params_hybrid}\")\n",
    "\n",
    "except (NameError, AttributeError, NotFittedError) as e:\n",
    "    print(\"\\nERROR: Could not run. 'hybrid_grid_search' object not found or not fitted.\")\n",
    "    print(\"Please make sure you have run the 'Hybrid GridSearchCV' cell successfully first.\")\n",
    "    print(f\"Details: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- 1b. Get the SELECTOR'S alpha from the ORIGINAL Lasso grid search ---\n",
    "try:\n",
    "    # This is the fixed alpha used *inside* the SelectFromModel step\n",
    "    best_lasso_alpha = grid_search.best_params_['regressor__alpha']\n",
    "    print(f\"\\n   Using best SELECTOR alpha from original Lasso grid search:\")\n",
    "    print(f\"   - Lasso Alpha for Selector: {best_lasso_alpha}\")\n",
    "except (NameError, AttributeError, NotFittedError) as e:\n",
    "    print(\"\\nERROR: Could not run. 'grid_search' (original Lasso) object not found or not fitted.\")\n",
    "    print(\"This is needed for the selector's 'alpha' parameter.\")\n",
    "    print(f\"Details: {e}\")\n",
    "    raise\n",
    "\n",
    "# === 2. Re-define the Preprocessor (for clarity and safety) ===\n",
    "# Using the same feature lists from your reference \"V4\" script\n",
    "numeric_features = [\n",
    "    'Supplier_Reliability', 'Equipment_Value', 'Base_Transport_Fee',\n",
    "    'Delivery_Days', 'Equipment_Volume', 'Equipment_Weight'\n",
    "]\n",
    "categorical_features = [\n",
    "    'Equipment_Type', 'CrossBorder_Shipping', 'Urgent_Shipping',\n",
    "    'Installation_Service', 'Transport_Method', 'Fragile_Equipment',\n",
    "    'Hospital_Info', 'Rural_Hospital', 'Order_Month',\n",
    "    'Order_Day_of_Week', 'Order_Is_Weekend'\n",
    "]\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# This is the final preprocessor that will be used in the pipeline\n",
    "final_preprocessor_hybrid = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop' \n",
    ")\n",
    "\n",
    "# === 3. Build the Final Model and Pipeline ===\n",
    "# Create the final pipeline with ALL the best parameters\n",
    "final_model_pipeline_hybrid = Pipeline(steps=[\n",
    "    ('preprocessor', final_preprocessor_hybrid),\n",
    "    \n",
    "    ('poly', PolynomialFeatures(degree=best_degree_hybrid, include_bias=False)), # <-- Uses best_degree\n",
    "    \n",
    "    ('selector', SelectFromModel(\n",
    "        Lasso(alpha=best_lasso_alpha, max_iter=10000, random_state=42),\n",
    "        threshold=best_threshold_hybrid, # <-- Uses best_threshold\n",
    "        prefit=False\n",
    "    )),\n",
    "    \n",
    "    ('regressor', Ridge(alpha=best_alpha_ridge, random_state=42)) # <-- Uses best_alpha (for Ridge)\n",
    "])\n",
    "\n",
    "# === 4. Fit on the ENTIRE dataset ===\n",
    "# We use 'X' and 'y' (the full, pre-split data) to train the final model\n",
    "print(f\"\\nFitting hybrid pipeline on {X.shape[0]} rows of data...\")\n",
    "final_model_pipeline_hybrid.fit(X, y)\n",
    "\n",
    "print(\"\\n✅ Final Hybrid (Lasso-Select + Ridge) model trained on entire dataset!\")\n",
    "print(\"You can now use this 'final_model_pipeline_hybrid' for all predictions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beddc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Create the Random Forest Pipeline\n",
    "# ==============================\n",
    "# We use the *exact same* 'preprocessor' you already have.\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor), # Using your existing object\n",
    "    ('regressor', RandomForestRegressor(random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Define a WIDE Hyperparameter Grid\n",
    "# ==============================\n",
    "# This grid tests many combinations to fight overfitting\n",
    "rf_param_grid = {\n",
    "    'regressor__n_estimators': [100, 300, 500],\n",
    "    'regressor__max_depth': [10, 20, None],  # Test limited-depth vs. fully-grown trees\n",
    "    'regressor__max_features': ['sqrt', 1.0],  # 'sqrt' is classic RF, '1.0' is all features (like Bagging)\n",
    "    'regressor__min_samples_split': [2, 5]   # Default (2) vs. slight regularization (5)\n",
    "}\n",
    "# Total fits = 3 (n_est) * 3 (depth) * 2 (max_feat) * 2 (min_split) = 36 combinations\n",
    "# 36 combinations * 5 folds (cv) = 180 fits.\n",
    "# This will take some time to run.\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Setup and Run GridSearchCV\n",
    "# ==============================\n",
    "# Assumes 'kf' is still in memory\n",
    "rf_grid_search = GridSearchCV(\n",
    "    rf_pipeline,\n",
    "    rf_param_grid,\n",
    "    cv=kf,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1, # Use all your CPU cores\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(\"\\n--- 🌳 Fitting Random Forest Model (Wide Grid Search) 🌳 ---\")\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Best parameters and CV RMSE\n",
    "# ==============================\n",
    "rf_best_params = rf_grid_search.best_params_\n",
    "rf_best_rmse = np.sqrt(-rf_grid_search.best_score_)\n",
    "\n",
    "print(\"\\n--- Random Forest Results ---\")\n",
    "print(f\"Best parameters: {rf_best_params}\")\n",
    "print(f\"Best CV RMSE (RF): {rf_best_rmse:.4f}\")\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Evaluate on Test Set\n",
    "# ==============================\n",
    "y_test_pred_rf = rf_grid_search.predict(X_test)\n",
    "test_rmse_rf = np.sqrt(mean_squared_error(y_test, y_test_pred_rf))\n",
    "\n",
    "print(f\"\\n--- 📊 Final Model Comparison (Test Set) ---\")\n",
    "try:\n",
    "    print(f\"Lasso RMSE:             {test_rmse:.4f}\")\n",
    "    print(f\"Hybrid Ridge RMSE:      {test_rmse_ridge_fs:.4f}\")\n",
    "    print(f\"XGBoost RMSE:           {test_rmse_xgb:.4f}\")\n",
    "except NameError:\n",
    "    print(\"Could not find previous model scores for comparison.\")\n",
    "\n",
    "print(f\"Random Forest RMSE:     {test_rmse_rf:.4f} (New)\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Feature Importance from Random Forest\n",
    "# ==============================\n",
    "try:\n",
    "    # Get names after OHE\n",
    "    best_preprocessor = rf_grid_search.best_estimator_.named_steps['preprocessor']\n",
    "    ohe = best_preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
    "    cat_fnames = ohe.get_feature_names_out(categorical_features)\n",
    "    all_feature_names = np.concatenate([numeric_features, cat_fnames])\n",
    "\n",
    "    # Get importances from the best model\n",
    "    best_rf_model = rf_grid_search.best_estimator_.named_steps['regressor']\n",
    "    importances = best_rf_model.feature_importances_\n",
    "\n",
    "    rf_feature_df = pd.DataFrame({\n",
    "        'Feature': all_feature_names,\n",
    "        'Importance': importances\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    print(f\"\\n--- Random Forest Feature Importances (All {len(rf_feature_df)} Features) ---\")\n",
    "    \n",
    "    # Display all features\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "        display(rf_feature_df)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nCould not generate feature importances: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86831bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "# ===================================================================\n",
    "# 🌳 Retraining final Random Forest model on ALL available data...\n",
    "# ===================================================================\n",
    "\n",
    "# === 1. Get Best Hyperparameters (DYNAMICALLY) ===\n",
    "try:\n",
    "    # Get the best params from the 'rf_grid_search' object (from your previous cell)\n",
    "    best_params_rf = rf_grid_search.best_params_\n",
    "    \n",
    "    # Extract each best parameter\n",
    "    best_n_estimators = best_params_rf['regressor__n_estimators']\n",
    "    best_max_depth = best_params_rf['regressor__max_depth']\n",
    "    best_max_features = best_params_rf['regressor__max_features']\n",
    "    best_min_samples_split = best_params_rf['regressor__min_samples_split']\n",
    "    \n",
    "    print(f\"   Using best hyperparameters: {best_params_rf}\")\n",
    "\n",
    "except (NameError, AttributeError, NotFittedError) as e:\n",
    "    print(\"\\nERROR: Could not run. 'rf_grid_search' object not found or not fitted.\")\n",
    "    print(\"Please make sure you have run the 'Random Forest GridSearchCV' cell successfully first.\")\n",
    "    print(f\"Details: {e}\")\n",
    "    # Stop the script if the params aren't found\n",
    "    raise\n",
    "\n",
    "# === 2. Re-define the Preprocessor (for clarity and safety) ===\n",
    "# Using the same feature lists from your Lasso retraining template\n",
    "numeric_features = [\n",
    "    'Supplier_Reliability', 'Equipment_Value', 'Base_Transport_Fee',\n",
    "    'Delivery_Days', 'Equipment_Volume', 'Equipment_Weight'\n",
    "]\n",
    "categorical_features = [\n",
    "    'Equipment_Type', 'CrossBorder_Shipping', 'Urgent_Shipping',\n",
    "    'Installation_Service', 'Transport_Method', 'Fragile_Equipment',\n",
    "    'Hospital_Info', 'Rural_Hospital', 'Order_Month',\n",
    "    'Order_Day_of_Week', 'Order_Is_Weekend'\n",
    "]\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# This is the final preprocessor that will be used in the pipeline\n",
    "final_preprocessor_rf = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop' \n",
    ")\n",
    "\n",
    "# === 3. Build the Final Model and Pipeline ===\n",
    "# Create the final pipeline with the best params\n",
    "# Note: There is no 'poly' step for the Random Forest\n",
    "final_model_pipeline_rf = Pipeline(steps=[\n",
    "    ('preprocessor', final_preprocessor_rf),\n",
    "    ('model', RandomForestRegressor(\n",
    "        n_estimators=best_n_estimators,      # <-- Uses best_n_estimators\n",
    "        max_depth=best_max_depth,          # <-- Uses best_max_depth\n",
    "        max_features=best_max_features,      # <-- Uses best_max_features\n",
    "        min_samples_split=best_min_samples_split, # <-- Uses best_min_samples_split\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# === 4. Fit on the ENTIRE dataset ===\n",
    "# We use 'X' and 'y' (the full, pre-split data) to train the final model\n",
    "print(f\"\\nFitting pipeline on {X.shape[0]} rows of data...\")\n",
    "final_model_pipeline_rf.fit(X, y)\n",
    "\n",
    "print(\"\\n✅ Final Random Forest model trained on entire dataset!\")\n",
    "print(\"You can now use this 'final_model_pipeline_rf' for all predictions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9ad279b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 🌳 Fitting XGBoost Model (with RobustScaler) 🌳 ---\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=3, regressor__n_estimators=100; total time=   0.0s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=3, regressor__n_estimators=100; total time=   0.0s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=3, regressor__n_estimators=100; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=3, regressor__n_estimators=100; total time=   0.0s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=3, regressor__n_estimators=100; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=3, regressor__n_estimators=300; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=3, regressor__n_estimators=300; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=3, regressor__n_estimators=300; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=3, regressor__n_estimators=300; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=3, regressor__n_estimators=300; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=3, regressor__n_estimators=500; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=3, regressor__n_estimators=500; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=3, regressor__n_estimators=500; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=5, regressor__n_estimators=100; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=5, regressor__n_estimators=100; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=3, regressor__n_estimators=500; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=5, regressor__n_estimators=100; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=5, regressor__n_estimators=100; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=5, regressor__n_estimators=100; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=3, regressor__n_estimators=500; total time=   0.2s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=5, regressor__n_estimators=300; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=7, regressor__n_estimators=100; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=5, regressor__n_estimators=300; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=7, regressor__n_estimators=100; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=5, regressor__n_estimators=300; total time=   0.2s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=5, regressor__n_estimators=300; total time=   0.2s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=5, regressor__n_estimators=500; total time=   0.2s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=5, regressor__n_estimators=300; total time=   0.2s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=7, regressor__n_estimators=100; total time=   0.2s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=7, regressor__n_estimators=100; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=7, regressor__n_estimators=100; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=3, regressor__n_estimators=100; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=3, regressor__n_estimators=100; total time=   0.0s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=5, regressor__n_estimators=500; total time=   0.3s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=3, regressor__n_estimators=100; total time=   0.0s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=3, regressor__n_estimators=100; total time=   0.0s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=3, regressor__n_estimators=300; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=5, regressor__n_estimators=500; total time=   0.2s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=3, regressor__n_estimators=300; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=3, regressor__n_estimators=300; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=3, regressor__n_estimators=300; total time=   0.2s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=5, regressor__n_estimators=500; total time=   0.3s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=7, regressor__n_estimators=300; total time=   0.4s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=7, regressor__n_estimators=300; total time=   0.4s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=3, regressor__n_estimators=500; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=3, regressor__n_estimators=500; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=7, regressor__n_estimators=500; total time=   0.4s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=3, regressor__n_estimators=500; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=7, regressor__n_estimators=500; total time=   0.4s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=5, regressor__n_estimators=100; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=5, regressor__n_estimators=100; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=3, regressor__n_estimators=300; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=3, regressor__n_estimators=500; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=3, regressor__n_estimators=500; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=5, regressor__n_estimators=100; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=7, regressor__n_estimators=300; total time=   0.4s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=5, regressor__n_estimators=500; total time=   0.3s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=5, regressor__n_estimators=100; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=5, regressor__n_estimators=100; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=5, regressor__n_estimators=300; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=5, regressor__n_estimators=300; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=7, regressor__n_estimators=300; total time=   0.4s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=5, regressor__n_estimators=300; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=7, regressor__n_estimators=500; total time=   0.7s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=5, regressor__n_estimators=500; total time=   0.2s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=7, regressor__n_estimators=300; total time=   0.4s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=5, regressor__n_estimators=300; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=3, regressor__n_estimators=100; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=5, regressor__n_estimators=500; total time=   0.3s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=5, regressor__n_estimators=300; total time=   0.2s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=5, regressor__n_estimators=500; total time=   0.3s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=5, regressor__n_estimators=500; total time=   0.3s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=7, regressor__n_estimators=100; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=7, regressor__n_estimators=500; total time=   0.4s\n",
      "[CV] END regressor__learning_rate=0.01, regressor__max_depth=7, regressor__n_estimators=500; total time=   0.5s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=7, regressor__n_estimators=100; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=7, regressor__n_estimators=100; total time=   0.1s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=7, regressor__n_estimators=100; total time=   0.2s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=7, regressor__n_estimators=100; total time=   0.2s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=7, regressor__n_estimators=300; total time=   0.3s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=7, regressor__n_estimators=300; total time=   0.4s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=5, regressor__n_estimators=500; total time=   0.4s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=7, regressor__n_estimators=300; total time=   0.4s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=7, regressor__n_estimators=300; total time=   0.4s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=7, regressor__n_estimators=300; total time=   0.4s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=7, regressor__n_estimators=500; total time=   0.4s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=7, regressor__n_estimators=500; total time=   0.4s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=7, regressor__n_estimators=500; total time=   0.5s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=7, regressor__n_estimators=500; total time=   0.4s\n",
      "[CV] END regressor__learning_rate=0.1, regressor__max_depth=7, regressor__n_estimators=500; total time=   0.5s\n",
      "\n",
      "--- XGBoost Results ---\n",
      "Best parameters: {'regressor__learning_rate': 0.01, 'regressor__max_depth': 3, 'regressor__n_estimators': 500}\n",
      "Best CV RMSE (XGBoost): 1.8944\n",
      "\n",
      "--- 📊 Final Model Comparison (Test Set) ---\n",
      "Lasso RMSE:             1.7858 (Original)\n",
      "Could not find previous model scores ('test_rmse' or 'test_rmse_ridge_fs') for comparison.\n",
      "XGBoost RMSE:           1.7728 (New)\n",
      "\n",
      "--- XGBoost Feature Importances (All 49 Features) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Equipment_Value</td>\n",
       "      <td>0.298215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Base_Transport_Fee</td>\n",
       "      <td>0.074989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Order_Day_of_Week_3</td>\n",
       "      <td>0.048988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Rural_Hospital_No</td>\n",
       "      <td>0.047939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Supplier_Reliability</td>\n",
       "      <td>0.043991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Transport_Method_Waterways</td>\n",
       "      <td>0.036947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Equipment_Type_Aluminium</td>\n",
       "      <td>0.034886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Delivery_Days</td>\n",
       "      <td>0.028688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Equipment_Volume</td>\n",
       "      <td>0.027506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Order_Day_of_Week_1</td>\n",
       "      <td>0.026078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Equipment_Type_Stone</td>\n",
       "      <td>0.025822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Order_Day_of_Week_0</td>\n",
       "      <td>0.024956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Urgent_Shipping_No</td>\n",
       "      <td>0.024469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Equipment_Weight</td>\n",
       "      <td>0.024421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Order_Month_3</td>\n",
       "      <td>0.023931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Order_Month_7</td>\n",
       "      <td>0.023068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Transport_Method_Airways</td>\n",
       "      <td>0.018748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Order_Month_5</td>\n",
       "      <td>0.018407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Order_Month_9</td>\n",
       "      <td>0.017602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Order_Day_of_Week_2</td>\n",
       "      <td>0.017546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Installation_Service_No</td>\n",
       "      <td>0.015094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Order_Month_8</td>\n",
       "      <td>0.014005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Hospital_Info_Wealthy</td>\n",
       "      <td>0.013233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Order_Is_Weekend_False</td>\n",
       "      <td>0.013227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Order_Day_of_Week_5</td>\n",
       "      <td>0.011573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CrossBorder_Shipping_No</td>\n",
       "      <td>0.010790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Order_Month_11</td>\n",
       "      <td>0.009670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Order_Month_1</td>\n",
       "      <td>0.009049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Transport_Method_Roadways</td>\n",
       "      <td>0.008185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Order_Month_4</td>\n",
       "      <td>0.007980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Fragile_Equipment_Yes</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Order_Day_of_Week_6</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Order_Day_of_Week_4</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Equipment_Type_Brass</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Equipment_Type_Bronze</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Equipment_Type_Clay</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Equipment_Type_Marble</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Order_Month_12</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Equipment_Type_Wood</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Order_Month_10</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Fragile_Equipment_No</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CrossBorder_Shipping_Yes</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Order_Month_6</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Urgent_Shipping_Yes</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Installation_Service_Yes</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Order_Month_2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Rural_Hospital_Yes</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Hospital_Info_Working Class</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Order_Is_Weekend_True</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Feature  Importance\n",
       "1               Equipment_Value    0.298215\n",
       "2            Base_Transport_Fee    0.074989\n",
       "43          Order_Day_of_Week_3    0.048988\n",
       "26            Rural_Hospital_No    0.047939\n",
       "0          Supplier_Reliability    0.043991\n",
       "21   Transport_Method_Waterways    0.036947\n",
       "6      Equipment_Type_Aluminium    0.034886\n",
       "3                 Delivery_Days    0.028688\n",
       "4              Equipment_Volume    0.027506\n",
       "41          Order_Day_of_Week_1    0.026078\n",
       "11         Equipment_Type_Stone    0.025822\n",
       "40          Order_Day_of_Week_0    0.024956\n",
       "15           Urgent_Shipping_No    0.024469\n",
       "5              Equipment_Weight    0.024421\n",
       "30                Order_Month_3    0.023931\n",
       "34                Order_Month_7    0.023068\n",
       "19     Transport_Method_Airways    0.018748\n",
       "32                Order_Month_5    0.018407\n",
       "36                Order_Month_9    0.017602\n",
       "42          Order_Day_of_Week_2    0.017546\n",
       "17      Installation_Service_No    0.015094\n",
       "35                Order_Month_8    0.014005\n",
       "24        Hospital_Info_Wealthy    0.013233\n",
       "47       Order_Is_Weekend_False    0.013227\n",
       "45          Order_Day_of_Week_5    0.011573\n",
       "13      CrossBorder_Shipping_No    0.010790\n",
       "38               Order_Month_11    0.009670\n",
       "28                Order_Month_1    0.009049\n",
       "20    Transport_Method_Roadways    0.008185\n",
       "31                Order_Month_4    0.007980\n",
       "23        Fragile_Equipment_Yes    0.000000\n",
       "46          Order_Day_of_Week_6    0.000000\n",
       "44          Order_Day_of_Week_4    0.000000\n",
       "7          Equipment_Type_Brass    0.000000\n",
       "8         Equipment_Type_Bronze    0.000000\n",
       "9           Equipment_Type_Clay    0.000000\n",
       "10        Equipment_Type_Marble    0.000000\n",
       "39               Order_Month_12    0.000000\n",
       "12          Equipment_Type_Wood    0.000000\n",
       "37               Order_Month_10    0.000000\n",
       "22         Fragile_Equipment_No    0.000000\n",
       "14     CrossBorder_Shipping_Yes    0.000000\n",
       "33                Order_Month_6    0.000000\n",
       "16          Urgent_Shipping_Yes    0.000000\n",
       "18     Installation_Service_Yes    0.000000\n",
       "29                Order_Month_2    0.000000\n",
       "27           Rural_Hospital_Yes    0.000000\n",
       "25  Hospital_Info_Working Class    0.000000\n",
       "48        Order_Is_Weekend_True    0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Create the XGBoost Pipeline\n",
    "# ==============================\n",
    "# We will use your *existing* 'preprocessor' variable,\n",
    "# which has the RobustScaler included. This is perfectly fine.\n",
    "# Assumes 'preprocessor' is in memory from your linear model setup.\n",
    "try:\n",
    "    xgb_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor), # <-- Using your existing object\n",
    "        ('regressor', XGBRegressor(random_state=42, n_jobs=-1,\n",
    "                                 objective='reg:squarederror'))\n",
    "    ])\n",
    "except NameError:\n",
    "    print(\"ERROR: 'preprocessor' object not found.\")\n",
    "    print(\"Please run the preprocessing cell (section 5) from your linear model script first.\")\n",
    "    raise\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Define Hyperparameter Grid for XGBoost\n",
    "# ==============================\n",
    "xgb_param_grid = {\n",
    "    'regressor__n_estimators': [100, 300, 500],\n",
    "    'regressor__learning_rate': [0.01, 0.1],\n",
    "    'regressor__max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Setup and Run GridSearchCV\n",
    "# ==============================\n",
    "# Assumes 'kf' (your KFold object) is still in memory\n",
    "try:\n",
    "    xgb_grid_search = GridSearchCV(\n",
    "        xgb_pipeline,\n",
    "        xgb_param_grid,\n",
    "        cv=kf,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "except NameError:\n",
    "    print(\"ERROR: 'kf' object (KFold) not found.\")\n",
    "    print(\"Please run the cell that defines 'kf' first.\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n--- 🌳 Fitting XGBoost Model (with RobustScaler) 🌳 ---\")\n",
    "# Assumes 'X_train' and 'y_train' are still in memory\n",
    "try:\n",
    "    xgb_grid_search.fit(X_train, y_train)\n",
    "except NameError:\n",
    "    print(\"ERROR: 'X_train' or 'y_train' not found.\")\n",
    "    print(\"Please run your Train-Test Split cell first.\")\n",
    "    raise\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Best parameters and CV RMSE\n",
    "# ==============================\n",
    "xgb_best_params = xgb_grid_search.best_params_\n",
    "xgb_best_rmse = np.sqrt(-xgb_grid_search.best_score_)\n",
    "\n",
    "print(\"\\n--- XGBoost Results ---\")\n",
    "print(f\"Best parameters: {xgb_best_params}\")\n",
    "print(f\"Best CV RMSE (XGBoost): {xgb_best_rmse:.4f}\")\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Evaluate on Test Set\n",
    "# ==============================\n",
    "# Assumes 'X_test' and 'y_test' are still in memory\n",
    "y_test_pred_xgb = xgb_grid_search.predict(X_test)\n",
    "test_rmse_xgb = np.sqrt(mean_squared_error(y_test, y_test_pred_xgb))\n",
    "\n",
    "print(f\"\\n--- 📊 Final Model Comparison (Test Set) ---\")\n",
    "# Assumes 'test_rmse' (Lasso) and 'test_rmse_ridge_fs' are in memory\n",
    "try:\n",
    "    print(f\"Lasso RMSE:             {test_rmse:.4f} (Original)\")\n",
    "    print(f\"Hybrid Ridge RMSE:      {test_rmse_ridge_fs:.4f} (Your Best Linear)\")\n",
    "except NameError:\n",
    "    print(\"Could not find previous model scores ('test_rmse' or 'test_rmse_ridge_fs') for comparison.\")\n",
    "\n",
    "print(f\"XGBoost RMSE:           {test_rmse_xgb:.4f} (New)\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Feature Importance from XGBoost (All Features)\n",
    "# ==============================\n",
    "try:\n",
    "    # Get names after OHE\n",
    "    best_preprocessor = xgb_grid_search.best_estimator_.named_steps['preprocessor']\n",
    "    ohe = best_preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
    "    cat_fnames = ohe.get_feature_names_out(categorical_features)\n",
    "    # Assumes 'numeric_features' list is in memory\n",
    "    all_feature_names = np.concatenate([numeric_features, cat_fnames])\n",
    "\n",
    "    # Get importances from the best model\n",
    "    best_xgb_model = xgb_grid_search.best_estimator_.named_steps['regressor']\n",
    "    importances = best_xgb_model.feature_importances_\n",
    "\n",
    "    xgb_feature_df = pd.DataFrame({\n",
    "        'Feature': all_feature_names,\n",
    "        'Importance': importances\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    print(f\"\\n--- XGBoost Feature Importances (All {len(xgb_feature_df)} Features) ---\")\n",
    "    \n",
    "    # Temporarily set pandas to display all rows\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "        display(xgb_feature_df)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nCould not generate feature importances: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "94e56ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Using best hyperparameters: {'regressor__learning_rate': 0.01, 'regressor__max_depth': 3, 'regressor__n_estimators': 500}\n",
      "\n",
      "Fitting pipeline on 5000 rows of data...\n",
      "\n",
      "✅ Final XGBoost model trained on entire dataset!\n",
      "You can now use this 'final_model_pipeline_xgb' for all predictions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "# ===================================================================\n",
    "# 🌳 Retraining final XGBoost model on ALL available data...\n",
    "# ===================================================================\n",
    "\n",
    "# === 1. Get Best Hyperparameters (DYNAMICALLY) ===\n",
    "try:\n",
    "    # Get the best params from the 'xgb_grid_search' object (from your previous cell)\n",
    "    best_params_xgb = xgb_grid_search.best_params_\n",
    "    \n",
    "    # Extract each best parameter\n",
    "    best_n_estimators = best_params_xgb['regressor__n_estimators']\n",
    "    best_learning_rate = best_params_xgb['regressor__learning_rate']\n",
    "    best_max_depth = best_params_xgb['regressor__max_depth']\n",
    "    \n",
    "    print(f\"   Using best hyperparameters: {best_params_xgb}\")\n",
    "\n",
    "except (NameError, AttributeError, NotFittedError) as e:\n",
    "    print(\"\\nERROR: Could not run. 'xgb_grid_search' object not found or not fitted.\")\n",
    "    print(\"Please make sure you have run the 'XGBoost GridSearchCV' cell successfully first.\")\n",
    "    print(f\"Details: {e}\")\n",
    "    # Stop the script if the params aren't found\n",
    "    raise\n",
    "\n",
    "# === 2. Re-define the Preprocessor (for clarity and safety) ===\n",
    "# Using the same feature lists from your other retraining cells\n",
    "numeric_features = [\n",
    "    'Supplier_Reliability', 'Equipment_Value', 'Base_Transport_Fee',\n",
    "    'Delivery_Days', 'Equipment_Volume', 'Equipment_Weight'\n",
    "]\n",
    "categorical_features = [\n",
    "    'Equipment_Type', 'CrossBorder_Shipping', 'Urgent_Shipping',\n",
    "    'Installation_Service', 'Transport_Method', 'Fragile_Equipment',\n",
    "    'Hospital_Info', 'Rural_Hospital', 'Order_Month',\n",
    "    'Order_Day_of_Week', 'Order_Is_Weekend'\n",
    "]\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# This is the final preprocessor that will be used in the pipeline\n",
    "final_preprocessor_xgb = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop' \n",
    ")\n",
    "\n",
    "# === 3. Build the Final Model and Pipeline ===\n",
    "# Create the final pipeline with the best params\n",
    "final_model_pipeline_xgb = Pipeline(steps=[\n",
    "    ('preprocessor', final_preprocessor_xgb),\n",
    "    ('model', XGBRegressor(\n",
    "        n_estimators=best_n_estimators,      # <-- Uses best_n_estimators\n",
    "        learning_rate=best_learning_rate,  # <-- Uses best_learning_rate\n",
    "        max_depth=best_max_depth,          # <-- Uses best_max_depth\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        objective='reg:squarederror'\n",
    "    ))\n",
    "])\n",
    "\n",
    "# === 4. Fit on the ENTIRE dataset ===\n",
    "# We use 'X' and 'y' (the full, pre-split data) to train the final model\n",
    "print(f\"\\nFitting pipeline on {X.shape[0]} rows of data...\")\n",
    "final_model_pipeline_xgb.fit(X, y)\n",
    "\n",
    "print(\"\\n✅ Final XGBoost model trained on entire dataset!\")\n",
    "print(\"You can now use this 'final_model_pipeline_xgb' for all predictions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9daf372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(df_raw):\n",
    "    \"\"\"\n",
    "    Applies all manual cleaning and feature engineering\n",
    "    to match the data used for model training.\n",
    "    \n",
    "    Takes a raw DataFrame (like test.csv) and returns\n",
    "    a DataFrame ready for the preprocessor pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Preparing {len(df_raw)} new rows...\")\n",
    "    \n",
    "    # Make a copy to avoid changing the original data\n",
    "    df = df_raw.copy()\n",
    "    \n",
    "    # 1. Clean column names (from your training script)\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    # 2. Clean all string/object columns (from your training script)\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "        df[col] = df[col].replace({'': np.nan, 'nan': np.nan, 'NaN': np.nan})\n",
    "\n",
    "    # 3. Normalize Yes/No columns (from your training script)\n",
    "    yes_no_cols = ['CrossBorder_Shipping', 'Urgent_Shipping', 'Installation_Service',\n",
    "                   'Fragile_Equipment', 'Rural_Hospital']\n",
    "    for col in yes_no_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].replace({\n",
    "                'YES': 'Yes', 'yes': 'Yes', 'Y': 'Yes', 'y': 'Yes',\n",
    "                'NO': 'No', 'no': 'No', 'N': 'No', 'n': 'No'\n",
    "            })\n",
    "\n",
    "    # 4. Convert date columns (from your training script)\n",
    "    df['Order_Placed_Date'] = pd.to_datetime(df['Order_Placed_Date'], errors='coerce')\n",
    "    df['Delivery_Date'] = pd.to_datetime(df['Delivery_Date'], errors='coerce')\n",
    "\n",
    "    # 5. Engineer Date Features (from your training script)\n",
    "    df['Delivery_Days'] = (df['Delivery_Date'] - df['Order_Placed_Date']).dt.days\n",
    "    df['Delivery_Days'] = pd.to_numeric(df['Delivery_Days'], errors='coerce')\n",
    "    \n",
    "    df['Order_Month'] = df['Order_Placed_Date'].dt.month.astype(str)\n",
    "    df['Order_Day_of_Week'] = df['Order_Placed_Date'].dt.dayofweek.astype(str)\n",
    "    df['Order_Is_Weekend'] = (df['Order_Placed_Date'].dt.dayofweek >= 5).astype(int)\n",
    "    \n",
    "    # 6. Engineer Equipment_Volume (from your training script)\n",
    "    df['Equipment_Volume'] = df['Equipment_Height'] * df['Equipment_Width']\n",
    "    \n",
    "    # 7. Engineer Delivery_Days features (from your training script)\n",
    "    # This *must* be in this order\n",
    "    df['Delivery_Days'] = df['Delivery_Days'].abs()\n",
    "    # 8. Log-Transform Skewed Features (from your training script)\n",
    "    df['Equipment_Value'] = np.log1p(df['Equipment_Value'])\n",
    "    df['Equipment_Volume'] = np.log1p(df['Equipment_Volume'])\n",
    "    df['Equipment_Weight'] = np.log1p(df['Equipment_Weight'])\n",
    "    df['Base_Transport_Fee'] = np.log1p(df['Base_Transport_Fee'])\n",
    "    \n",
    "    print(\"Feature preparation complete.\")\n",
    "    \n",
    "    # 9. Return the feature-engineered DataFrame\n",
    "    # The preprocessor pipeline will select the columns it needs from this.\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "24661e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading new test data...\n",
      "Applying feature engineering to new data...\n",
      "Preparing 500 new rows...\n",
      "Feature preparation complete.\n",
      "Getting predictions from the final model...\n",
      "\n",
      "Final Predictions (in $):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hospital_Id</th>\n",
       "      <th>Transport_Cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fffe33003400</td>\n",
       "      <td>278.333984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fffe3700330036003600</td>\n",
       "      <td>192.789917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fffe3300390038003400</td>\n",
       "      <td>1107.906128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fffe310030003900</td>\n",
       "      <td>131.184555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fffe3700330031003200</td>\n",
       "      <td>644.596252</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Hospital_Id  Transport_Cost\n",
       "0          fffe33003400      278.333984\n",
       "1  fffe3700330036003600      192.789917\n",
       "2  fffe3300390038003400     1107.906128\n",
       "3      fffe310030003900      131.184555\n",
       "4  fffe3700330031003200      644.596252"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file 'mixed_hybrid_ridge.csv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Assume 'final_model_pipeline' and 'prepare_features' are in memory\n",
    "# If not, you would load your model:\n",
    "# import joblib\n",
    "# final_model_pipeline = joblib.load('final_xgb_model.pkl') \n",
    "\n",
    "# 1. Load your new, raw test data\n",
    "print(\"Loading new test data...\")\n",
    "df_new_test = pd.read_csv('../data/test.csv') \n",
    "\n",
    "# 2. Save IDs for the final submission\n",
    "# We need to map our predictions back to the original IDs\n",
    "submission_ids = df_new_test['Hospital_Id']\n",
    "\n",
    "# 3. Apply the *exact same* feature engineering\n",
    "print(\"Applying feature engineering to new data...\")\n",
    "X_new_prepared = prepare_features(df_new_test)\n",
    "\n",
    "# 4. Get predictions\n",
    "print(\"Getting predictions from the final model...\")\n",
    "log_predictions = final_model_pipeline_xgb.predict(X_new_prepared)\n",
    "\n",
    "# 5. Convert predictions back from log-scale! (THE FIX)\n",
    "# Remember, you trained on log(Transport_Cost + 1)\n",
    "final_predictions = np.expm1(log_predictions)\n",
    "\n",
    "# (Safety check: ensure no negative predictions)\n",
    "# final_predictions[final_predictions < 0] = 0\n",
    "\n",
    "# 6. Create the final submission file\n",
    "submission_df = pd.DataFrame({\n",
    "    'Hospital_Id': submission_ids,\n",
    "    'Transport_Cost': final_predictions\n",
    "})\n",
    "\n",
    "# Display the first few predictions\n",
    "print(\"\\nFinal Predictions (in $):\")\n",
    "display(submission_df.head())\n",
    "\n",
    "# Save to CSV\n",
    "submission_df.to_csv('mixed_hybrid_xxgb.csv', index=False)\n",
    "print(\"Submission file 'mixed_hybrid_ridge.csv' created successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MlLab)",
   "language": "python",
   "name": "mllab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
