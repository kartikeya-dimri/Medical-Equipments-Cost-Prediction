{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf176dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 1: IMPORTS (V46 - RF/XGB Focus) ===\n",
    "print(\"--- Cell 1: Imports ---\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, FunctionTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# New models\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "print(\"✅ Libraries imported (incl. RF & XGB).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646cc6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 2: DATA LOADING & INITIAL CLEANING ===\n",
    "print(\"\\n--- Cell 2: Data Loading & Initial Cleaning ---\")\n",
    "try:\n",
    "    df = pd.read_csv('../data/train.csv')\n",
    "    df.columns = df.columns.str.strip()\n",
    "    print(f\"Loaded train.csv: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: ../data/train.csv not found.\")\n",
    "    df = None\n",
    "except Exception as e: \n",
    "    print(f\"Error loading train.csv: {e}\"); df = None\n",
    "\n",
    "if df is not None:\n",
    "    print(\"Converting date columns (temporarily)...\")\n",
    "    df['Order_Placed_Date'] = pd.to_datetime(df['Order_Placed_Date'], errors='coerce')\n",
    "    df['Delivery_Date'] = pd.to_datetime(df['Delivery_Date'], errors='coerce')\n",
    "    print(\"✅ Basic loading complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32de2381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 3: V45/V46 FEATURE ENGINEERING & SELECTION (Focus 2) ===\n",
    "if df is not None:\n",
    "    print(\"\\n--- Cell 3: V46 Feature Engineering & Selection ---\")\n",
    "\n",
    "    print(\"[Step 1/4] Repairing negative values using abs()...\")\n",
    "    df['Delivery_Days_temp'] = (df['Delivery_Date'] - df['Order_Placed_Date']).dt.days\n",
    "    df['Delivery_Days_temp'] = df['Delivery_Days_temp'].abs() # Repair\n",
    "    df['Transport_Cost'] = df['Transport_Cost'].abs() # Use abs()\n",
    "    \n",
    "    try:\n",
    "        prediction_cap_value_995 = df['Transport_Cost'].dropna().quantile(0.995)\n",
    "        print(f\"   ✓ Calculated Prediction Cap (99.5th percentile): {prediction_cap_value_995:,.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ! Warning: Could not calculate prediction cap: {e}. Using fallback.\")\n",
    "        prediction_cap_value_995 = 1500000\n",
    "    print(\"   ✓ Negatives repaired.\")\n",
    "\n",
    "    print(\"\\n[Step 2/4] Defining target variable (y)...\")\n",
    "    y = np.log1p(df['Transport_Cost']) # Log-transform of abs(cost)\n",
    "    print(f\"   ✓ Target (y) created: {y.shape}\")\n",
    "\n",
    "    print(\"\\n[Step 3/4] Selecting features (X) - Focus 2...\")\n",
    "    features_to_keep = ['Equipment_Value', 'Equipment_Weight']\n",
    "    features_present = [col for col in features_to_keep if col in df.columns]\n",
    "    X = df[features_present].copy() # Create X with ONLY these columns\n",
    "    print(f\"   ✓ Feature set 'X' created (Focus 2): {X.shape}\")\n",
    "    print(f\"   ✓ Features: {X.columns.tolist()}\")\n",
    "\n",
    "    print(\"\\n[Step 4/4] Splitting data into Train/Validation (80/20)...\")\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print(f\"   ✓ Data split complete.\")\n",
    "\n",
    "    print(\"✅ V46 Feature Engineering/Selection complete.\")\n",
    "\n",
    "else:\n",
    "    print(\"   ✗ Feature engineering skipped due to data loading error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8066df4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 4: V46 PREPROCESSING PIPELINE DEFINITION & FITTING ===\n",
    "if 'X_train' in locals() and not X_train.empty:\n",
    "    print(\"\\n--- Cell 4: V46 Preprocessing Pipeline (Impute -> Clip -> Log -> Scale) ---\")\n",
    "\n",
    "    # --- Define Transformer Pipeline for the 2 features ---\n",
    "    # Apply Impute -> Clip(0) -> Log1p -> Scale\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('clipper', FunctionTransformer(lambda x: np.maximum(x, 0), validate=False)), # Clip >= 0\n",
    "        ('log', FunctionTransformer(np.log1p, validate=False)), # Apply log1p\n",
    "        ('scaler', StandardScaler()) # Then scale\n",
    "    ])\n",
    "    print(\"   ✓ Transformer defined (Impute -> Clip(0) -> Log1p -> StandardScaler).\")\n",
    "\n",
    "    # --- Assemble Preprocessor ---\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, X_train.columns.tolist()) # Apply to all columns in X_train\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "    print(\"   ✓ Preprocessor assembled.\")\n",
    "\n",
    "    # --- Fit Preprocessor ---\n",
    "    print(\"Fitting preprocessor on X_train...\")\n",
    "    preprocessor.fit(X_train) # Fit only on the training part\n",
    "    print(\"✅ V46 Preprocessor fitted.\")\n",
    "else:\n",
    "      print(\"   ✗ Preprocessor fitting skipped (X_train not defined or empty).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95fc52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 5: HYPERPARAMETER TUNING (RANDOM FOREST) ===\n",
    "if 'X_train' in locals() and 'preprocessor' in locals() and preprocessor is not None:\n",
    "    print(\"\\n--- Cell 5: Tuning Random Forest (V46 Data) ---\")\n",
    "\n",
    "    # --- RF pipeline ---\n",
    "    rf_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor), # Use V46 preprocessor\n",
    "        ('rf', RandomForestRegressor(random_state=42, n_jobs=-1))\n",
    "    ])\n",
    "\n",
    "    # --- Hyperparameter grid (for RandomizedSearch) ---\n",
    "    param_dist_rf = {\n",
    "        'rf__n_estimators': [100, 200, 300, 400, 500],\n",
    "        'rf__max_depth': [None, 10, 20, 30, 40],\n",
    "        'rf__min_samples_leaf': [1, 2, 4, 6],\n",
    "        'rf__min_samples_split': [2, 5, 10],\n",
    "        'rf__max_features': [1.0, 'sqrt', 'log2'] # 1.0 is all features (equiv. to 'auto')\n",
    "    }\n",
    "    print(f\"RandomizedSearch Parameter Grid (RF):\\\\n{param_dist_rf}\")\n",
    "\n",
    "    # --- RandomizedSearchCV setup ---\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    rf_search = RandomizedSearchCV(\n",
    "        estimator=rf_pipeline,\n",
    "        param_distributions=param_dist_rf,\n",
    "        n_iter=50, # Number of combinations to try\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        cv=kf,\n",
    "        n_jobs=-1, # Use all cores\n",
    "        random_state=42,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    print(\"Starting RandomizedSearchCV for Random Forest...\")\n",
    "    rf_search.fit(X_train, y_train) # Fit on the 80% training split\n",
    "\n",
    "    print(\"\\n✅ Random Forest Tuning complete!\")\n",
    "    rf_best_params = rf_search.best_params_\n",
    "    rf_best_cv_rmse = -rf_search.best_score_\n",
    "    print(f\"   Best hyperparameters (RF): {rf_best_params}\")\n",
    "    print(f\"   Best CV RMSE (log-space, RF): {rf_best_cv_rmse:.4f}\")\n",
    "\n",
    "    # --- Evaluate on Validation Set ---\n",
    "    rf_best_model = rf_search.best_estimator_\n",
    "    y_val_pred_log = rf_best_model.predict(X_val)\n",
    "    rf_val_rmse_log = np.sqrt(mean_squared_error(y_val, y_val_pred_log))\n",
    "    y_val_orig = np.expm1(y_val)\n",
    "    y_val_pred_orig = np.maximum(np.expm1(y_val_pred_log), 0) # Clip preds >= 0\n",
    "    rf_val_rmse_orig = np.sqrt(mean_squared_error(y_val_orig, y_val_pred_orig))\n",
    "    print(f\"--- RF Validation Set Performance ---\")\n",
    "    print(f\"   Validation RMSE (log-space): {rf_val_rmse_log:.4f}\")\n",
    "    print(f\"   Validation RMSE (original scale): {rf_val_rmse_orig:,.2f}\")\n",
    "\n",
    "else:\n",
    "    print(\"   ✗ RF Tuning skipped - check previous cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421085f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 6: HYPERPARAMETER TUNING (XGBOOST) ===\n",
    "if 'X_train' in locals() and 'preprocessor' in locals() and preprocessor is not None:\n",
    "    print(\"\\n--- Cell 6: Tuning XGBoost (V46 Data) ---\")\n",
    "\n",
    "    # --- XGB pipeline ---\n",
    "    xgb_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor), # Use V46 preprocessor\n",
    "        ('xgb', XGBRegressor(random_state=42, objective='reg:squarederror', eval_metric='rmse', n_jobs=-1))\n",
    "    ])\n",
    "\n",
    "    # --- Hyperparameter grid (for RandomizedSearch) ---\n",
    "    param_dist_xgb = {\n",
    "        'xgb__n_estimators': [100, 200, 300, 400, 500, 600],\n",
    "        'xgb__max_depth': [3, 5, 7, 9, 11],\n",
    "        'xgb__learning_rate': [0.01, 0.02, 0.05, 0.1, 0.15],\n",
    "        'xgb__subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'xgb__colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'xgb__gamma': [0, 0.1, 0.2, 0.3] # Regularization\n",
    "    }\n",
    "    print(f\"RandomizedSearch Parameter Grid (XGB):\\\\n{param_dist_xgb}\")\n",
    "\n",
    "    # --- RandomizedSearchCV setup ---\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    xgb_search = RandomizedSearchCV(\n",
    "        estimator=xgb_pipeline,\n",
    "        param_distributions=param_dist_xgb,\n",
    "        n_iter=50, # Number of combinations to try\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        cv=kf,\n",
    "        n_jobs=-1, # Use all cores\n",
    "        random_state=42,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    print(\"Starting RandomizedSearchCV for XGBoost...\")\n",
    "    xgb_search.fit(X_train, y_train) # Fit on the 80% training split\n",
    "\n",
    "    print(\"\\n✅ XGBoost Tuning complete!\")\n",
    "    xgb_best_params = xgb_search.best_params_\n",
    "    xgb_best_cv_rmse = -xgb_search.best_score_\n",
    "    print(f\"   Best hyperparameters (XGB): {xgb_best_params}\")\n",
    "    print(f\"   Best CV RMSE (log-space, XGB): {xgb_best_cv_rmse:.4f}\")\n",
    "\n",
    "    # --- Evaluate on Validation Set ---\n",
    "    xgb_best_model = xgb_search.best_estimator_\n",
    "    y_val_pred_log = xgb_best_model.predict(X_val)\n",
    "    xgb_val_rmse_log = np.sqrt(mean_squared_error(y_val, y_val_pred_log))\n",
    "    y_val_orig = np.expm1(y_val)\n",
    "    y_val_pred_orig = np.maximum(np.expm1(y_val_pred_log), 0) # Clip preds >= 0\n",
    "    xgb_val_rmse_orig = np.sqrt(mean_squared_error(y_val_orig, y_val_pred_orig))\n",
    "    print(f\"--- XGB Validation Set Performance ---\")\n",
    "    print(f\"   Validation RMSE (log-space): {xgb_val_rmse_log:.4f}\")\n",
    "    print(f\"   Validation RMSE (original scale): {xgb_val_rmse_orig:,.2f}\")\n",
    "\n",
    "else:\n",
    "    print(\"   ✗ XGB Tuning skipped - check previous cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e56970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 7: TRAIN FINAL MODEL (RF or XGB) ===\n",
    "\n",
    "# Check if both tuning cells have run and produced scores\n",
    "if ('rf_val_rmse_log' in locals() and 'xgb_val_rmse_log' in locals() and \n",
    "    'X' in locals() and 'y' in locals()):\n",
    "    print(\"\\n--- Cell 7: Training Final Model on ALL V46 Data ---\")\n",
    "\n",
    "    # --- Select the Best Model based on Validation RMSE ---\n",
    "    if rf_val_rmse_log < xgb_val_rmse_log:\n",
    "        print(f\"   Selecting Random Forest (Val RMSE: {rf_val_rmse_log:.4f}) over XGBoost (Val RMSE: {xgb_val_rmse_log:.4f})\")\n",
    "        final_model_name = 'rf'\n",
    "        final_params = rf_best_params\n",
    "        # Re-create the model object with best params\n",
    "        final_model_obj = RandomForestRegressor(\n",
    "            n_estimators=final_params['rf__n_estimators'],\n",
    "            max_depth=final_params['rf__max_depth'],\n",
    "            min_samples_leaf=final_params['rf__min_samples_leaf'],\n",
    "            min_samples_split=final_params['rf__min_samples_split'],\n",
    "            max_features=final_params['rf__max_features'],\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    else:\n",
    "        print(f\"   Selecting XGBoost (Val RMSE: {xgb_val_rmse_log:.4f}) over Random Forest (Val RMSE: {rf_val_rmse_log:.4f})\")\n",
    "        final_model_name = 'xgb'\n",
    "        final_params = xgb_best_params\n",
    "        # Re-create the model object with best params\n",
    "        final_model_obj = XGBRegressor(\n",
    "            n_estimators=final_params['xgb__n_estimators'],\n",
    "            max_depth=final_params['xgb__max_depth'],\n",
    "            learning_rate=final_params['xgb__learning_rate'],\n",
    "            subsample=final_params['xgb__subsample'],\n",
    "            colsample_bytree=final_params['xgb__colsample_bytree'],\n",
    "            gamma=final_params['xgb__gamma'],\n",
    "            random_state=42, \n",
    "            objective='reg:squarederror', \n",
    "            eval_metric='rmse',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    \n",
    "    print(f\"   ✓ Using best params for {final_model_name}: {final_params}\")\n",
    "\n",
    "    # --- Re-fit preprocessor on FULL X data ---\n",
    "    print(\"   Re-fitting V46 preprocessor on full dataset X...\")\n",
    "    # Rebuild the final preprocessor object using definitions from Cell 4\n",
    "    log_transformer_pipeline = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('clipper', FunctionTransformer(lambda x: np.maximum(x, 0), validate=False)),\n",
    "        ('log', FunctionTransformer(np.log1p, validate=False)),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    final_preprocessor = ColumnTransformer(transformers=[\n",
    "        ('num', log_transformer_pipeline, X.columns.tolist())\n",
    "        ], remainder='drop')\n",
    "    \n",
    "    final_preprocessor.fit(X) # Fit on ALL X data\n",
    "    print(\"   ✓ Preprocessor re-fitted on full X dataset.\")\n",
    "\n",
    "    # Create the final pipeline\n",
    "    final_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', final_preprocessor), # Use the re-fitted preprocessor\n",
    "        (final_model_name, final_model_obj)  # Add the best model\n",
    "    ])\n",
    "\n",
    "    # === Fit final model on full V46 dataset ===\n",
    "    print(f\"Fitting final {final_model_name} model on the full V46 dataset (X, y)...\")\n",
    "    print(\"X shape:\", X.shape)\n",
    "    print(\"y shape:\", y.shape)\n",
    "\n",
    "    final_pipeline.fit(X, y) # Train on full X and y\n",
    "\n",
    "    print(f\"\\n✅ Final (V46 {final_model_name.upper()}) model trained.\")\n",
    "    print(\"The 'final_pipeline' object is ready for prediction.\")\n",
    "\n",
    "else:\n",
    "    print(\"   ✗ Final model training skipped. Required variables not found.\")\n",
    "    if 'rf_val_rmse_log' not in locals(): print(\"     - Random Forest tuning (Cell 5) not run.\")\n",
    "    if 'xgb_val_rmse_log' not in locals(): print(\"     - XGBoost tuning (Cell 6) not run.\")\n",
    "    if 'X' not in locals() or 'y' not in locals(): print(\"     - Full dataset 'X' or 'y' not defined (Cell 3).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee39fad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 8: V46 TEST DATA PREPARATION FUNCTION ===\n",
    "# (This is identical to V45 as it only selects the 2 features)\n",
    "\n",
    "def prepare_features_V46(df_raw, train_cols_expected):\n",
    "    \"\"\"Applies V46 cleaning and selects the 2 key features\"\"\"\n",
    "    print(\"Preparing test features (V46 - Focus 2)...\")\n",
    "    df_test = df_raw.copy()\n",
    "    df_test.columns = df_test.columns.str.strip()\n",
    "\n",
    "    # --- Select ONLY the 2 key features ---\n",
    "    features_present_in_test = [col for col in train_cols_expected if col in df_test.columns]\n",
    "\n",
    "    missing_in_test = set(train_cols_expected) - set(features_present_in_test)\n",
    "    if missing_in_test:\n",
    "        print(f\"   ! Warning: Expected features missing in test data: {missing_in_test}. Adding as NaN.\")\n",
    "        for col in missing_in_test:\n",
    "            df_test[col] = np.nan\n",
    "\n",
    "    try:\n",
    "        df_test_final = df_test.reindex(columns=train_cols_expected)\n",
    "        print(f\"Test data prepared. Shape: {df_test_final.shape}\")\n",
    "        return df_test_final\n",
    "    except Exception as e:\n",
    "        print(f\"   ✗ Error ensuring column consistency: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"\\n✅ V46 Test preparation function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3687272d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 9: GENERATE SUBMISSION (V46 - Final Model) ===\n",
    "\n",
    "# Check if the final model, prepare function, cap value, and X exist\n",
    "if ('final_pipeline' in locals() and\n",
    "    'prepare_features_V46' in globals() and\n",
    "    'X' in locals() and \n",
    "    'prediction_cap_value_995' in locals()):\n",
    "    print(\"\\n--- Cell 9: Generating Submission File (V46 Pipeline) ---\")\n",
    "    try:\n",
    "        # Load the raw test data\n",
    "        df_test_raw = pd.read_csv('../data/test.csv')\n",
    "        submission_ids = df_test_raw['Hospital_Id']\n",
    "        print(f\"Loaded test.csv: {df_test_raw.shape}\")\n",
    "\n",
    "        # Prepare test features using V46 function\n",
    "        X_test_final_raw = prepare_features_V46(df_test_raw, X.columns) \n",
    "\n",
    "        if X_test_final_raw is not None:\n",
    "            print(\"Getting predictions from the final V46 model...\")\n",
    "            log_predictions = final_pipeline.predict(X_test_final_raw)\n",
    "\n",
    "            print(\"Converting predictions from log scale...\")\n",
    "            final_predictions = np.expm1(log_predictions)\n",
    "\n",
    "            # --- Apply Aggressive Safety Clip (99.5th Percentile) ---\n",
    "            print(f\"Applying safety clip to predictions at {prediction_cap_value_995:,.2f} (99.5th percentile)\")\n",
    "            final_predictions = np.clip(final_predictions, 0, prediction_cap_value_995)\n",
    "\n",
    "            print(\"Creating submission DataFrame...\")\n",
    "            submission_df = pd.DataFrame({\n",
    "                'Hospital_Id': submission_ids,\n",
    "                'Transport_Cost': final_predictions\n",
    "            })\n",
    "\n",
    "            output_filename = 'model.csv'\n",
    "            submission_df.to_csv(output_filename, index=False)\n",
    "            print(f\"\\n✅ Submission file '{output_filename}' saved successfully.\")\n",
    "            print(\"Final Predictions Head:\")\n",
    "            print(submission_df.head())\n",
    "\n",
    "        else:\n",
    "            print(\"   ✗ Submission generation failed: Error preparing test features.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"   ✗ Error: ../data/test.csv not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ✗ An unexpected error occurred during submission generation: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n--- Submission Generation Skipped ---\")\n",
    "    if 'final_pipeline' not in locals(): print(\"   Reason: Final V46 model not trained (Run Cell 7).\")\n",
    "    if 'prepare_features_V46' not in globals(): print(\"   Reason: 'prepare_features_V46' function not defined (Run Cell 8).\")\n",
    "    if 'X' not in locals(): print(\"   Reason: Training features 'X' not defined (Run Cell 3).\")\n",
    "    if 'prediction_cap_value_995' not in locals(): print(\"   Reason: 'prediction_cap_value_995' not defined (Run Cell 3).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
