{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad8d2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import PowerTransformer, OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from lightgbm import LGBMRegressor # A powerful gradient boosting model\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4617a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "\n",
    "# 1️⃣ Load Data\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv('../data/train.csv')\n",
    "df.columns = df.columns.str.strip()\n",
    "display(df.head())\n",
    "print(f\"Initial data shape: {df.shape}\")\n",
    "\n",
    "# 2️⃣ Clean all string/object columns: strip spaces, replace blanks with NaN\n",
    "print(\"Cleaning string columns...\")\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    df[col] = df[col].astype(str).str.strip()\n",
    "    df[col] = df[col].replace({'': np.nan, 'nan': np.nan, 'NaN': np.nan})\n",
    "\n",
    "# 3️⃣ Normalize Yes/No columns to consistent \"Yes\"/\"No\"\n",
    "print(\"Normalizing Yes/No columns...\")\n",
    "yes_no_cols = ['CrossBorder_Shipping', 'Urgent_Shipping', 'Installation_Service',\n",
    "               'Fragile_Equipment', 'Rural_Hospital']\n",
    "for col in yes_no_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace({\n",
    "            'YES': 'Yes', 'yes': 'Yes', 'Y': 'Yes', 'y': 'Yes',\n",
    "            'NO': 'No', 'no': 'No', 'N': 'No', 'n': 'No'\n",
    "        })\n",
    "\n",
    "# 4️⃣ Convert date columns to datetime\n",
    "print(\"Converting date columns...\")\n",
    "df['Order_Placed_Date'] = pd.to_datetime(df['Order_Placed_Date'], errors='coerce')\n",
    "df['Delivery_Date'] = pd.to_datetime(df['Delivery_Date'], errors='coerce')\n",
    "\n",
    "# 5️⃣ Create new feature: Delivery_Days (difference in days)\n",
    "print(\"Engineering Delivery_Days feature...\")\n",
    "df['Delivery_Days'] = (df['Delivery_Date'] - df['Order_Placed_Date']).dt.days\n",
    "df['Delivery_Days'] = pd.to_numeric(df['Delivery_Days'], errors='coerce')\n",
    "\n",
    "# === ADDED: Date Feature Engineering ===\n",
    "print(\"Engineering more date features...\")\n",
    "df['Order_Month'] = df['Order_Placed_Date'].dt.month\n",
    "df['Order_Day_of_Week'] = df['Order_Placed_Date'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "df['Order_Is_Weekend'] = df['Order_Day_of_Week'].isin([5, 6])\n",
    "# === END ADDED ===\n",
    "\n",
    "# 6️⃣ (Original) delete initial date rows\n",
    "# df = df.dropna(subset=['Order_Placed_Date', 'Delivery_Date'])\n",
    "\n",
    "# 7️⃣ Drop exact duplicate rows\n",
    "print(\"Dropping duplicates...\")\n",
    "before = len(df)\n",
    "df = df.drop_duplicates()\n",
    "after = len(df)\n",
    "print(f\"Dropped {before - after} duplicate rows.\")\n",
    "\n",
    "# 8️⃣ Quick check after cleaning\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\" CLEANING & FEATURE ENGINEERING COMPLETE \")\n",
    "print(\"=\"*30)\n",
    "print(f\"After basic cleaning shape: {df.shape}\")\n",
    "\n",
    "print(\"\\nMissing values (raw count):\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "# === ADDED: Missing Value Percentage View ===\n",
    "print(\"\\nMissing values (percentage):\")\n",
    "missing_pct = (df.isna().sum() / len(df) * 100).sort_values(ascending=False)\n",
    "print(missing_pct[missing_pct > 0])\n",
    "# === END ADDED ===\n",
    "\n",
    "print(\"\\nDataFrame head:\")\n",
    "display(df.head())\n",
    "# print(df['Delivery_Days'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6847d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# 📊 START OF EXPLORATORY DATA ANALYSIS (EDA)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\" STARTING EDA \")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# 🔹 Define column lists\n",
    "num_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "# === ADDED: Exclude new date features from 'num_cols' for general stats ===\n",
    "date_num_features = ['Order_Month', 'Order_Day_of_Week', 'Delivery_Days']\n",
    "for col in ['Transport_Cost'] + date_num_features:\n",
    "    if col in num_cols:\n",
    "        num_cols.remove(col)\n",
    "# === END ADDED ===\n",
    "        \n",
    "cat_cols = df.select_dtypes(include='object').columns.tolist()\n",
    "# === ADDED: Add boolean 'Is_Weekend' to cat_cols for analysis ===\n",
    "if 'Order_Is_Weekend' in df.columns:\n",
    "    cat_cols.append('Order_Is_Weekend')\n",
    "# === END ADDED ===\n",
    "\n",
    "print(f\"Numeric features identified: {num_cols}\")\n",
    "print(f\"Categorical features identified: {cat_cols}\")\n",
    "print(f\"Date-derived features identified: {date_num_features}\")\n",
    "\n",
    "\n",
    "# === ADDED: 1. Target Variable Analysis (Transport_Cost) ===\n",
    "print(\"\\n===== 1. TARGET VARIABLE ANALYSIS: Transport_Cost =====\")\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Original Distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['Transport_Cost'], kde=True, bins=40)\n",
    "plt.title('Distribution of Transport_Cost (Original)')\n",
    "plt.xlabel('Transport_Cost')\n",
    "\n",
    "# Plot 2: Log-Transformed Distribution\n",
    "# We add 1 to handle potential zero values before logging\n",
    "plt.subplot(1, 2, 2)\n",
    "log_target = np.log1p(df['Transport_Cost'])\n",
    "sns.histplot(log_target, kde=True, bins=40, color='green')\n",
    "plt.title('Distribution of log(Transport_Cost + 1)')\n",
    "plt.xlabel('log(Transport_Cost + 1)')\n",
    "\n",
    "plt.suptitle('Target Variable Distribution Analysis', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "print(f\"Skewness of Transport_Cost: {df['Transport_Cost'].skew():.4f}\")\n",
    "print(f\"Skewness of log(Transport_Cost + 1): {log_target.skew():.4f}\")\n",
    "# === END ADDED ===\n",
    "\n",
    "\n",
    "print(\"\\n===== 2. NUMERIC FEATURE ANALYSIS =====\")\n",
    "print(\"===== BASIC NUMERIC STATISTICS =====\")\n",
    "if not num_cols:\n",
    "    print(\"No numeric columns found to describe (excluding target/dates).\")\n",
    "else:\n",
    "    display(df[num_cols].describe().T)\n",
    "\n",
    "    print(\"\\n===== SKEWNESS =====\")\n",
    "    display(df[num_cols].skew())\n",
    "\n",
    "# 🔹 Numeric distributions + boxplots\n",
    "# (Your original loop)\n",
    "# === MODIFIED: Added a check for empty list ===\n",
    "print(\"\\nGenerating numeric distribution plots...\")\n",
    "analysis_num_cols = num_cols + ['Delivery_Days'] # Add Delivery_Days back for plotting\n",
    "if 'Transport_Cost' not in analysis_num_cols:\n",
    "    analysis_num_cols.append('Transport_Cost') # Add Target back for plotting\n",
    "    \n",
    "for col in analysis_num_cols:\n",
    "    if col in df.columns:\n",
    "        plt.figure(figsize=(12,4))\n",
    "        \n",
    "        plt.subplot(1,2,1)\n",
    "        sns.histplot(df[col], kde=True, bins=30)\n",
    "        plt.title(f'{col} distribution')\n",
    "        \n",
    "        plt.subplot(1,2,2)\n",
    "        sns.boxplot(x=df[col])\n",
    "        plt.title(f'{col} boxplot')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Warning: Column '{col}' not found for plotting.\")\n",
    "\n",
    "\n",
    "print(\"\\n===== 3. CORRELATION ANALYSIS =====\")\n",
    "# 🔹 Correlation heatmap\n",
    "# (Your original code)\n",
    "plt.figure(figsize=(10,8))\n",
    "corr = df[num_cols + ['Transport_Cost', 'Delivery_Days']].corr()\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n===== 4. CATEGORICAL FEATURE ANALYSIS =====\")\n",
    "# 🔹 Categorical distributions\n",
    "# (Your original loop)\n",
    "print(\"\\nGenerating categorical distribution plots...\")\n",
    "high_cardinality_cols = []\n",
    "for col in cat_cols:\n",
    "    print(f\"\\n===== Column: {col} =====\")\n",
    "    print(df[col].value_counts(dropna=False))\n",
    "    \n",
    "    nunique = df[col].nunique()\n",
    "    if nunique > 20:\n",
    "        high_cardinality_cols.append(col)\n",
    "        print(f\"SKIPPING countplot for {col} (High Cardinality: {nunique} unique values)\")\n",
    "        continue\n",
    "        \n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.countplot(y=col, data=df, order=df[col].value_counts().index)\n",
    "    plt.title(f'Count of {col}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# === ADDED: 4a. High-Cardinality Column Summary ===\n",
    "print(\"\\n===== 4a. HIGH-CARDINALITY CATEGORICAL SUMMARY =====\")\n",
    "if high_cardinality_cols:\n",
    "    print(f\"High-cardinality features detected: {high_cardinality_cols}\")\n",
    "    for col in high_cardinality_cols:\n",
    "        print(f\"\\n--- Top 10 values for: {col} ---\")\n",
    "        print(df[col].value_counts(dropna=False).head(10))\n",
    "        print(f\"...and {df[col].nunique() - 10} other unique values.\")\n",
    "else:\n",
    "    print(\"No high-cardinality categorical features detected (threshold > 20).\")\n",
    "# === END ADDED ===\n",
    "\n",
    "\n",
    "print(\"\\n===== 5. BIVARIATE ANALYSIS (FEATURES vs. TARGET) =====\")\n",
    "# 🔹 Numeric features vs target\n",
    "# (Your original loop)\n",
    "print(\"\\nGenerating numeric features vs. Transport_Cost...\")\n",
    "for col in num_cols + ['Delivery_Days']:\n",
    "    if col in df.columns:\n",
    "        plt.figure(figsize=(6,4))\n",
    "        sns.scatterplot(x=df[col], y=df['Transport_Cost'])\n",
    "        plt.title(f'{col} vs Transport_Cost')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# 🔹 Categorical features vs target (low-cardinality)\n",
    "# (Your original loop)\n",
    "print(\"\\nGenerating categorical features vs. Transport_Cost...\")\n",
    "for col in cat_cols:\n",
    "    if col in df.columns and df[col].nunique() < 20:\n",
    "        plt.figure(figsize=(10,4))\n",
    "        sns.boxplot(x=col, y='Transport_Cost', data=df)\n",
    "        plt.title(f'{col} vs Transport_Cost')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# === ADDED: 5a. Date-Derived Features vs. Target ===\n",
    "print(\"\\nGenerating date-derived features vs. Transport_Cost...\")\n",
    "date_features_to_plot = ['Order_Month', 'Order_Day_of_Week', 'Order_Is_Weekend']\n",
    "for col in date_features_to_plot:\n",
    "    if col in df.columns:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        sns.boxplot(x=col, y='Transport_Cost', data=df)\n",
    "        plt.title(f'{col} vs Transport_Cost')\n",
    "        if col == 'Order_Day_of_Week':\n",
    "            plt.xticks(ticks=range(7), labels=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "# === END ADDED ===\n",
    "\n",
    "\n",
    "print(\"\\n===== 6. OUTLIER DETECTION =====\")\n",
    "# 🔹 Outlier detection (Z-score)\n",
    "# (Your original code)\n",
    "# === MODIFIED: Added nan_policy='omit' to handle missing values gracefully ===\n",
    "try:\n",
    "    z_scores = df[num_cols + ['Transport_Cost', 'Delivery_Days']].apply(lambda x: zscore(x, nan_policy='omit'))\n",
    "    outliers = (abs(z_scores) > 3).sum()\n",
    "    print(\"\\n===== NUMBER OF OUTLIERS PER COLUMN (Z-score > 3) =====\")\n",
    "    print(outliers[outliers > 0].sort_values(ascending=False))\n",
    "except ValueError as e:\n",
    "    print(f\"Could not calculate Z-scores, likely due to all-NaN column. Error: {e}\")\n",
    "# === END MODIFIED ===\n",
    "\n",
    "\n",
    "print(\"\\n===== 7. MISSING VALUE VISUALIZATION =====\")\n",
    "# 🔹 Missing value visualization\n",
    "# (Your original code)\n",
    "print(\"\\nGenerating missing value matrix...\")\n",
    "msno.matrix(df)\n",
    "plt.title('Missing Value Matrix')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nGenerating missing value bar chart...\")\n",
    "msno.bar(df)\n",
    "plt.title('Missing Value Bar Chart')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\" EDA COMPLETE \")\n",
    "print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "61ad723c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing script started...\n",
      "Filtered 0 rows with bad data (negative cost or delivery days).\n",
      "Features for modeling: ['Supplier_Reliability', 'Equipment_Type', 'Equipment_Value', 'Base_Transport_Fee', 'CrossBorder_Shipping', 'Urgent_Shipping', 'Installation_Service', 'Transport_Method', 'Fragile_Equipment', 'Hospital_Info', 'Rural_Hospital', 'Delivery_Days', 'Order_Month', 'Order_Day_of_Week', 'Order_Is_Weekend', 'Equipment_Volume']\n",
      "Baseline RMSE (train, log-space): 1.6738\n",
      "Baseline RMSE (test, log-space): 1.7725\n",
      "Baseline RMSE (test, original-scale): 625946.6068\n",
      "Training set shape: (2186, 16)\n",
      "Test set shape: (547, 16)\n",
      "\n",
      "Fitting preprocessor on X_train...\n",
      "Preprocessing complete.\n",
      "Processed X_train shape: (2186, 48)\n",
      "Processed X_test shape: (547, 48)\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocessing script started...\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: PRE-SPLIT (Data Cleaning & Feature Engineering)\n",
    "# These actions are applied to the whole dataset before splitting.\n",
    "# ==============================================================================\n",
    "\n",
    "# 1. Filter Bad Data\n",
    "# EDA Finding: We found impossible values like Transport_Cost < 0 and Delivery_Days < 0.\n",
    "# Action: Remove these rows entirely.\n",
    "initial_rows = len(df)\n",
    "df = df[df['Transport_Cost'] >= 0]\n",
    "df = df[df['Delivery_Days'] >= 0]\n",
    "print(f\"Filtered {initial_rows - len(df)} rows with bad data (negative cost or delivery days).\")\n",
    "\n",
    "# 2. Feature Engineering\n",
    "# EDA Finding: Equipment_Height & Equipment_Width were highly correlated (0.77).\n",
    "# Action: Combine them into a single 'Equipment_Volume' feature.\n",
    "df['Equipment_Volume'] = df['Equipment_Height'] * df['Equipment_Width']\n",
    "\n",
    "# 3. Log-Transform Skewed Features\n",
    "# EDA Finding: Equipment_Value (skew=24) and our new Equipment_Volume\n",
    "# (derived from skewed features) are extremely right-skewed.\n",
    "# Action: Apply np.log1p to normalize them.\n",
    "df['Equipment_Value'] = np.log1p(df['Equipment_Value'])\n",
    "df['Equipment_Volume'] = np.log1p(df['Equipment_Volume'])\n",
    "\n",
    "# 4. Define Target (y) and Features (X)\n",
    "# EDA Finding: Target 'Transport_Cost' is extremely skewed (skew=30).\n",
    "# Action: Use np.log1p on the target. We will predict the log, then convert back.\n",
    "y = np.log1p(df['Transport_Cost'])\n",
    "\n",
    "# Action: Define X by dropping the target, original engineered columns, \n",
    "# and high-cardinality/redundant/ID columns identified in the EDA.\n",
    "X = df.drop(columns=[\n",
    "    # Target\n",
    "    'Transport_Cost',\n",
    "    \n",
    "    # Replaced by Equipment_Volume\n",
    "    'Equipment_Height',\n",
    "    'Equipment_Width',\n",
    "    \n",
    "    # Redundant (corr 0.90 with Value)\n",
    "    'Equipment_Weight',\n",
    "    \n",
    "    # High-Cardinality IDs / Unused\n",
    "    'Hospital_Id',\n",
    "    'Supplier_Name',\n",
    "    'Hospital_Location',\n",
    "    \n",
    "    # Replaced by date features\n",
    "    'Order_Placed_Date',\n",
    "    'Delivery_Date'\n",
    "])\n",
    "\n",
    "print(f\"Features for modeling: {X.columns.tolist()}\")\n",
    "\n",
    "# 5. Train-Test Split\n",
    "# Action: Split the data into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "train_mean = y_train.mean()\n",
    "\n",
    "# 1️⃣ Baseline RMSE on the training set (log-space)\n",
    "y_train_pred_baseline = np.full_like(y_train, train_mean)\n",
    "baseline_rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred_baseline))\n",
    "print(f\"Baseline RMSE (train, log-space): {baseline_rmse_train:.4f}\")\n",
    "\n",
    "# 2️⃣ Baseline RMSE on the test set (using train mean as predictor) — log-space\n",
    "y_test_pred_baseline = np.full_like(y_test, train_mean)\n",
    "baseline_rmse_test_log = np.sqrt(mean_squared_error(y_test, y_test_pred_baseline))\n",
    "print(f\"Baseline RMSE (test, log-space): {baseline_rmse_test_log:.4f}\")\n",
    "\n",
    "# 3️⃣ Baseline RMSE in original (Transport_Cost) scale\n",
    "y_test_actual_orig = np.expm1(y_test)\n",
    "y_test_baseline_pred_orig = np.expm1(y_test_pred_baseline)\n",
    "baseline_rmse_test_orig = np.sqrt(mean_squared_error(y_test_actual_orig, y_test_baseline_pred_orig))\n",
    "print(f\"Baseline RMSE (test, original-scale): {baseline_rmse_test_orig:.4f}\")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: POST-SPLIT (Pipelines & ColumnTransformer)\n",
    "# This prevents data leakage. We FIT on X_train, then TRANSFORM X_train and X_test.\n",
    "# ==============================================================================\n",
    "\n",
    "# 1. Define Feature Lists\n",
    "# Action: Separate our final columns into numeric and categorical lists.\n",
    "\n",
    "numeric_features = [\n",
    "    'Supplier_Reliability',\n",
    "    'Equipment_Value',      # Already log-transformed\n",
    "    'Base_Transport_Fee',\n",
    "    'Delivery_Days',\n",
    "    'Equipment_Volume'      # Already log-transformed\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'Equipment_Type',\n",
    "    'CrossBorder_Shipping',\n",
    "    'Urgent_Shipping',\n",
    "    'Installation_Service',\n",
    "    'Transport_Method',\n",
    "    'Fragile_Equipment',\n",
    "    'Hospital_Info',\n",
    "    'Rural_Hospital',\n",
    "    'Order_Month',\n",
    "    'Order_Day_of_Week',\n",
    "    'Order_Is_Weekend'\n",
    "]\n",
    "\n",
    "# 2. Create the Numeric Pipeline\n",
    "# EDA Finding: Numeric features had missing values (e.g., Supplier_Reliability)\n",
    "# and were on different scales.\n",
    "# Action: Impute missing values with the median (robust to outliers)\n",
    "# and then scale all features.\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# 3. Create the Categorical Pipeline\n",
    "# EDA Finding: Categorical features had missing values (e.g., Transport_Method,\n",
    "# Rural_Hospital) and need to be converted to numbers.\n",
    "# Action: Impute missing values with the most frequent value and then\n",
    "# one-hot encode. 'handle_unknown='ignore'' ensures our model doesn't\n",
    "# crash if it sees a new category in the test data.\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# 4. Create the Full Preprocessor\n",
    "# Action: Combine the numeric and categorical pipelines using ColumnTransformer.\n",
    "# This single object will handle all preprocessing for us.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'  # Drop any columns we didn't explicitly list\n",
    ")\n",
    "\n",
    "# 5. Apply the Preprocessor\n",
    "# Action: FIT the preprocessor on X_train ONLY (to learn medians, modes, etc.)\n",
    "# and then TRANSFORM both X_train and X_test.\n",
    "# This gives us our final, model-ready datasets.\n",
    "\n",
    "print(\"\\nFitting preprocessor on X_train...\")\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"Preprocessing complete.\")\n",
    "print(f\"Processed X_train shape: {X_train_processed.shape}\")\n",
    "print(f\"Processed X_test shape: {X_test_processed.shape}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# K-Fold Validation Setup (Bonus)\n",
    "# ==============================================================================\n",
    "# You mentioned K-Fold. The *best* way to use this `preprocessor` is to\n",
    "# put it in a pipeline with your model. This way, the K-Fold cross-validation\n",
    "# will correctly re-fit the preprocessor on each fold, preventing all leakage.\n",
    "\n",
    "# Example (don't run, just for info):\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# 1. Choose a model\n",
    "# model = LinearRegression()\n",
    "\n",
    "# 2. Create the full ML pipeline\n",
    "# full_pipeline = Pipeline(steps=[\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('regressor', model)\n",
    "# ])\n",
    "\n",
    "# 3. Run cross-validation on the *original* X_train and y_train\n",
    "# This is the correct, leak-proof way to do it.\n",
    "# scores = cross_val_score(full_pipeline, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error')\n",
    "# print(f\"K-Fold Scores: {scores}\")\n",
    "# print(f\"Mean RMSE: {np.mean(scores)}\")\n",
    "\n",
    "# 4. Fit the final model\n",
    "# full_pipeline.fit(X_train, y_train)\n",
    "# print(\"Final model pipeline is fitted and ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0a25e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 1. Define Feature Groups and Drop Useless Columns ---\n",
    "features_to_drop = [\n",
    "    'Hospital_Id', \n",
    "    'Supplier_Name', \n",
    "    'Hospital_Location', \n",
    "    'Order_Placed_Date', \n",
    "    'Delivery_Date'\n",
    "]\n",
    "numeric_features = [\n",
    "    'Supplier_Reliability', \n",
    "    'Equipment_Height', \n",
    "    'Equipment_Width', \n",
    "    'Equipment_Weight', \n",
    "    'Equipment_Value', \n",
    "    'Base_Transport_Fee', \n",
    "    'Delivery_Days'\n",
    "]\n",
    "categorical_features = [\n",
    "    'Equipment_Type', \n",
    "    'CrossBorder_Shipping', \n",
    "    'Urgent_Shipping', \n",
    "    'Installation_Service', \n",
    "    'Transport_Method', \n",
    "    'Fragile_Equipment', \n",
    "    'Hospital_Info', \n",
    "    'Rural_Hospital'\n",
    "]\n",
    "target_variable = 'Transport_Cost'\n",
    "\n",
    "# Apply the drop\n",
    "df_clean = df.drop(columns=features_to_drop)\n",
    "\n",
    "# Drop any rows where the target is *initially* missing\n",
    "df_clean = df_clean.dropna(subset=[target_variable])\n",
    "\n",
    "# --- 2. Handle the Extreme Skew of the Target Variable ---\n",
    "# This line can create NaNs if Transport_Cost <= -1\n",
    "df_clean[target_variable] = np.log1p(df_clean[target_variable])\n",
    "\n",
    "# --- 💡💡💡 HERE IS THE NEW FIX 💡💡💡 ---\n",
    "# We must now drop any rows that *became* NaN after the log transform\n",
    "initial_rows = len(df_clean)\n",
    "df_clean = df_clean.dropna(subset=[target_variable])\n",
    "final_rows = len(df_clean)\n",
    "\n",
    "if initial_rows > final_rows:\n",
    "    print(f\"Dropped {initial_rows - final_rows} rows due to invalid log transform (e.g., Transport_Cost <= -1).\")\n",
    "# --- END OF FIX ---\n",
    "\n",
    "\n",
    "# --- 2b. Inspect the Target Variable Distribution ---\n",
    "print(\"\\n===== TARGET VARIABLE (Transport_Cost) SUMMARY =====\")\n",
    "print(df_clean[target_variable].describe())\n",
    "print(f\"\\nSkewness: {df_clean[target_variable].skew():.4f}\")\n",
    "\n",
    "# If you want to see the mean in both raw and original (unlogged) form:\n",
    "mean_logged = df_clean[target_variable].mean()\n",
    "print(f\"\\nMean (after log1p): {mean_logged:.4f}\")\n",
    "\n",
    "# Convert back to original scale for interpretation\n",
    "mean_original = np.expm1(mean_logged)\n",
    "print(f\"Approximate mean Transport_Cost (original scale): {mean_original:.2f}\")\n",
    "\n",
    "\n",
    "# --- 3. Create the Train-Test Split ---\n",
    "# Now X and y will be clean\n",
    "X = df_clean.drop(columns=target_variable)\n",
    "y = df_clean[target_variable]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- 4. Build the Preprocessing Pipelines ---\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('transformer', PowerTransformer(method='yeo-johnson'))\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# --- 5. Create the Master Preprocessor ---\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# --- 6. Apply Preprocessing (for inspection) ---\n",
    "# (This part is just for your own inspection, the model pipeline uses the raw X_train)\n",
    "try:\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "    # --- 7. Get Feature Names After Transformation (Optional, but helpful) ---\n",
    "    feature_names = numeric_features + \\\n",
    "                    preprocessor.named_transformers_['cat'] \\\n",
    "                                .named_steps['onehot'] \\\n",
    "                                .get_feature_names_out(categorical_features).tolist()\n",
    "    \n",
    "    X_train_processed = pd.DataFrame(X_train_processed, columns=feature_names, index=X_train.index)\n",
    "    X_test_processed = pd.DataFrame(X_test_processed, columns=feature_names, index=X_test.index)\n",
    "    \n",
    "    print(\"\\n--- Training Data Head (Processed) ---\")\n",
    "    display(X_train_processed.head())\n",
    "\n",
    "except Exception as e:\n",
    "    # This can fail if the split results in an empty dataframe (if all rows were bad)\n",
    "    print(f\"Could not process data, possibly too many NaNs dropped: {e}\")\n",
    "    print(\"Proceeding with NumPy arrays if possible.\")\n",
    "\n",
    "\n",
    "# --- 8. Final Check ---\n",
    "print(\"--- Preprocessing Complete ---\")\n",
    "print(f\"Original X_train shape: {X_train.shape}\")\n",
    "if 'X_train_processed' in locals():\n",
    "    print(f\"Processed X_train shape: {X_train_processed.shape}\")\n",
    "print(f\"\\nOriginal X_test shape: {X_test.shape}\")\n",
    "if 'X_test_processed' in locals():\n",
    "    print(f\"Processed X_test shape: {X_test_processed.shape}\")\n",
    "print(f\"\\ny_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ae0dbad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running 5-Fold Cross-Validation ---\n",
      "\n",
      "Model: Linear Regression\n",
      "  Avg. Log-Scale RMSE: 0.7662 (from 5 folds)\n",
      "\n",
      "Model: Random Forest\n",
      "  Avg. Log-Scale RMSE: 0.3931 (from 5 folds)\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001920 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 957\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002057 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Number of data points in the train set: 1749, number of used features: 48\n",
      "[LightGBM] [Info] Total Bins 957\n",
      "[LightGBM] [Info] Number of data points in the train set: 1749, number of used features: 48\n",
      "[LightGBM] [Info] Start training from score 6.551130\n",
      "[LightGBM] [Info] Start training from score 6.538262\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002201 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 957\n",
      "[LightGBM] [Info] Number of data points in the train set: 1749, number of used features: 48\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002133 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 960\n",
      "[LightGBM] [Info] Number of data points in the train set: 1748, number of used features: 48\n",
      "[LightGBM] [Info] Start training from score 6.533946\n",
      "[LightGBM] [Info] Start training from score 6.567700\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002782 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 959\n",
      "[LightGBM] [Info] Number of data points in the train set: 1749, number of used features: 48\n",
      "[LightGBM] [Info] Start training from score 6.576045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/MlLab/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/MlLab/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/MlLab/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/MlLab/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: LightGBM\n",
      "  Avg. Log-Scale RMSE: 0.3768 (from 5 folds)\n",
      "\n",
      "--- Validation Complete ---\n",
      "Lower RMSE is better. Choose the best model for final training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/MlLab/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- 1. Define Models to Test ---\n",
    "# We'll put them in a dictionary to loop through them easily\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'LightGBM': LGBMRegressor(random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "# --- 2. Set up K-Fold ---\n",
    "# We use the *original* X_train and y_train\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# --- 3. Loop, Validate, and Get Scores ---\n",
    "print(\"--- Running 5-Fold Cross-Validation ---\")\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    \n",
    "    # Create the FULL pipeline: Preprocessor -> Model\n",
    "    full_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor), # This is the 'preprocessor' from your last cell\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    # Run cross-validation\n",
    "    # We pass the RAW X_train and y_train\n",
    "    # 'cross_val_score' handles the fit/transform loop internally\n",
    "    cv_scores = cross_val_score(full_pipeline, \n",
    "                                X_train,  # <-- Raw training features\n",
    "                                y_train,  # <-- Raw training target\n",
    "                                cv=kf, \n",
    "                                scoring='neg_root_mean_squared_error',\n",
    "                                n_jobs=-1)\n",
    "    \n",
    "    # We take the absolute value of the negative RMSE\n",
    "    avg_rmse = np.abs(cv_scores).mean()\n",
    "    \n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    print(f\"  Avg. Log-Scale RMSE: {avg_rmse:.4f} (from 5 folds)\")\n",
    "\n",
    "print(\"\\n--- Validation Complete ---\")\n",
    "print(\"Lower RMSE is better. Choose the best model for final training.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MlLab)",
   "language": "python",
   "name": "mllab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
