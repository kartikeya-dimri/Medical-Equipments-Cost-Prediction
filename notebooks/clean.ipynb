{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad8d2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# core\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# data + plotting\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "from scipy.stats import zscore\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "# sklearn (preprocessing / pipeline / model selection / metrics)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# classical models (if you use them elsewhere)\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# gradient boosting / lightgbm / xgboost\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# utilities\n",
    "import joblib   # optional: save/load pipeline\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abbea68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4617a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "\n",
    "# 1️⃣ Load Data\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv('../data/train.csv')\n",
    "df.columns = df.columns.str.strip()\n",
    "display(df.head())\n",
    "print(f\"Initial data shape: {df.shape}\")\n",
    "\n",
    "# 2️⃣ Clean all string/object columns: strip spaces, replace blanks with NaN\n",
    "print(\"Cleaning string columns...\")\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    df[col] = df[col].astype(str).str.strip()\n",
    "    df[col] = df[col].replace({'': np.nan, 'nan': np.nan, 'NaN': np.nan})\n",
    "\n",
    "# 3️⃣ Normalize Yes/No columns to consistent \"Yes\"/\"No\"\n",
    "print(\"Normalizing Yes/No columns...\")\n",
    "yes_no_cols = ['CrossBorder_Shipping', 'Urgent_Shipping', 'Installation_Service',\n",
    "               'Fragile_Equipment', 'Rural_Hospital']\n",
    "for col in yes_no_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace({\n",
    "            'YES': 'Yes', 'yes': 'Yes', 'Y': 'Yes', 'y': 'Yes',\n",
    "            'NO': 'No', 'no': 'No', 'N': 'No', 'n': 'No'\n",
    "        })\n",
    "\n",
    "# 4️⃣ Convert date columns to datetime\n",
    "print(\"Converting date columns...\")\n",
    "df['Order_Placed_Date'] = pd.to_datetime(df['Order_Placed_Date'], errors='coerce')\n",
    "df['Delivery_Date'] = pd.to_datetime(df['Delivery_Date'], errors='coerce')\n",
    "\n",
    "# 5️⃣ Create new feature: Delivery_Days (difference in days)\n",
    "print(\"Engineering Delivery_Days feature...\")\n",
    "df['Delivery_Days'] = (df['Delivery_Date'] - df['Order_Placed_Date']).dt.days\n",
    "df['Delivery_Days'] = pd.to_numeric(df['Delivery_Days'], errors='coerce')\n",
    "\n",
    "# === ADDED: Date Feature Engineering ===\n",
    "print(\"Engineering more date features...\")\n",
    "df['Order_Month'] = df['Order_Placed_Date'].dt.month\n",
    "df['Order_Day_of_Week'] = df['Order_Placed_Date'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "df['Order_Is_Weekend'] = df['Order_Day_of_Week'].isin([5, 6])\n",
    "# === END ADDED ===\n",
    "\n",
    "# 6️⃣ (Original) delete initial date rows\n",
    "# df = df.dropna(subset=['Order_Placed_Date', 'Delivery_Date'])\n",
    "\n",
    "# 7️⃣ Drop exact duplicate rows\n",
    "print(\"Dropping duplicates...\")\n",
    "before = len(df)\n",
    "df = df.drop_duplicates()\n",
    "after = len(df)\n",
    "print(f\"Dropped {before - after} duplicate rows.\")\n",
    "\n",
    "# 8️⃣ Quick check after cleaning\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\" CLEANING & FEATURE ENGINEERING COMPLETE \")\n",
    "print(\"=\"*30)\n",
    "print(f\"After basic cleaning shape: {df.shape}\")\n",
    "\n",
    "print(\"\\nMissing values (raw count):\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "# === ADDED: Missing Value Percentage View ===\n",
    "print(\"\\nMissing values (percentage):\")\n",
    "missing_pct = (df.isna().sum() / len(df) * 100).sort_values(ascending=False)\n",
    "print(missing_pct[missing_pct > 0])\n",
    "# === END ADDED ===\n",
    "\n",
    "print(\"\\nDataFrame head:\")\n",
    "display(df.head())\n",
    "# print(df['Delivery_Days'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6847d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# 📊 START OF EXPLORATORY DATA ANALYSIS (EDA)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\" STARTING EDA \")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# 🔹 Define column lists\n",
    "num_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "# === ADDED: Exclude new date features from 'num_cols' for general stats ===\n",
    "date_num_features = ['Order_Month', 'Order_Day_of_Week', 'Delivery_Days']\n",
    "for col in ['Transport_Cost'] + date_num_features:\n",
    "    if col in num_cols:\n",
    "        num_cols.remove(col)\n",
    "# === END ADDED ===\n",
    "        \n",
    "cat_cols = df.select_dtypes(include='object').columns.tolist()\n",
    "# === ADDED: Add boolean 'Is_Weekend' to cat_cols for analysis ===\n",
    "if 'Order_Is_Weekend' in df.columns:\n",
    "    cat_cols.append('Order_Is_Weekend')\n",
    "# === END ADDED ===\n",
    "\n",
    "print(f\"Numeric features identified: {num_cols}\")\n",
    "print(f\"Categorical features identified: {cat_cols}\")\n",
    "print(f\"Date-derived features identified: {date_num_features}\")\n",
    "\n",
    "\n",
    "# === ADDED: 1. Target Variable Analysis (Transport_Cost) ===\n",
    "print(\"\\n===== 1. TARGET VARIABLE ANALYSIS: Transport_Cost =====\")\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Original Distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['Transport_Cost'], kde=True, bins=40)\n",
    "plt.title('Distribution of Transport_Cost (Original)')\n",
    "plt.xlabel('Transport_Cost')\n",
    "\n",
    "# Plot 2: Log-Transformed Distribution\n",
    "# We add 1 to handle potential zero values before logging\n",
    "plt.subplot(1, 2, 2)\n",
    "log_target = np.log1p(df['Transport_Cost'])\n",
    "sns.histplot(log_target, kde=True, bins=40, color='green')\n",
    "plt.title('Distribution of log(Transport_Cost + 1)')\n",
    "plt.xlabel('log(Transport_Cost + 1)')\n",
    "\n",
    "plt.suptitle('Target Variable Distribution Analysis', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "print(f\"Skewness of Transport_Cost: {df['Transport_Cost'].skew():.4f}\")\n",
    "print(f\"Skewness of log(Transport_Cost + 1): {log_target.skew():.4f}\")\n",
    "# === END ADDED ===\n",
    "\n",
    "\n",
    "print(\"\\n===== 2. NUMERIC FEATURE ANALYSIS =====\")\n",
    "print(\"===== BASIC NUMERIC STATISTICS =====\")\n",
    "if not num_cols:\n",
    "    print(\"No numeric columns found to describe (excluding target/dates).\")\n",
    "else:\n",
    "    display(df[num_cols].describe().T)\n",
    "\n",
    "    print(\"\\n===== SKEWNESS =====\")\n",
    "    display(df[num_cols].skew())\n",
    "\n",
    "# 🔹 Numeric distributions + boxplots\n",
    "# (Your original loop)\n",
    "# === MODIFIED: Added a check for empty list ===\n",
    "print(\"\\nGenerating numeric distribution plots...\")\n",
    "analysis_num_cols = num_cols + ['Delivery_Days'] # Add Delivery_Days back for plotting\n",
    "if 'Transport_Cost' not in analysis_num_cols:\n",
    "    analysis_num_cols.append('Transport_Cost') # Add Target back for plotting\n",
    "    \n",
    "for col in analysis_num_cols:\n",
    "    if col in df.columns:\n",
    "        plt.figure(figsize=(12,4))\n",
    "        \n",
    "        plt.subplot(1,2,1)\n",
    "        sns.histplot(df[col], kde=True, bins=30)\n",
    "        plt.title(f'{col} distribution')\n",
    "        \n",
    "        plt.subplot(1,2,2)\n",
    "        sns.boxplot(x=df[col])\n",
    "        plt.title(f'{col} boxplot')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Warning: Column '{col}' not found for plotting.\")\n",
    "\n",
    "\n",
    "print(\"\\n===== 3. CORRELATION ANALYSIS =====\")\n",
    "# 🔹 Correlation heatmap\n",
    "# (Your original code)\n",
    "plt.figure(figsize=(10,8))\n",
    "corr = df[num_cols + ['Transport_Cost', 'Delivery_Days']].corr()\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n===== 4. CATEGORICAL FEATURE ANALYSIS =====\")\n",
    "# 🔹 Categorical distributions\n",
    "# (Your original loop)\n",
    "print(\"\\nGenerating categorical distribution plots...\")\n",
    "high_cardinality_cols = []\n",
    "for col in cat_cols:\n",
    "    print(f\"\\n===== Column: {col} =====\")\n",
    "    print(df[col].value_counts(dropna=False))\n",
    "    \n",
    "    nunique = df[col].nunique()\n",
    "    if nunique > 20:\n",
    "        high_cardinality_cols.append(col)\n",
    "        print(f\"SKIPPING countplot for {col} (High Cardinality: {nunique} unique values)\")\n",
    "        continue\n",
    "        \n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.countplot(y=col, data=df, order=df[col].value_counts().index)\n",
    "    plt.title(f'Count of {col}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# === ADDED: 4a. High-Cardinality Column Summary ===\n",
    "print(\"\\n===== 4a. HIGH-CARDINALITY CATEGORICAL SUMMARY =====\")\n",
    "if high_cardinality_cols:\n",
    "    print(f\"High-cardinality features detected: {high_cardinality_cols}\")\n",
    "    for col in high_cardinality_cols:\n",
    "        print(f\"\\n--- Top 10 values for: {col} ---\")\n",
    "        print(df[col].value_counts(dropna=False).head(10))\n",
    "        print(f\"...and {df[col].nunique() - 10} other unique values.\")\n",
    "else:\n",
    "    print(\"No high-cardinality categorical features detected (threshold > 20).\")\n",
    "# === END ADDED ===\n",
    "\n",
    "\n",
    "print(\"\\n===== 5. BIVARIATE ANALYSIS (FEATURES vs. TARGET) =====\")\n",
    "# 🔹 Numeric features vs target\n",
    "# (Your original loop)\n",
    "print(\"\\nGenerating numeric features vs. Transport_Cost...\")\n",
    "for col in num_cols + ['Delivery_Days']:\n",
    "    if col in df.columns:\n",
    "        plt.figure(figsize=(6,4))\n",
    "        sns.scatterplot(x=df[col], y=df['Transport_Cost'])\n",
    "        plt.title(f'{col} vs Transport_Cost')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# 🔹 Categorical features vs target (low-cardinality)\n",
    "# (Your original loop)\n",
    "print(\"\\nGenerating categorical features vs. Transport_Cost...\")\n",
    "for col in cat_cols:\n",
    "    if col in df.columns and df[col].nunique() < 20:\n",
    "        plt.figure(figsize=(10,4))\n",
    "        sns.boxplot(x=col, y='Transport_Cost', data=df)\n",
    "        plt.title(f'{col} vs Transport_Cost')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# === ADDED: 5a. Date-Derived Features vs. Target ===\n",
    "print(\"\\nGenerating date-derived features vs. Transport_Cost...\")\n",
    "date_features_to_plot = ['Order_Month', 'Order_Day_of_Week', 'Order_Is_Weekend']\n",
    "for col in date_features_to_plot:\n",
    "    if col in df.columns:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        sns.boxplot(x=col, y='Transport_Cost', data=df)\n",
    "        plt.title(f'{col} vs Transport_Cost')\n",
    "        if col == 'Order_Day_of_Week':\n",
    "            plt.xticks(ticks=range(7), labels=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "# === END ADDED ===\n",
    "\n",
    "\n",
    "print(\"\\n===== 6. OUTLIER DETECTION =====\")\n",
    "# 🔹 Outlier detection (Z-score)\n",
    "# (Your original code)\n",
    "# === MODIFIED: Added nan_policy='omit' to handle missing values gracefully ===\n",
    "try:\n",
    "    z_scores = df[num_cols + ['Transport_Cost', 'Delivery_Days']].apply(lambda x: zscore(x, nan_policy='omit'))\n",
    "    outliers = (abs(z_scores) > 3).sum()\n",
    "    print(\"\\n===== NUMBER OF OUTLIERS PER COLUMN (Z-score > 3) =====\")\n",
    "    print(outliers[outliers > 0].sort_values(ascending=False))\n",
    "except ValueError as e:\n",
    "    print(f\"Could not calculate Z-scores, likely due to all-NaN column. Error: {e}\")\n",
    "# === END MODIFIED ===\n",
    "\n",
    "\n",
    "print(\"\\n===== 7. MISSING VALUE VISUALIZATION =====\")\n",
    "# 🔹 Missing value visualization\n",
    "# (Your original code)\n",
    "print(\"\\nGenerating missing value matrix...\")\n",
    "msno.matrix(df)\n",
    "plt.title('Missing Value Matrix')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nGenerating missing value bar chart...\")\n",
    "msno.bar(df)\n",
    "plt.title('Missing Value Bar Chart')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\" EDA COMPLETE \")\n",
    "print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "61ad723c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing script started...\n",
      "Filtered 2267 rows with bad data (negative cost or delivery days).\n",
      "Features for modeling: ['Supplier_Reliability', 'Equipment_Type', 'Equipment_Value', 'Base_Transport_Fee', 'CrossBorder_Shipping', 'Urgent_Shipping', 'Installation_Service', 'Transport_Method', 'Fragile_Equipment', 'Hospital_Info', 'Rural_Hospital', 'Delivery_Days', 'Order_Month', 'Order_Day_of_Week', 'Order_Is_Weekend', 'Equipment_Volume']\n",
      "Baseline RMSE (test, log-space): 1.7725\n",
      "Baseline RMSE (test, original-scale): 625946.6068\n",
      "Training set shape: (2186, 16)\n",
      "Test set shape: (547, 16)\n",
      "\n",
      "Fitting preprocessor on X_train...\n",
      "Preprocessing complete.\n",
      "Processed X_train shape: (2186, 48)\n",
      "Processed X_test shape: (547, 48)\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocessing script started...\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: PRE-SPLIT (Data Cleaning & Feature Engineering)\n",
    "# These actions are applied to the whole dataset before splitting.\n",
    "# ==============================================================================\n",
    "\n",
    "# 1. Filter Bad Data\n",
    "# EDA Finding: We found impossible values like Transport_Cost < 0 and Delivery_Days < 0.\n",
    "# Action: Remove these rows entirely.\n",
    "initial_rows = len(df)\n",
    "df = df[df['Transport_Cost'] >= 0]\n",
    "df = df[df['Delivery_Days'] >= 0]\n",
    "print(f\"Filtered {initial_rows - len(df)} rows with bad data (negative cost or delivery days).\")\n",
    "\n",
    "# 2. Feature Engineering\n",
    "# EDA Finding: Equipment_Height & Equipment_Width were highly correlated (0.77).\n",
    "# Action: Combine them into a single 'Equipment_Volume' feature.\n",
    "df['Equipment_Volume'] = df['Equipment_Height'] * df['Equipment_Width']\n",
    "\n",
    "# 3. Log-Transform Skewed Features\n",
    "# EDA Finding: Equipment_Value (skew=24) and our new Equipment_Volume\n",
    "# (derived from skewed features) are extremely right-skewed.\n",
    "# Action: Apply np.log1p to normalize them.\n",
    "df['Equipment_Value'] = np.log1p(df['Equipment_Value'])\n",
    "df['Equipment_Volume'] = np.log1p(df['Equipment_Volume'])\n",
    "\n",
    "# 4. Define Target (y) and Features (X)\n",
    "# EDA Finding: Target 'Transport_Cost' is extremely skewed (skew=30).\n",
    "# Action: Use np.log1p on the target. We will predict the log, then convert back.\n",
    "y = np.log1p(df['Transport_Cost']) # this y is in log-space\n",
    "\n",
    "# Action: Define X by dropping the target, original engineered columns, \n",
    "# and high-cardinality/redundant/ID columns identified in the EDA.\n",
    "X = df.drop(columns=[\n",
    "    # Target\n",
    "    'Transport_Cost',\n",
    "    \n",
    "    # Replaced by Equipment_Volume\n",
    "    'Equipment_Height',\n",
    "    'Equipment_Width',\n",
    "    \n",
    "    # Redundant (corr 0.90 with Value)\n",
    "    'Equipment_Weight',\n",
    "    \n",
    "    # High-Cardinality IDs / Unused\n",
    "    'Hospital_Id',\n",
    "    'Supplier_Name',\n",
    "    'Hospital_Location',\n",
    "    \n",
    "    # Replaced by date features\n",
    "    'Order_Placed_Date',\n",
    "    'Delivery_Date'\n",
    "])\n",
    "\n",
    "print(f\"Features for modeling: {X.columns.tolist()}\")\n",
    "\n",
    "# 5. Train-Test Split\n",
    "# Action: Split the data into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "train_mean = y_train.mean()\n",
    "\n",
    "# 1️⃣ Baseline RMSE on the training set (log-space)\n",
    "# y_train_pred_baseline = np.full_like(y_train, train_mean)\n",
    "# baseline_rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred_baseline))\n",
    "# print(f\"Baseline RMSE (train, log-space): {baseline_rmse_train:.4f}\")\n",
    "\n",
    "# 2️⃣ Baseline RMSE on the test set (using train mean as predictor) — log-space\n",
    "y_test_pred_baseline = np.full_like(y_test, train_mean)\n",
    "baseline_rmse_test_log = np.sqrt(mean_squared_error(y_test, y_test_pred_baseline))\n",
    "print(f\"Baseline RMSE (test, log-space): {baseline_rmse_test_log:.4f}\")\n",
    "\n",
    "# 3️⃣ Baseline RMSE in original (Transport_Cost) scale\n",
    "y_test_actual_orig = np.expm1(y_test)\n",
    "y_test_baseline_pred_orig = np.expm1(y_test_pred_baseline)\n",
    "baseline_rmse_test_orig = np.sqrt(mean_squared_error(y_test_actual_orig, y_test_baseline_pred_orig))\n",
    "print(f\"Baseline RMSE (test, original-scale): {baseline_rmse_test_orig:.4f}\")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: POST-SPLIT (Pipelines & ColumnTransformer)\n",
    "# This prevents data leakage. We FIT on X_train, then TRANSFORM X_train and X_test.\n",
    "# ==============================================================================\n",
    "\n",
    "# 1. Define Feature Lists\n",
    "# Action: Separate our final columns into numeric and categorical lists.\n",
    "\n",
    "numeric_features = [\n",
    "    'Supplier_Reliability',\n",
    "    'Equipment_Value',      # Already log-transformed\n",
    "    'Base_Transport_Fee',\n",
    "    'Delivery_Days',\n",
    "    'Equipment_Volume'      # Already log-transformed\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'Equipment_Type',\n",
    "    'CrossBorder_Shipping',\n",
    "    'Urgent_Shipping',\n",
    "    'Installation_Service',\n",
    "    'Transport_Method',\n",
    "    'Fragile_Equipment',\n",
    "    'Hospital_Info',\n",
    "    'Rural_Hospital',\n",
    "    'Order_Month',\n",
    "    'Order_Day_of_Week',\n",
    "    'Order_Is_Weekend'\n",
    "]\n",
    "\n",
    "# 2. Create the Numeric Pipeline\n",
    "# EDA Finding: Numeric features had missing values (e.g., Supplier_Reliability)\n",
    "# and were on different scales.\n",
    "# Action: Impute missing values with the median (robust to outliers)\n",
    "# and then scale all features.\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# 3. Create the Categorical Pipeline\n",
    "# EDA Finding: Categorical features had missing values (e.g., Transport_Method,\n",
    "# Rural_Hospital) and need to be converted to numbers.\n",
    "# Action: Impute missing values with the most frequent value and then\n",
    "# one-hot encode. 'handle_unknown='ignore'' ensures our model doesn't\n",
    "# crash if it sees a new category in the test data.\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# 4. Create the Full Preprocessor\n",
    "# Action: Combine the numeric and categorical pipelines using ColumnTransformer.\n",
    "# This single object will handle all preprocessing for us.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'  # Drop any columns we didn't explicitly list\n",
    ")\n",
    "\n",
    "# 5. Apply the Preprocessor\n",
    "# Action: FIT the preprocessor on X_train ONLY (to learn medians, modes, etc.)\n",
    "# and then TRANSFORM both X_train and X_test.\n",
    "# This gives us our final, model-ready datasets.\n",
    "\n",
    "print(\"\\nFitting preprocessor on X_train...\")\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test) # what if we have missig data in testing set?\n",
    "\n",
    "print(\"Preprocessing complete.\")\n",
    "print(f\"Processed X_train shape: {X_train_processed.shape}\")\n",
    "print(f\"Processed X_test shape: {X_test_processed.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0a25e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ae0dbad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression 5-Fold Avg. RMSE (log-space): 0.6712\n",
      "Test RMSE (log-space)      : 0.7249\n",
      "Test RMSE (original scale) : 622505.75\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----- 1) Set up 5-Fold -----\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# ----- 2) Create the pipeline -----\n",
    "lr_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),  # your ColumnTransformer\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "# ----- 3) Run 5-Fold Cross-Validation -----\n",
    "cv_scores = cross_val_score(\n",
    "    lr_pipeline,\n",
    "    X_train,       # raw training features\n",
    "    y_train,       # log-transformed target\n",
    "    cv=kf,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "avg_rmse = np.abs(cv_scores).mean()\n",
    "print(f\"Linear Regression 5-Fold Avg. RMSE (log-space): {avg_rmse:.4f}\")\n",
    "\n",
    "# ----- 4) Fit final model on full training data -----\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "# print(\"Linear Regression final model trained on full training set.\")\n",
    "\n",
    "# ----- 5) Predict on test set -----\n",
    "y_test_pred_log = lr_pipeline.predict(X_test)\n",
    "y_test_pred_orig = np.expm1(y_test_pred_log)  # back to original scale\n",
    "\n",
    "rmse_test_log = np.sqrt(np.mean((y_test - y_test_pred_log)**2))\n",
    "rmse_test_orig = np.sqrt(np.mean((np.expm1(y_test) - y_test_pred_orig)**2))\n",
    "\n",
    "print(f\"Test RMSE (log-space)      : {rmse_test_log:.4f}\")  \n",
    "print(f\"Test RMSE (original scale) : {rmse_test_orig:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fe61007a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best polynomial degree: 2\n",
      "Best CV RMSE (log-space): 0.5372\n",
      "Polynomial Regression final model trained on full training set.\n",
      "Test RMSE (log-space)      : 0.5201\n",
      "Test RMSE (original scale) : 482129.09\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----- 1) Set up 5-Fold -----\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# ----- 2) Create the pipeline -----\n",
    "poly_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),         # your ColumnTransformer\n",
    "    ('poly', PolynomialFeatures()),         # will tune degree\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "# ----- 3) Set up GridSearch for hyperparameter tuning -----\n",
    "param_grid = {\n",
    "    'poly__degree': [2,3]   # you can expand to 5 if dataset is small\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    poly_pipeline,\n",
    "    param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=kf,\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "# ----- 4) Run GridSearch -----\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best degree\n",
    "best_degree = grid_search.best_params_['poly__degree']\n",
    "best_rmse = -grid_search.best_score_\n",
    "print(f\"Best polynomial degree: {best_degree}\")\n",
    "print(f\"Best CV RMSE (log-space): {best_rmse:.4f}\")\n",
    "\n",
    "# ----- 5) Fit final model on full training data -----\n",
    "final_poly_model = grid_search.best_estimator_\n",
    "final_poly_model.fit(X_train, y_train)\n",
    "print(\"Polynomial Regression final model trained on full training set.\")\n",
    "\n",
    "# ----- 6) Predict on test set -----\n",
    "y_test_pred_log = final_poly_model.predict(X_test)\n",
    "y_test_pred_orig = np.expm1(y_test_pred_log)  # back to original scale\n",
    "\n",
    "rmse_test_log = np.sqrt(np.mean((y_test - y_test_pred_log)**2))\n",
    "rmse_test_orig = np.sqrt(np.mean((np.expm1(y_test) - y_test_pred_orig)**2))\n",
    "\n",
    "print(f\"Test RMSE (log-space)      : {rmse_test_log:.4f}\")\n",
    "print(f\"Test RMSE (original scale) : {rmse_test_orig:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2959af42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'poly__degree': 3, 'ridge__alpha': 100}\n",
      "Best CV RMSE (log-space): 0.4981973090176825\n",
      "Test RMSE (log-space)      : 0.5278\n",
      "Test RMSE (original scale) : 665702.93\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----- 1) Create the pipeline -----\n",
    "ridge_poly_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),  # your ColumnTransformer\n",
    "    ('poly', PolynomialFeatures()),  # polynomial expansion\n",
    "    ('ridge', Ridge())               # ridge regression\n",
    "])\n",
    "\n",
    "# ----- 2) Set hyperparameter grid -----\n",
    "param_grid = {\n",
    "    'poly__degree': [3],      # try degrees 1, 2, 3, 4, 5\n",
    "    'ridge__alpha': [ 100]  # try different regularization strengths\n",
    "}\n",
    "\n",
    "# ----- 3) Grid Search with 5-Fold CV -----\n",
    "grid_search = GridSearchCV(\n",
    "    ridge_poly_pipeline,\n",
    "    param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=5,\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "# ----- 4) Fit on training data -----\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# ----- 5) Best hyperparameters -----\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best CV RMSE (log-space):\", -grid_search.best_score_)\n",
    "\n",
    "# ----- 6) Predict on test set -----\n",
    "y_test_pred_log = grid_search.predict(X_test)\n",
    "y_test_pred_orig = np.expm1(y_test_pred_log)\n",
    "\n",
    "rmse_test_log = np.sqrt(np.mean((y_test - y_test_pred_log)**2))\n",
    "rmse_test_orig = np.sqrt(np.mean((np.expm1(y_test) - y_test_pred_orig)**2))\n",
    "\n",
    "print(f\"Test RMSE (log-space)      : {rmse_test_log:.4f}\")\n",
    "print(f\"Test RMSE (original scale) : {rmse_test_orig:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8ba86e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lasso__alpha': 0.01, 'poly__degree': 3}\n",
      "Best CV RMSE (log-space): 0.441190042720711\n",
      "Test RMSE (log-space)      : 0.4473\n",
      "Test RMSE (original scale) : 455988.25\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----- 1) Create the pipeline -----\n",
    "lasso_poly_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),  # your ColumnTransformer\n",
    "    ('poly', PolynomialFeatures()),  # polynomial expansion\n",
    "    ('lasso', Lasso(max_iter=10000)) # Lasso regression\n",
    "])\n",
    "\n",
    "# ----- 2) Set hyperparameter grid -----\n",
    "param_grid = {\n",
    "    'poly__degree': [3],       # try degrees 1, 2, 3\n",
    "    'lasso__alpha': [0.001, 0.01, 0.1, 1, 10]  # regularization strengths\n",
    "}\n",
    "\n",
    "# ----- 3) Grid Search with 5-Fold CV -----\n",
    "grid_search = GridSearchCV(\n",
    "    lasso_poly_pipeline,\n",
    "    param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# ----- 4) Fit on training data -----\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# ----- 5) Best hyperparameters -----\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best CV RMSE (log-space):\", -grid_search.best_score_)\n",
    "\n",
    "# ----- 6) Predict on test set -----\n",
    "y_test_pred_log = grid_search.predict(X_test)\n",
    "y_test_pred_orig = np.expm1(y_test_pred_log)\n",
    "\n",
    "rmse_test_log = np.sqrt(np.mean((y_test - y_test_pred_log)**2))\n",
    "rmse_test_orig = np.sqrt(np.mean((np.expm1(y_test) - y_test_pred_orig)**2))\n",
    "\n",
    "print(f\"Test RMSE (log-space)      : {rmse_test_log:.4f}\")\n",
    "print(f\"Test RMSE (original scale) : {rmse_test_orig:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "099a12f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting GridSearchCV for ElasticNet + PolynomialFeatures ...\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[CV] END enet__alpha=0.1, enet__l1_ratio=0.01, poly__degree=3; total time=  21.2s\n",
      "[CV] END enet__alpha=0.1, enet__l1_ratio=0.01, poly__degree=3; total time=  21.9s\n",
      "[CV] END enet__alpha=0.1, enet__l1_ratio=0.01, poly__degree=3; total time=  23.0s\n",
      "[CV] END enet__alpha=0.1, enet__l1_ratio=0.01, poly__degree=3; total time=  19.7s\n",
      "[CV] END enet__alpha=0.1, enet__l1_ratio=0.01, poly__degree=3; total time=  25.5s\n",
      "[CV] END enet__alpha=0.1, enet__l1_ratio=0.1, poly__degree=3; total time=   5.9s\n",
      "[CV] END enet__alpha=0.1, enet__l1_ratio=0.1, poly__degree=3; total time=   6.6s\n",
      "[CV] END enet__alpha=0.1, enet__l1_ratio=0.1, poly__degree=3; total time=   6.7s\n",
      "[CV] END enet__alpha=0.1, enet__l1_ratio=0.1, poly__degree=3; total time=   6.0s\n",
      "[CV] END enet__alpha=0.1, enet__l1_ratio=0.1, poly__degree=3; total time=   6.7s\n",
      "[CV] END enet__alpha=0.1, enet__l1_ratio=0.2, poly__degree=3; total time=   4.8s\n",
      "[CV] END enet__alpha=0.1, enet__l1_ratio=0.2, poly__degree=3; total time=   5.1s\n",
      "[CV] END enet__alpha=0.1, enet__l1_ratio=0.2, poly__degree=3; total time=   4.8s\n",
      "[CV] END enet__alpha=0.1, enet__l1_ratio=0.2, poly__degree=3; total time=   4.5s\n",
      "[CV] END enet__alpha=0.1, enet__l1_ratio=0.2, poly__degree=3; total time=   4.4s\n",
      "[CV] END enet__alpha=0.1, enet__l1_ratio=0.5, poly__degree=3; total time=   1.1s\n",
      "[CV] END enet__alpha=0.1, enet__l1_ratio=0.5, poly__degree=3; total time=   1.0s\n",
      "[CV] END enet__alpha=0.1, enet__l1_ratio=0.5, poly__degree=3; total time=   1.1s\n",
      "[CV] END enet__alpha=0.1, enet__l1_ratio=0.5, poly__degree=3; total time=   1.2s\n",
      "[CV] END enet__alpha=0.1, enet__l1_ratio=0.5, poly__degree=3; total time=   1.9s\n",
      "[CV] END enet__alpha=0.1, enet__l1_ratio=0.8, poly__degree=3; total time=   0.5s\n",
      "[CV] END enet__alpha=0.1, enet__l1_ratio=0.8, poly__degree=3; total time=   0.4s\n",
      "[CV] END enet__alpha=0.1, enet__l1_ratio=0.8, poly__degree=3; total time=   0.5s\n",
      "[CV] END enet__alpha=0.1, enet__l1_ratio=0.8, poly__degree=3; total time=   0.6s\n",
      "[CV] END enet__alpha=0.1, enet__l1_ratio=0.8, poly__degree=3; total time=   0.5s\n",
      "[CV] END .enet__alpha=1, enet__l1_ratio=0.01, poly__degree=3; total time=   1.7s\n",
      "[CV] END .enet__alpha=1, enet__l1_ratio=0.01, poly__degree=3; total time=   1.5s\n",
      "[CV] END .enet__alpha=1, enet__l1_ratio=0.01, poly__degree=3; total time=   1.5s\n",
      "[CV] END .enet__alpha=1, enet__l1_ratio=0.01, poly__degree=3; total time=   1.5s\n",
      "[CV] END .enet__alpha=1, enet__l1_ratio=0.01, poly__degree=3; total time=   1.5s\n",
      "[CV] END ..enet__alpha=1, enet__l1_ratio=0.1, poly__degree=3; total time=   0.6s\n",
      "[CV] END ..enet__alpha=1, enet__l1_ratio=0.1, poly__degree=3; total time=   0.6s\n",
      "[CV] END ..enet__alpha=1, enet__l1_ratio=0.1, poly__degree=3; total time=   0.6s\n",
      "[CV] END ..enet__alpha=1, enet__l1_ratio=0.1, poly__degree=3; total time=   0.6s\n",
      "[CV] END ..enet__alpha=1, enet__l1_ratio=0.1, poly__degree=3; total time=   0.6s\n",
      "[CV] END ..enet__alpha=1, enet__l1_ratio=0.2, poly__degree=3; total time=   0.4s\n",
      "[CV] END ..enet__alpha=1, enet__l1_ratio=0.2, poly__degree=3; total time=   0.4s\n",
      "[CV] END ..enet__alpha=1, enet__l1_ratio=0.2, poly__degree=3; total time=   0.4s\n",
      "[CV] END ..enet__alpha=1, enet__l1_ratio=0.2, poly__degree=3; total time=   0.4s\n",
      "[CV] END ..enet__alpha=1, enet__l1_ratio=0.2, poly__degree=3; total time=   0.4s\n",
      "[CV] END ..enet__alpha=1, enet__l1_ratio=0.5, poly__degree=3; total time=   0.3s\n",
      "[CV] END ..enet__alpha=1, enet__l1_ratio=0.5, poly__degree=3; total time=   0.3s\n",
      "[CV] END ..enet__alpha=1, enet__l1_ratio=0.5, poly__degree=3; total time=   0.3s\n",
      "[CV] END ..enet__alpha=1, enet__l1_ratio=0.5, poly__degree=3; total time=   0.3s\n",
      "[CV] END ..enet__alpha=1, enet__l1_ratio=0.5, poly__degree=3; total time=   0.3s\n",
      "[CV] END ..enet__alpha=1, enet__l1_ratio=0.8, poly__degree=3; total time=   0.2s\n",
      "[CV] END ..enet__alpha=1, enet__l1_ratio=0.8, poly__degree=3; total time=   0.3s\n",
      "[CV] END ..enet__alpha=1, enet__l1_ratio=0.8, poly__degree=3; total time=   0.2s\n",
      "[CV] END ..enet__alpha=1, enet__l1_ratio=0.8, poly__degree=3; total time=   0.2s\n",
      "[CV] END ..enet__alpha=1, enet__l1_ratio=0.8, poly__degree=3; total time=   0.3s\n",
      "[CV] END enet__alpha=10, enet__l1_ratio=0.01, poly__degree=3; total time=   0.3s\n",
      "[CV] END enet__alpha=10, enet__l1_ratio=0.01, poly__degree=3; total time=   0.3s\n",
      "[CV] END enet__alpha=10, enet__l1_ratio=0.01, poly__degree=3; total time=   0.3s\n",
      "[CV] END enet__alpha=10, enet__l1_ratio=0.01, poly__degree=3; total time=   0.3s\n",
      "[CV] END enet__alpha=10, enet__l1_ratio=0.01, poly__degree=3; total time=   0.3s\n",
      "[CV] END .enet__alpha=10, enet__l1_ratio=0.1, poly__degree=3; total time=   0.2s\n",
      "[CV] END .enet__alpha=10, enet__l1_ratio=0.1, poly__degree=3; total time=   0.2s\n",
      "[CV] END .enet__alpha=10, enet__l1_ratio=0.1, poly__degree=3; total time=   0.2s\n",
      "[CV] END .enet__alpha=10, enet__l1_ratio=0.1, poly__degree=3; total time=   0.2s\n",
      "[CV] END .enet__alpha=10, enet__l1_ratio=0.1, poly__degree=3; total time=   0.2s\n",
      "[CV] END .enet__alpha=10, enet__l1_ratio=0.2, poly__degree=3; total time=   0.2s\n",
      "[CV] END .enet__alpha=10, enet__l1_ratio=0.2, poly__degree=3; total time=   0.2s\n",
      "[CV] END .enet__alpha=10, enet__l1_ratio=0.2, poly__degree=3; total time=   0.2s\n",
      "[CV] END .enet__alpha=10, enet__l1_ratio=0.2, poly__degree=3; total time=   0.2s\n",
      "[CV] END .enet__alpha=10, enet__l1_ratio=0.2, poly__degree=3; total time=   0.2s\n",
      "[CV] END .enet__alpha=10, enet__l1_ratio=0.5, poly__degree=3; total time=   0.2s\n",
      "[CV] END .enet__alpha=10, enet__l1_ratio=0.5, poly__degree=3; total time=   0.2s\n",
      "[CV] END .enet__alpha=10, enet__l1_ratio=0.5, poly__degree=3; total time=   0.2s\n",
      "[CV] END .enet__alpha=10, enet__l1_ratio=0.5, poly__degree=3; total time=   0.2s\n",
      "[CV] END .enet__alpha=10, enet__l1_ratio=0.5, poly__degree=3; total time=   0.2s\n",
      "[CV] END .enet__alpha=10, enet__l1_ratio=0.8, poly__degree=3; total time=   0.2s\n",
      "[CV] END .enet__alpha=10, enet__l1_ratio=0.8, poly__degree=3; total time=   0.2s\n",
      "[CV] END .enet__alpha=10, enet__l1_ratio=0.8, poly__degree=3; total time=   0.2s\n",
      "[CV] END .enet__alpha=10, enet__l1_ratio=0.8, poly__degree=3; total time=   0.2s\n",
      "[CV] END .enet__alpha=10, enet__l1_ratio=0.8, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=100, enet__l1_ratio=0.01, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=100, enet__l1_ratio=0.01, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=100, enet__l1_ratio=0.01, poly__degree=3; total time=   0.3s\n",
      "[CV] END enet__alpha=100, enet__l1_ratio=0.01, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=100, enet__l1_ratio=0.01, poly__degree=3; total time=   0.3s\n",
      "[CV] END enet__alpha=100, enet__l1_ratio=0.1, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=100, enet__l1_ratio=0.1, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=100, enet__l1_ratio=0.1, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=100, enet__l1_ratio=0.1, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=100, enet__l1_ratio=0.1, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=100, enet__l1_ratio=0.2, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=100, enet__l1_ratio=0.2, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=100, enet__l1_ratio=0.2, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=100, enet__l1_ratio=0.2, poly__degree=3; total time=   0.3s\n",
      "[CV] END enet__alpha=100, enet__l1_ratio=0.2, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=100, enet__l1_ratio=0.5, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=100, enet__l1_ratio=0.5, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=100, enet__l1_ratio=0.5, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=100, enet__l1_ratio=0.5, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=100, enet__l1_ratio=0.5, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=100, enet__l1_ratio=0.8, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=100, enet__l1_ratio=0.8, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=100, enet__l1_ratio=0.8, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=100, enet__l1_ratio=0.8, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=100, enet__l1_ratio=0.8, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=1000, enet__l1_ratio=0.01, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=1000, enet__l1_ratio=0.01, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=1000, enet__l1_ratio=0.01, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=1000, enet__l1_ratio=0.01, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=1000, enet__l1_ratio=0.01, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=1000, enet__l1_ratio=0.1, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=1000, enet__l1_ratio=0.1, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=1000, enet__l1_ratio=0.1, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=1000, enet__l1_ratio=0.1, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=1000, enet__l1_ratio=0.1, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=1000, enet__l1_ratio=0.2, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=1000, enet__l1_ratio=0.2, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=1000, enet__l1_ratio=0.2, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=1000, enet__l1_ratio=0.2, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=1000, enet__l1_ratio=0.2, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=1000, enet__l1_ratio=0.5, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=1000, enet__l1_ratio=0.5, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=1000, enet__l1_ratio=0.5, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=1000, enet__l1_ratio=0.5, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=1000, enet__l1_ratio=0.5, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=1000, enet__l1_ratio=0.8, poly__degree=3; total time=   0.2s\n",
      "[CV] END enet__alpha=1000, enet__l1_ratio=0.8, poly__degree=3; total time=   0.3s\n",
      "[CV] END enet__alpha=1000, enet__l1_ratio=0.8, poly__degree=3; total time=   0.3s\n",
      "[CV] END enet__alpha=1000, enet__l1_ratio=0.8, poly__degree=3; total time=   0.3s\n",
      "[CV] END enet__alpha=1000, enet__l1_ratio=0.8, poly__degree=3; total time=   0.2s\n",
      "\n",
      "Best hyperparameters: {'enet__alpha': 0.1, 'enet__l1_ratio': 0.1, 'poly__degree': 3}\n",
      "Best CV RMSE (log-space): 0.4550\n",
      "\n",
      "Test RMSE (log-space)      : 0.4607\n",
      "Test RMSE (original scale) : 431894.82\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----- 1) CV splitter -----\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# ----- 2) Pipeline: preprocessor -> poly -> elastic net -----\n",
    "enet_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),                       # your ColumnTransformer\n",
    "    ('poly', PolynomialFeatures(include_bias=False)),     # polynomial expansion (tune degree)\n",
    "    ('enet', ElasticNet(max_iter=20000, random_state=42)) # ElasticNet regression\n",
    "])\n",
    "\n",
    "# ----- 3) Hyperparameter grid -----\n",
    "param_grid = {\n",
    "    'poly__degree': [3],                      # try degrees 1..3 (increase carefully)\n",
    "    'enet__alpha': [0.1, 1, 10,100,1000],   # regularization strengths\n",
    "    'enet__l1_ratio': [0.01, 0.1,0.2, 0.5, 0.8]               # mix between L1 (1.0) and L2 (0.0)\n",
    "}\n",
    "\n",
    "# ----- 4) GridSearchCV (use n_jobs=1 in notebooks to avoid multiprocessing cwd issues) -----\n",
    "grid_search = GridSearchCV(\n",
    "    enet_pipeline,\n",
    "    param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=kf,\n",
    "    n_jobs=1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# ----- 5) Run grid search -----\n",
    "print(\"Starting GridSearchCV for ElasticNet + PolynomialFeatures ...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# ----- 6) Best params & CV score -----\n",
    "best_params = grid_search.best_params_\n",
    "best_cv_rmse = -grid_search.best_score_\n",
    "print(f\"\\nBest hyperparameters: {best_params}\")\n",
    "print(f\"Best CV RMSE (log-space): {best_cv_rmse:.4f}\")\n",
    "\n",
    "# ----- 7) Final model (best estimator) -----\n",
    "final_enet = grid_search.best_estimator_\n",
    "\n",
    "# ----- 8) Evaluate on test set -----\n",
    "y_test_pred_log = final_enet.predict(X_test)\n",
    "y_test_pred_orig = np.expm1(y_test_pred_log)\n",
    "\n",
    "rmse_test_log = np.sqrt(mean_squared_error(y_test, y_test_pred_log))\n",
    "rmse_test_orig = np.sqrt(mean_squared_error(np.expm1(y_test), y_test_pred_orig))\n",
    "\n",
    "print(f\"\\nTest RMSE (log-space)      : {rmse_test_log:.4f}\")\n",
    "print(f\"Test RMSE (original scale) : {rmse_test_orig:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "21e282f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting GridSearchCV for XGBoost...\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END xgb__learning_rate=0.01, xgb__max_depth=3, xgb__n_estimators=100, xgb__subsample=0.8; total time=   0.1s\n",
      "[CV] END xgb__learning_rate=0.01, xgb__max_depth=3, xgb__n_estimators=100, xgb__subsample=0.8; total time=   0.1s\n",
      "[CV] END xgb__learning_rate=0.01, xgb__max_depth=3, xgb__n_estimators=100, xgb__subsample=0.8; total time=   0.1s\n",
      "[CV] END xgb__learning_rate=0.01, xgb__max_depth=3, xgb__n_estimators=100, xgb__subsample=0.8; total time=   0.1s\n",
      "[CV] END xgb__learning_rate=0.01, xgb__max_depth=3, xgb__n_estimators=100, xgb__subsample=0.8; total time=   0.1s\n",
      "[CV] END xgb__learning_rate=0.01, xgb__max_depth=3, xgb__n_estimators=100, xgb__subsample=1.0; total time=   0.1s\n",
      "[CV] END xgb__learning_rate=0.01, xgb__max_depth=3, xgb__n_estimators=100, xgb__subsample=1.0; total time=   0.1s\n",
      "[CV] END xgb__learning_rate=0.01, xgb__max_depth=3, xgb__n_estimators=100, xgb__subsample=1.0; total time=   0.1s\n",
      "[CV] END xgb__learning_rate=0.01, xgb__max_depth=3, xgb__n_estimators=100, xgb__subsample=1.0; total time=   0.1s\n",
      "[CV] END xgb__learning_rate=0.01, xgb__max_depth=3, xgb__n_estimators=100, xgb__subsample=1.0; total time=   0.1s\n",
      "[CV] END xgb__learning_rate=0.01, xgb__max_depth=3, xgb__n_estimators=300, xgb__subsample=0.8; total time=   0.3s\n",
      "[CV] END xgb__learning_rate=0.01, xgb__max_depth=3, xgb__n_estimators=300, xgb__subsample=0.8; total time=   0.3s\n",
      "[CV] END xgb__learning_rate=0.01, xgb__max_depth=3, xgb__n_estimators=300, xgb__subsample=0.8; total time=   0.3s\n",
      "[CV] END xgb__learning_rate=0.01, xgb__max_depth=3, xgb__n_estimators=300, xgb__subsample=0.8; total time=   0.3s\n",
      "[CV] END xgb__learning_rate=0.01, xgb__max_depth=3, xgb__n_estimators=300, xgb__subsample=0.8; total time=   0.3s\n",
      "[CV] END xgb__learning_rate=0.01, xgb__max_depth=3, xgb__n_estimators=300, xgb__subsample=1.0; total time=   0.3s\n",
      "[CV] END xgb__learning_rate=0.01, xgb__max_depth=3, xgb__n_estimators=300, xgb__subsample=1.0; total time=   0.3s\n",
      "[CV] END xgb__learning_rate=0.01, xgb__max_depth=3, xgb__n_estimators=300, xgb__subsample=1.0; total time=   0.3s\n",
      "[CV] END xgb__learning_rate=0.01, xgb__max_depth=3, xgb__n_estimators=300, xgb__subsample=1.0; total time=   0.3s\n",
      "[CV] END xgb__learning_rate=0.01, xgb__max_depth=3, xgb__n_estimators=300, xgb__subsample=1.0; total time=   0.3s\n",
      "[CV] END xgb__learning_rate=0.01, xgb__max_depth=6, xgb__n_estimators=100, xgb__subsample=0.8; total time=   0.2s\n",
      "[CV] END xgb__learning_rate=0.01, xgb__max_depth=6, xgb__n_estimators=100, xgb__subsample=0.8; total time=   0.2s\n",
      "[CV] END xgb__learning_rate=0.01, xgb__max_depth=6, xgb__n_estimators=100, xgb__subsample=0.8; total time=   0.2s\n",
      "[CV] END xgb__learning_rate=0.01, xgb__max_depth=6, xgb__n_estimators=100, xgb__subsample=0.8; total time=   0.2s\n",
      "[CV] END xgb__learning_rate=0.01, xgb__max_depth=6, xgb__n_estimators=100, xgb__subsample=0.8; total time=   0.2s\n",
      "[CV] END xgb__learning_rate=0.01, xgb__max_depth=6, xgb__n_estimators=100, xgb__subsample=1.0; total time=   0.2s\n",
      "[CV] END xgb__learning_rate=0.01, xgb__max_depth=6, xgb__n_estimators=100, xgb__subsample=1.0; total time=   0.2s\n",
      "[CV] END xgb__learning_rate=0.01, xgb__max_depth=6, xgb__n_estimators=100, xgb__subsample=1.0; total time=   0.2s\n",
      "[CV] END xgb__learning_rate=0.01, xgb__max_depth=6, xgb__n_estimators=100, xgb__subsample=1.0; total time=   0.2s\n",
      "[CV] END xgb__learning_rate=0.01, xgb__max_depth=6, xgb__n_estimators=100, xgb__subsample=1.0; total time=   0.2s\n",
      "[CV] END xgb__learning_rate=0.01, xgb__max_depth=6, xgb__n_estimators=300, xgb__subsample=0.8; total time=   0.6s\n",
      "[CV] END xgb__learning_rate=0.01, xgb__max_depth=6, xgb__n_estimators=300, xgb__subsample=0.8; total time=   0.6s\n",
      "[CV] END xgb__learning_rate=0.01, xgb__max_depth=6, xgb__n_estimators=300, xgb__subsample=0.8; total time=   0.5s\n",
      "[CV] END xgb__learning_rate=0.01, xgb__max_depth=6, xgb__n_estimators=300, xgb__subsample=0.8; total time=   0.5s\n",
      "[CV] END xgb__learning_rate=0.01, xgb__max_depth=6, xgb__n_estimators=300, xgb__subsample=0.8; total time=   0.5s\n",
      "[CV] END xgb__learning_rate=0.01, xgb__max_depth=6, xgb__n_estimators=300, xgb__subsample=1.0; total time=   0.5s\n",
      "[CV] END xgb__learning_rate=0.01, xgb__max_depth=6, xgb__n_estimators=300, xgb__subsample=1.0; total time=   0.5s\n",
      "[CV] END xgb__learning_rate=0.01, xgb__max_depth=6, xgb__n_estimators=300, xgb__subsample=1.0; total time=   0.5s\n",
      "[CV] END xgb__learning_rate=0.01, xgb__max_depth=6, xgb__n_estimators=300, xgb__subsample=1.0; total time=   0.5s\n",
      "[CV] END xgb__learning_rate=0.01, xgb__max_depth=6, xgb__n_estimators=300, xgb__subsample=1.0; total time=   0.5s\n",
      "[CV] END xgb__learning_rate=0.1, xgb__max_depth=3, xgb__n_estimators=100, xgb__subsample=0.8; total time=   0.1s\n",
      "[CV] END xgb__learning_rate=0.1, xgb__max_depth=3, xgb__n_estimators=100, xgb__subsample=0.8; total time=   0.1s\n",
      "[CV] END xgb__learning_rate=0.1, xgb__max_depth=3, xgb__n_estimators=100, xgb__subsample=0.8; total time=   0.1s\n",
      "[CV] END xgb__learning_rate=0.1, xgb__max_depth=3, xgb__n_estimators=100, xgb__subsample=0.8; total time=   0.1s\n",
      "[CV] END xgb__learning_rate=0.1, xgb__max_depth=3, xgb__n_estimators=100, xgb__subsample=0.8; total time=   0.1s\n",
      "[CV] END xgb__learning_rate=0.1, xgb__max_depth=3, xgb__n_estimators=100, xgb__subsample=1.0; total time=   0.1s\n",
      "[CV] END xgb__learning_rate=0.1, xgb__max_depth=3, xgb__n_estimators=100, xgb__subsample=1.0; total time=   0.1s\n",
      "[CV] END xgb__learning_rate=0.1, xgb__max_depth=3, xgb__n_estimators=100, xgb__subsample=1.0; total time=   0.1s\n",
      "[CV] END xgb__learning_rate=0.1, xgb__max_depth=3, xgb__n_estimators=100, xgb__subsample=1.0; total time=   0.1s\n",
      "[CV] END xgb__learning_rate=0.1, xgb__max_depth=3, xgb__n_estimators=100, xgb__subsample=1.0; total time=   0.1s\n",
      "[CV] END xgb__learning_rate=0.1, xgb__max_depth=3, xgb__n_estimators=300, xgb__subsample=0.8; total time=   0.3s\n",
      "[CV] END xgb__learning_rate=0.1, xgb__max_depth=3, xgb__n_estimators=300, xgb__subsample=0.8; total time=   0.3s\n",
      "[CV] END xgb__learning_rate=0.1, xgb__max_depth=3, xgb__n_estimators=300, xgb__subsample=0.8; total time=   0.3s\n",
      "[CV] END xgb__learning_rate=0.1, xgb__max_depth=3, xgb__n_estimators=300, xgb__subsample=0.8; total time=   0.3s\n",
      "[CV] END xgb__learning_rate=0.1, xgb__max_depth=3, xgb__n_estimators=300, xgb__subsample=0.8; total time=   0.3s\n",
      "[CV] END xgb__learning_rate=0.1, xgb__max_depth=3, xgb__n_estimators=300, xgb__subsample=1.0; total time=   0.3s\n",
      "[CV] END xgb__learning_rate=0.1, xgb__max_depth=3, xgb__n_estimators=300, xgb__subsample=1.0; total time=   0.3s\n",
      "[CV] END xgb__learning_rate=0.1, xgb__max_depth=3, xgb__n_estimators=300, xgb__subsample=1.0; total time=   0.3s\n",
      "[CV] END xgb__learning_rate=0.1, xgb__max_depth=3, xgb__n_estimators=300, xgb__subsample=1.0; total time=   0.3s\n",
      "[CV] END xgb__learning_rate=0.1, xgb__max_depth=3, xgb__n_estimators=300, xgb__subsample=1.0; total time=   0.3s\n",
      "[CV] END xgb__learning_rate=0.1, xgb__max_depth=6, xgb__n_estimators=100, xgb__subsample=0.8; total time=   0.2s\n",
      "[CV] END xgb__learning_rate=0.1, xgb__max_depth=6, xgb__n_estimators=100, xgb__subsample=0.8; total time=   0.2s\n",
      "[CV] END xgb__learning_rate=0.1, xgb__max_depth=6, xgb__n_estimators=100, xgb__subsample=0.8; total time=   0.2s\n",
      "[CV] END xgb__learning_rate=0.1, xgb__max_depth=6, xgb__n_estimators=100, xgb__subsample=0.8; total time=   0.2s\n",
      "[CV] END xgb__learning_rate=0.1, xgb__max_depth=6, xgb__n_estimators=100, xgb__subsample=0.8; total time=   0.2s\n",
      "[CV] END xgb__learning_rate=0.1, xgb__max_depth=6, xgb__n_estimators=100, xgb__subsample=1.0; total time=   0.2s\n",
      "[CV] END xgb__learning_rate=0.1, xgb__max_depth=6, xgb__n_estimators=100, xgb__subsample=1.0; total time=   0.2s\n",
      "[CV] END xgb__learning_rate=0.1, xgb__max_depth=6, xgb__n_estimators=100, xgb__subsample=1.0; total time=   0.2s\n",
      "[CV] END xgb__learning_rate=0.1, xgb__max_depth=6, xgb__n_estimators=100, xgb__subsample=1.0; total time=   0.2s\n",
      "[CV] END xgb__learning_rate=0.1, xgb__max_depth=6, xgb__n_estimators=100, xgb__subsample=1.0; total time=   0.2s\n",
      "[CV] END xgb__learning_rate=0.1, xgb__max_depth=6, xgb__n_estimators=300, xgb__subsample=0.8; total time=   0.5s\n",
      "[CV] END xgb__learning_rate=0.1, xgb__max_depth=6, xgb__n_estimators=300, xgb__subsample=0.8; total time=   0.5s\n",
      "[CV] END xgb__learning_rate=0.1, xgb__max_depth=6, xgb__n_estimators=300, xgb__subsample=0.8; total time=   0.5s\n",
      "[CV] END xgb__learning_rate=0.1, xgb__max_depth=6, xgb__n_estimators=300, xgb__subsample=0.8; total time=   0.5s\n",
      "[CV] END xgb__learning_rate=0.1, xgb__max_depth=6, xgb__n_estimators=300, xgb__subsample=0.8; total time=   0.5s\n",
      "[CV] END xgb__learning_rate=0.1, xgb__max_depth=6, xgb__n_estimators=300, xgb__subsample=1.0; total time=   0.5s\n",
      "[CV] END xgb__learning_rate=0.1, xgb__max_depth=6, xgb__n_estimators=300, xgb__subsample=1.0; total time=   0.5s\n",
      "[CV] END xgb__learning_rate=0.1, xgb__max_depth=6, xgb__n_estimators=300, xgb__subsample=1.0; total time=   0.5s\n",
      "[CV] END xgb__learning_rate=0.1, xgb__max_depth=6, xgb__n_estimators=300, xgb__subsample=1.0; total time=   0.5s\n",
      "[CV] END xgb__learning_rate=0.1, xgb__max_depth=6, xgb__n_estimators=300, xgb__subsample=1.0; total time=   0.5s\n",
      "\n",
      "Best hyperparameters: {'xgb__learning_rate': 0.1, 'xgb__max_depth': 3, 'xgb__n_estimators': 300, 'xgb__subsample': 0.8}\n",
      "Best CV RMSE (log-space): 0.3480\n",
      "\n",
      "Test RMSE (log-space)      : 0.3379\n",
      "Test RMSE (original scale) : 502106.57\n",
      "\n",
      "Top 20 XGBoost feature importances:\n",
      "                   feature  importance\n",
      "           Equipment_Value    0.456740\n",
      "        Base_Transport_Fee    0.125197\n",
      "      Supplier_Reliability    0.085102\n",
      "        Urgent_Shipping_No    0.062960\n",
      "   Installation_Service_No    0.024155\n",
      "          Equipment_Volume    0.022207\n",
      "  Transport_Method_Airways    0.020790\n",
      "      Fragile_Equipment_No    0.013483\n",
      "             Order_Month_3    0.013441\n",
      "            Order_Month_11    0.011014\n",
      "Transport_Method_Waterways    0.009893\n",
      "   CrossBorder_Shipping_No    0.009887\n",
      "      Equipment_Type_Stone    0.008781\n",
      "       Order_Day_of_Week_5    0.007604\n",
      "             Delivery_Days    0.007596\n",
      "     Hospital_Info_Wealthy    0.006976\n",
      "    Order_Is_Weekend_False    0.006889\n",
      "       Order_Day_of_Week_4    0.006652\n",
      "     Equipment_Type_Marble    0.006481\n",
      "             Order_Month_7    0.006403\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----- 1) CV splitter -----\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# ----- 2) XGB pipeline (preprocessor -> xgb) -----\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('xgb', XGBRegressor(objective='reg:squarederror',\n",
    "                         random_state=42,\n",
    "                         n_jobs=-1,\n",
    "                         tree_method='hist'))  # 'hist' is faster for larger data\n",
    "])\n",
    "\n",
    "# ----- 3) Hyperparameter grid (example) -----\n",
    "param_grid = {\n",
    "    'xgb__n_estimators': [100, 300],\n",
    "    'xgb__max_depth': [3, 6],\n",
    "    'xgb__learning_rate': [0.01, 0.1],\n",
    "    'xgb__subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# ----- 4) GridSearchCV -----\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=kf,\n",
    "    n_jobs=1,   # use 1 in notebooks to avoid multiprocessing cwd issues\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# ----- 5) Run grid search -----\n",
    "print(\"Starting GridSearchCV for XGBoost...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# ----- 6) Best params and CV score -----\n",
    "best_params = grid_search.best_params_\n",
    "best_cv_rmse = -grid_search.best_score_\n",
    "print(\"\\nBest hyperparameters:\", best_params)\n",
    "print(f\"Best CV RMSE (log-space): {best_cv_rmse:.4f}\")\n",
    "\n",
    "# ----- 7) Final model (best estimator) -----\n",
    "final_xgb = grid_search.best_estimator_\n",
    "\n",
    "# ----- 8) Evaluate on test set -----\n",
    "y_test_pred_log = final_xgb.predict(X_test)\n",
    "y_test_pred_orig = np.expm1(y_test_pred_log)\n",
    "\n",
    "rmse_test_log = np.sqrt(mean_squared_error(y_test, y_test_pred_log))\n",
    "rmse_test_orig = np.sqrt(mean_squared_error(np.expm1(y_test), y_test_pred_orig))\n",
    "\n",
    "print(f\"\\nTest RMSE (log-space)      : {rmse_test_log:.4f}\")\n",
    "print(f\"Test RMSE (original scale) : {rmse_test_orig:.2f}\")\n",
    "\n",
    "# ----- 9) (Optional) Feature importances mapped to feature names -----\n",
    "# This extracts names from the preprocessor (numeric + one-hot cat names)\n",
    "pre = final_xgb.named_steps['preprocessor']\n",
    "ohe = pre.named_transformers_['cat'].named_steps['onehot']\n",
    "num_names = numeric_features\n",
    "cat_names = list(ohe.get_feature_names_out(categorical_features))\n",
    "feature_names = np.concatenate([num_names, cat_names])\n",
    "\n",
    "# xgboost stores feature importances by index (0..n-1)\n",
    "xgb_model = final_xgb.named_steps['xgb']\n",
    "importances = xgb_model.feature_importances_\n",
    "\n",
    "# If shapes mismatch (e.g., due to different handling), ensure lengths match before creating df\n",
    "if len(importances) == len(feature_names):\n",
    "    fi_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    print(\"\\nTop 20 XGBoost feature importances:\")\n",
    "    print(fi_df.head(20).to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nFeature importance length does not match derived feature name length. Skipping feature-name mapping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b9d237",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "lgb_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('lgb', LGBMRegressor(random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'lgb__n_estimators': [100, 300],\n",
    "    'lgb__max_depth': [4, 8],\n",
    "    'lgb__learning_rate': [0.01, 0.1],\n",
    "    'lgb__num_leaves': [31, 63]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=lgb_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=kf,\n",
    "    n_jobs=1,     # safer in notebooks; use >1 or -1 in script environments\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(\"Starting GridSearchCV for LightGBM...\")\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "best_cv_rmse = -grid.best_score_\n",
    "print(f\"Best CV RMSE (log-space): {best_cv_rmse:.4f}\")\n",
    "\n",
    "# Final model: best estimator already includes the preprocessor\n",
    "final_lgb = grid.best_estimator_\n",
    "\n",
    "# Evaluate on test set\n",
    "y_test_pred_log = final_lgb.predict(X_test)\n",
    "y_test_pred_orig = np.expm1(y_test_pred_log)\n",
    "\n",
    "rmse_test_log = np.sqrt(mean_squared_error(y_test, y_test_pred_log))\n",
    "rmse_test_orig = np.sqrt(mean_squared_error(np.expm1(y_test), y_test_pred_orig))\n",
    "\n",
    "print(f\"Test RMSE (log-space): {rmse_test_log:.4f}\")\n",
    "print(f\"Test RMSE (original scale): {rmse_test_orig:.2f}\")\n",
    "\n",
    "# Optional: feature importances mapped to names (if preprocessor produces matching columns)\n",
    "pre = final_lgb.named_steps['preprocessor']\n",
    "ohe = pre.named_transformers_['cat'].named_steps['onehot']\n",
    "num_names = numeric_features\n",
    "cat_names = list(ohe.get_feature_names_out(categorical_features))\n",
    "feature_names = np.concatenate([num_names, cat_names])\n",
    "\n",
    "lgb_model = final_lgb.named_steps['lgb']\n",
    "importances = lgb_model.feature_importances_\n",
    "\n",
    "if len(importances) == len(feature_names):\n",
    "    fi_df = pd.DataFrame({'feature': feature_names, 'importance': importances}) \\\n",
    "             .sort_values('importance', ascending=False)\n",
    "    print(fi_df.head(20).to_string(index=False))\n",
    "else:\n",
    "    print(\"Warning: feature importance length != feature name length. Skipping mapping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0522d23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 1) Pipeline -----\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('rf', RandomForestRegressor(random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# ----- 2) Expanded parameter grid -----\n",
    "param_dist = {\n",
    "    'rf__n_estimators': [100, 300, 500, 700],\n",
    "    'rf__max_depth': [None, 10, 20, 30],\n",
    "    'rf__min_samples_split': [2, 5, 10],\n",
    "    'rf__min_samples_leaf': [1, 2, 4],\n",
    "    'rf__max_features': ['sqrt', 'log2', 0.5, 1.0]\n",
    "}\n",
    "\n",
    "# ----- 3) 5-Fold CV -----\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# ----- 4) Randomized Search -----\n",
    "random_search = RandomizedSearchCV(\n",
    "    rf_pipeline,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=40,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=kf,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,  # shows progress for each combination\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# ----- 5) Train -----\n",
    "print(\"🚀 Starting RandomizedSearchCV for Random Forest...\")\n",
    "random_search.fit(X_train, y_train)\n",
    "print(\"✅ RandomizedSearchCV complete.\")\n",
    "\n",
    "# ----- 6) Evaluate -----\n",
    "best_model = random_search.best_estimator_\n",
    "print(\"🔹 Best model selected. Predicting on test set...\")\n",
    "y_pred_log = best_model.predict(X_test)\n",
    "y_pred_orig = np.expm1(y_pred_log)\n",
    "\n",
    "rmse_log = np.sqrt(mean_squared_error(y_test, y_pred_log))\n",
    "rmse_orig = np.sqrt(mean_squared_error(np.expm1(y_test), y_pred_orig))\n",
    "\n",
    "# ----- 7) Final Results -----\n",
    "print(\"\\n===== FINAL RESULTS =====\")\n",
    "print(\"✅ Best Parameters:\", random_search.best_params_)\n",
    "print(f\"✅ CV RMSE (log-space): {-random_search.best_score_:.4f}\")\n",
    "print(f\"✅ Test RMSE (log-space): {rmse_log:.4f}\")\n",
    "print(f\"✅ Test RMSE (original scale): {rmse_orig:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9c055b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\" TRAINING FINAL MODEL ON ALL DATA \")\n",
    "print(\"=\"*30)\n",
    "\n",
    "print(\"You've found the best parameters. Now, we'll train a new model using\")\n",
    "print(\"these parameters on the *entire* dataset (X and y) to create\")\n",
    "print(\"the final, production-ready model.\")\n",
    "\n",
    "# --- 1. Re-define the unfitted preprocessor ---\n",
    "# We MUST do this to get a fresh, unfitted preprocessor\n",
    "# so it can be properly fitted on the *full* X dataset.\n",
    "\n",
    "# Define Feature Lists (as before)\n",
    "numeric_features = [\n",
    "    'Supplier_Reliability', 'Equipment_Value', 'Base_Transport_Fee',\n",
    "    'Delivery_Days', 'Equipment_Volume'\n",
    "]\n",
    "categorical_features = [\n",
    "    'Equipment_Type', 'CrossBorder_Shipping', 'Urgent_Shipping',\n",
    "    'Installation_Service', 'Transport_Method', 'Fragile_Equipment',\n",
    "    'Hospital_Info', 'Rural_Hospital', 'Order_Month',\n",
    "    'Order_Day_of_Week', 'Order_Is_Weekend'\n",
    "]\n",
    "\n",
    "# Create the Numeric Pipeline (unfitted)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Create the Categorical Pipeline (unfitted)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Create the Full Preprocessor (unfitted)\n",
    "final_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# --- 2. Create the final, unfitted XGBoost pipeline ---\n",
    "# This pipeline contains the unfitted preprocessor and an unfitted model\n",
    "final_model_pipeline = Pipeline([\n",
    "    ('preprocessor', final_preprocessor),\n",
    "    ('xgb', XGBRegressor(objective='reg:squarederror',\n",
    "                         random_state=42,\n",
    "                         n_jobs=-1,\n",
    "                         tree_method='hist'))\n",
    "])\n",
    "\n",
    "# --- 3. Get best parameters from your grid search ---\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"\\nUsing best parameters: {best_params}\")\n",
    "\n",
    "# --- 4. Set the best parameters on the new pipeline ---\n",
    "final_model_pipeline.set_params(**best_params)\n",
    "\n",
    "# --- 5. Fit the final pipeline on ALL data (X, y) ---\n",
    "# This will fit the preprocessor (imputers, scalers) on ALL X\n",
    "# and then train the XGBoost model on ALL X and y.\n",
    "print(\"Fitting final model on the entire (X, y) dataset...\")\n",
    "final_model_pipeline.fit(X, y)\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "print(\"The 'final_model_pipeline' object is now your fully-trained model,\")\n",
    "print(\"ready to be saved and used for predictions.\")\n",
    "\n",
    "# --- 6. (Optional) Save your final model ---\n",
    "# You can now save this model to a file for later use.\n",
    "# import joblib\n",
    "# joblib.dump(final_model_pipeline, 'final_xgb_model.pkl')\n",
    "# print(\"\\nFinal model saved to 'final_xgb_model.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690c252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_features(df_raw):\n",
    "    \"\"\"\n",
    "    Applies all manual cleaning and feature engineering\n",
    "    to match the data used for model training.\n",
    "    \n",
    "    Takes a raw DataFrame (like test.csv) and returns\n",
    "    a DataFrame ready for the model pipeline.\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid changing the original data\n",
    "    df = df_raw.copy()\n",
    "    \n",
    "    # 1. Clean column names (from your training script)\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    # 2. Clean all string/object columns (from your training script)\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "        df[col] = df[col].replace({'': np.nan, 'nan': np.nan, 'NaN': np.nan})\n",
    "\n",
    "    # 3. Normalize Yes/No columns (from your training script)\n",
    "    yes_no_cols = ['CrossBorder_Shipping', 'Urgent_Shipping', 'Installation_Service',\n",
    "                   'Fragile_Equipment', 'Rural_Hospital']\n",
    "    for col in yes_no_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].replace({\n",
    "                'YES': 'Yes', 'yes': 'Yes', 'Y': 'Yes', 'y': 'Yes',\n",
    "                'NO': 'No', 'no': 'No', 'N': 'No', 'n': 'No'\n",
    "            })\n",
    "\n",
    "    # 4. Convert date columns (from your training script)\n",
    "    df['Order_Placed_Date'] = pd.to_datetime(df['Order_Placed_Date'], errors='coerce')\n",
    "    df['Delivery_Date'] = pd.to_datetime(df['Delivery_Date'], errors='coerce')\n",
    "\n",
    "    # 5. Engineer Date Features (from your training script)\n",
    "    df['Delivery_Days'] = (df['Delivery_Date'] - df['Order_Placed_Date']).dt.days\n",
    "    df['Delivery_Days'] = pd.to_numeric(df['Delivery_Days'], errors='coerce')\n",
    "    \n",
    "    df['Order_Month'] = df['Order_Placed_Date'].dt.month\n",
    "    df['Order_Day_of_Week'] = df['Order_Placed_Date'].dt.dayofweek\n",
    "    df['Order_Is_Weekend'] = df['Order_Day_of_Week'].isin([5, 6])\n",
    "    \n",
    "    # 6. Handle bad data (CRITICAL FIX)\n",
    "    # Instead of dropping rows, we set bad data to NaN.\n",
    "    # Your pipeline's imputer will then handle it.\n",
    "    df.loc[df['Delivery_Days'] < 0, 'Delivery_Days'] = np.nan\n",
    "    # let us print how many nans were set\n",
    "    num_bad_delivery_days = df['Delivery_Days'].isna().sum()\n",
    "    print(f\"Set {num_bad_delivery_days} invalid Delivery_Days to NaN.\")\n",
    "\n",
    "    # 7. Engineer Volume Feature (from your training script)\n",
    "    df['Equipment_Volume'] = df['Equipment_Height'] * df['Equipment_Width']\n",
    "\n",
    "    # 8. Log-Transform Skewed Features (from your training script)\n",
    "    # The model was trained on these log-transformed features.\n",
    "    df['Equipment_Value'] = np.log1p(df['Equipment_Value'])\n",
    "    df['Equipment_Volume'] = np.log1p(df['Equipment_Volume'])\n",
    "    \n",
    "    # 9. Return the feature-engineered DataFrame\n",
    "    # The pipeline will select the columns it needs from this.\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41acd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume 'final_model_pipeline' is your trained model object from the previous step\n",
    "# import joblib\n",
    "# final_model_pipeline = joblib.load('final_xgb_model.pkl') # If you saved it\n",
    "\n",
    "# 1. Load your new, raw test data\n",
    "print(\"Loading new test data...\")\n",
    "# I'm using 'test.csv' as the example filename\n",
    "df_new_test = pd.read_csv('../data/test.csv') \n",
    "\n",
    "# 2. Save IDs for the final submission\n",
    "# We need to map our predictions back to the original IDs\n",
    "submission_ids = df_new_test['Hospital_Id']\n",
    "\n",
    "# 3. Apply the *exact same* feature engineering\n",
    "print(\"Applying feature engineering to new data...\")\n",
    "X_new_prepared = prepare_features(df_new_test)\n",
    "\n",
    "# 4. Get predictions\n",
    "# The pipeline will handle the rest:\n",
    "# - Selects the correct columns\n",
    "# - Imputes missing values (using 'median'/'most_frequent' from training)\n",
    "# - Scales numeric features (using 'scaler' from training)\n",
    "# - One-hot encodes categorical features (using 'onehot' from training)\n",
    "# - Runs the XGBoost model\n",
    "print(\"Getting predictions from the final model...\")\n",
    "log_predictions = final_model_pipeline.predict(X_new_prepared)\n",
    "\n",
    "# 5. Convert predictions back from log-scale!\n",
    "# Remember, you trained on log(Transport_Cost + 1)\n",
    "final_predictions = np.expm1(log_predictions)\n",
    "\n",
    "# 6. Create the final submission file\n",
    "submission_df = pd.DataFrame({\n",
    "    'Hospital_Id': submission_ids,\n",
    "    'Transport_Cost': final_predictions\n",
    "})\n",
    "\n",
    "# Display the first few predictions\n",
    "print(\"\\nFinal Predictions:\")\n",
    "display(submission_df.head())\n",
    "\n",
    "# Save to CSV\n",
    "submission_df.to_csv('submission1.csv', index=False)\n",
    "print(\"Submission file 'submission.csv' created successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MlLab)",
   "language": "python",
   "name": "mllab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
