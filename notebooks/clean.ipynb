{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1ad8d2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# core\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# data + plotting\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "from scipy.stats import zscore\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "# sklearn (preprocessing / pipeline / model selection / metrics)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# classical models (if you use them elsewhere)\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# gradient boosting / lightgbm / xgboost\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# utilities\n",
    "import joblib   # optional: save/load pipeline\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abbea68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7f4617a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hospital_Id</th>\n",
       "      <th>Supplier_Name</th>\n",
       "      <th>Supplier_Reliability</th>\n",
       "      <th>Equipment_Height</th>\n",
       "      <th>Equipment_Width</th>\n",
       "      <th>Equipment_Weight</th>\n",
       "      <th>Equipment_Type</th>\n",
       "      <th>Equipment_Value</th>\n",
       "      <th>Base_Transport_Fee</th>\n",
       "      <th>CrossBorder_Shipping</th>\n",
       "      <th>Urgent_Shipping</th>\n",
       "      <th>Installation_Service</th>\n",
       "      <th>Transport_Method</th>\n",
       "      <th>Fragile_Equipment</th>\n",
       "      <th>Hospital_Info</th>\n",
       "      <th>Rural_Hospital</th>\n",
       "      <th>Order_Placed_Date</th>\n",
       "      <th>Delivery_Date</th>\n",
       "      <th>Hospital_Location</th>\n",
       "      <th>Transport_Cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fffe3200360030003700</td>\n",
       "      <td>Jo Valencia</td>\n",
       "      <td>0.44</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.62</td>\n",
       "      <td>17.13</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Roadways</td>\n",
       "      <td>No</td>\n",
       "      <td>Working Class</td>\n",
       "      <td>No</td>\n",
       "      <td>10/20/17</td>\n",
       "      <td>10/20/17</td>\n",
       "      <td>APO AA 33776</td>\n",
       "      <td>179.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fffe3400380037003400</td>\n",
       "      <td>Wanda Warren</td>\n",
       "      <td>0.58</td>\n",
       "      <td>29.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1210684.0</td>\n",
       "      <td>Marble</td>\n",
       "      <td>9703.37</td>\n",
       "      <td>35.42</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Roadways</td>\n",
       "      <td>No</td>\n",
       "      <td>Working Class</td>\n",
       "      <td>No</td>\n",
       "      <td>02/22/16</td>\n",
       "      <td>02/24/16</td>\n",
       "      <td>South Kevin, VT 84493</td>\n",
       "      <td>627732.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fffe3200350036003700</td>\n",
       "      <td>Robert Ackies</td>\n",
       "      <td>0.97</td>\n",
       "      <td>39.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3305.0</td>\n",
       "      <td>Aluminium</td>\n",
       "      <td>40.21</td>\n",
       "      <td>18.54</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Roadways</td>\n",
       "      <td>No</td>\n",
       "      <td>Working Class</td>\n",
       "      <td>No</td>\n",
       "      <td>01/11/18</td>\n",
       "      <td>01/10/18</td>\n",
       "      <td>Kevinshire, NE 31279</td>\n",
       "      <td>1565.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fffe3800320034003400</td>\n",
       "      <td>Charlotte Membreno</td>\n",
       "      <td>0.70</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>606.0</td>\n",
       "      <td>Brass</td>\n",
       "      <td>4.55</td>\n",
       "      <td>17.48</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Roadways</td>\n",
       "      <td>No</td>\n",
       "      <td>Working Class</td>\n",
       "      <td>No</td>\n",
       "      <td>08/06/16</td>\n",
       "      <td>08/06/16</td>\n",
       "      <td>DPO AP 61572</td>\n",
       "      <td>257.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fffe3600340033003000</td>\n",
       "      <td>Nena Silva</td>\n",
       "      <td>0.66</td>\n",
       "      <td>27.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Marble</td>\n",
       "      <td>2726.80</td>\n",
       "      <td>30.23</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Roadways</td>\n",
       "      <td>No</td>\n",
       "      <td>Working Class</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12/15/16</td>\n",
       "      <td>12/17/16</td>\n",
       "      <td>Joshuamouth, AK 01550</td>\n",
       "      <td>8553.52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Hospital_Id       Supplier_Name  Supplier_Reliability  \\\n",
       "0  fffe3200360030003700         Jo Valencia                  0.44   \n",
       "1  fffe3400380037003400        Wanda Warren                  0.58   \n",
       "2  fffe3200350036003700       Robert Ackies                  0.97   \n",
       "3  fffe3800320034003400  Charlotte Membreno                  0.70   \n",
       "4  fffe3600340033003000          Nena Silva                  0.66   \n",
       "\n",
       "   Equipment_Height  Equipment_Width  Equipment_Weight Equipment_Type  \\\n",
       "0              21.0              6.0               NaN            NaN   \n",
       "1              29.0             20.0         1210684.0         Marble   \n",
       "2              39.0             15.0            3305.0      Aluminium   \n",
       "3               8.0              5.0             606.0          Brass   \n",
       "4              27.0             13.0               NaN         Marble   \n",
       "\n",
       "   Equipment_Value  Base_Transport_Fee CrossBorder_Shipping Urgent_Shipping  \\\n",
       "0             3.62               17.13                   No              No   \n",
       "1          9703.37               35.42                   No             Yes   \n",
       "2            40.21               18.54                   No              No   \n",
       "3             4.55               17.48                   No              No   \n",
       "4          2726.80               30.23                  Yes              No   \n",
       "\n",
       "  Installation_Service Transport_Method Fragile_Equipment  Hospital_Info  \\\n",
       "0                   No         Roadways                No  Working Class   \n",
       "1                  Yes         Roadways                No  Working Class   \n",
       "2                   No         Roadways                No  Working Class   \n",
       "3                   No         Roadways                No  Working Class   \n",
       "4                   No         Roadways                No  Working Class   \n",
       "\n",
       "  Rural_Hospital Order_Placed_Date Delivery_Date      Hospital_Location  \\\n",
       "0             No          10/20/17      10/20/17           APO AA 33776   \n",
       "1             No          02/22/16      02/24/16  South Kevin, VT 84493   \n",
       "2             No          01/11/18      01/10/18   Kevinshire, NE 31279   \n",
       "3             No          08/06/16      08/06/16           DPO AP 61572   \n",
       "4            NaN          12/15/16      12/17/16  Joshuamouth, AK 01550   \n",
       "\n",
       "   Transport_Cost  \n",
       "0          179.50  \n",
       "1       627732.45  \n",
       "2         1565.92  \n",
       "3          257.71  \n",
       "4         8553.52  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial data shape: (5000, 20)\n",
      "Cleaning string columns...\n",
      "Normalizing Yes/No columns...\n",
      "Converting date columns...\n",
      "Engineering Delivery_Days feature...\n",
      "Engineering more date features...\n",
      "Dropping duplicates...\n",
      "Dropped 0 duplicate rows.\n",
      "\n",
      "==============================\n",
      " CLEANING & FEATURE ENGINEERING COMPLETE \n",
      "==============================\n",
      "After basic cleaning shape: (5000, 24)\n",
      "\n",
      "Missing values (raw count):\n",
      "Hospital_Id                0\n",
      "Supplier_Name              0\n",
      "Supplier_Reliability     587\n",
      "Equipment_Height         283\n",
      "Equipment_Width          443\n",
      "Equipment_Weight         460\n",
      "Equipment_Type           599\n",
      "Equipment_Value            0\n",
      "Base_Transport_Fee         0\n",
      "CrossBorder_Shipping       0\n",
      "Urgent_Shipping            0\n",
      "Installation_Service       0\n",
      "Transport_Method        1071\n",
      "Fragile_Equipment          0\n",
      "Hospital_Info              0\n",
      "Rural_Hospital           586\n",
      "Order_Placed_Date          0\n",
      "Delivery_Date              0\n",
      "Hospital_Location          0\n",
      "Transport_Cost             0\n",
      "Delivery_Days              0\n",
      "Order_Month                0\n",
      "Order_Day_of_Week          0\n",
      "Order_Is_Weekend           0\n",
      "dtype: int64\n",
      "\n",
      "Missing values (percentage):\n",
      "Transport_Method        21.42\n",
      "Equipment_Type          11.98\n",
      "Supplier_Reliability    11.74\n",
      "Rural_Hospital          11.72\n",
      "Equipment_Weight         9.20\n",
      "Equipment_Width          8.86\n",
      "Equipment_Height         5.66\n",
      "dtype: float64\n",
      "\n",
      "DataFrame head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hospital_Id</th>\n",
       "      <th>Supplier_Name</th>\n",
       "      <th>Supplier_Reliability</th>\n",
       "      <th>Equipment_Height</th>\n",
       "      <th>Equipment_Width</th>\n",
       "      <th>Equipment_Weight</th>\n",
       "      <th>Equipment_Type</th>\n",
       "      <th>Equipment_Value</th>\n",
       "      <th>Base_Transport_Fee</th>\n",
       "      <th>CrossBorder_Shipping</th>\n",
       "      <th>...</th>\n",
       "      <th>Hospital_Info</th>\n",
       "      <th>Rural_Hospital</th>\n",
       "      <th>Order_Placed_Date</th>\n",
       "      <th>Delivery_Date</th>\n",
       "      <th>Hospital_Location</th>\n",
       "      <th>Transport_Cost</th>\n",
       "      <th>Delivery_Days</th>\n",
       "      <th>Order_Month</th>\n",
       "      <th>Order_Day_of_Week</th>\n",
       "      <th>Order_Is_Weekend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fffe3200360030003700</td>\n",
       "      <td>Jo Valencia</td>\n",
       "      <td>0.44</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.62</td>\n",
       "      <td>17.13</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>Working Class</td>\n",
       "      <td>No</td>\n",
       "      <td>2017-10-20</td>\n",
       "      <td>2017-10-20</td>\n",
       "      <td>APO AA 33776</td>\n",
       "      <td>179.50</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fffe3400380037003400</td>\n",
       "      <td>Wanda Warren</td>\n",
       "      <td>0.58</td>\n",
       "      <td>29.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1210684.0</td>\n",
       "      <td>Marble</td>\n",
       "      <td>9703.37</td>\n",
       "      <td>35.42</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>Working Class</td>\n",
       "      <td>No</td>\n",
       "      <td>2016-02-22</td>\n",
       "      <td>2016-02-24</td>\n",
       "      <td>South Kevin, VT 84493</td>\n",
       "      <td>627732.45</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fffe3200350036003700</td>\n",
       "      <td>Robert Ackies</td>\n",
       "      <td>0.97</td>\n",
       "      <td>39.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3305.0</td>\n",
       "      <td>Aluminium</td>\n",
       "      <td>40.21</td>\n",
       "      <td>18.54</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>Working Class</td>\n",
       "      <td>No</td>\n",
       "      <td>2018-01-11</td>\n",
       "      <td>2018-01-10</td>\n",
       "      <td>Kevinshire, NE 31279</td>\n",
       "      <td>1565.92</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fffe3800320034003400</td>\n",
       "      <td>Charlotte Membreno</td>\n",
       "      <td>0.70</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>606.0</td>\n",
       "      <td>Brass</td>\n",
       "      <td>4.55</td>\n",
       "      <td>17.48</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>Working Class</td>\n",
       "      <td>No</td>\n",
       "      <td>2016-08-06</td>\n",
       "      <td>2016-08-06</td>\n",
       "      <td>DPO AP 61572</td>\n",
       "      <td>257.71</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fffe3600340033003000</td>\n",
       "      <td>Nena Silva</td>\n",
       "      <td>0.66</td>\n",
       "      <td>27.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Marble</td>\n",
       "      <td>2726.80</td>\n",
       "      <td>30.23</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>Working Class</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-12-15</td>\n",
       "      <td>2016-12-17</td>\n",
       "      <td>Joshuamouth, AK 01550</td>\n",
       "      <td>8553.52</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Hospital_Id       Supplier_Name  Supplier_Reliability  \\\n",
       "0  fffe3200360030003700         Jo Valencia                  0.44   \n",
       "1  fffe3400380037003400        Wanda Warren                  0.58   \n",
       "2  fffe3200350036003700       Robert Ackies                  0.97   \n",
       "3  fffe3800320034003400  Charlotte Membreno                  0.70   \n",
       "4  fffe3600340033003000          Nena Silva                  0.66   \n",
       "\n",
       "   Equipment_Height  Equipment_Width  Equipment_Weight Equipment_Type  \\\n",
       "0              21.0              6.0               NaN            NaN   \n",
       "1              29.0             20.0         1210684.0         Marble   \n",
       "2              39.0             15.0            3305.0      Aluminium   \n",
       "3               8.0              5.0             606.0          Brass   \n",
       "4              27.0             13.0               NaN         Marble   \n",
       "\n",
       "   Equipment_Value  Base_Transport_Fee CrossBorder_Shipping  ...  \\\n",
       "0             3.62               17.13                   No  ...   \n",
       "1          9703.37               35.42                   No  ...   \n",
       "2            40.21               18.54                   No  ...   \n",
       "3             4.55               17.48                   No  ...   \n",
       "4          2726.80               30.23                  Yes  ...   \n",
       "\n",
       "   Hospital_Info Rural_Hospital Order_Placed_Date Delivery_Date  \\\n",
       "0  Working Class             No        2017-10-20    2017-10-20   \n",
       "1  Working Class             No        2016-02-22    2016-02-24   \n",
       "2  Working Class             No        2018-01-11    2018-01-10   \n",
       "3  Working Class             No        2016-08-06    2016-08-06   \n",
       "4  Working Class            NaN        2016-12-15    2016-12-17   \n",
       "\n",
       "       Hospital_Location Transport_Cost Delivery_Days Order_Month  \\\n",
       "0           APO AA 33776         179.50             0          10   \n",
       "1  South Kevin, VT 84493      627732.45             2           2   \n",
       "2   Kevinshire, NE 31279        1565.92            -1           1   \n",
       "3           DPO AP 61572         257.71             0           8   \n",
       "4  Joshuamouth, AK 01550        8553.52             2          12   \n",
       "\n",
       "  Order_Day_of_Week  Order_Is_Weekend  \n",
       "0                 4             False  \n",
       "1                 0             False  \n",
       "2                 3             False  \n",
       "3                 5              True  \n",
       "4                 3             False  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set plot style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "\n",
    "# 1Ô∏è‚É£ Load Data\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv('../data/train.csv')\n",
    "df.columns = df.columns.str.strip()\n",
    "display(df.head())\n",
    "print(f\"Initial data shape: {df.shape}\")\n",
    "\n",
    "# 2Ô∏è‚É£ Clean all string/object columns: strip spaces, replace blanks with NaN\n",
    "print(\"Cleaning string columns...\")\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    df[col] = df[col].astype(str).str.strip()\n",
    "    df[col] = df[col].replace({'': np.nan, 'nan': np.nan, 'NaN': np.nan})\n",
    "\n",
    "# 3Ô∏è‚É£ Normalize Yes/No columns to consistent \"Yes\"/\"No\"\n",
    "print(\"Normalizing Yes/No columns...\")\n",
    "yes_no_cols = ['CrossBorder_Shipping', 'Urgent_Shipping', 'Installation_Service',\n",
    "               'Fragile_Equipment', 'Rural_Hospital']\n",
    "for col in yes_no_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace({\n",
    "            'YES': 'Yes', 'yes': 'Yes', 'Y': 'Yes', 'y': 'Yes',\n",
    "            'NO': 'No', 'no': 'No', 'N': 'No', 'n': 'No'\n",
    "        })\n",
    "\n",
    "# 4Ô∏è‚É£ Convert date columns to datetime\n",
    "print(\"Converting date columns...\")\n",
    "df['Order_Placed_Date'] = pd.to_datetime(df['Order_Placed_Date'], errors='coerce')\n",
    "df['Delivery_Date'] = pd.to_datetime(df['Delivery_Date'], errors='coerce')\n",
    "\n",
    "# 5Ô∏è‚É£ Create new feature: Delivery_Days (difference in days)\n",
    "print(\"Engineering Delivery_Days feature...\")\n",
    "df['Delivery_Days'] = (df['Delivery_Date'] - df['Order_Placed_Date']).dt.days\n",
    "df['Delivery_Days'] = pd.to_numeric(df['Delivery_Days'], errors='coerce')\n",
    "\n",
    "# === ADDED: Date Feature Engineering ===\n",
    "print(\"Engineering more date features...\")\n",
    "df['Order_Month'] = df['Order_Placed_Date'].dt.month\n",
    "df['Order_Day_of_Week'] = df['Order_Placed_Date'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "df['Order_Is_Weekend'] = df['Order_Day_of_Week'].isin([5, 6])\n",
    "# === END ADDED ===\n",
    "\n",
    "# 6Ô∏è‚É£ (Original) delete initial date rows\n",
    "# df = df.dropna(subset=['Order_Placed_Date', 'Delivery_Date'])\n",
    "\n",
    "# 7Ô∏è‚É£ Drop exact duplicate rows\n",
    "print(\"Dropping duplicates...\")\n",
    "before = len(df)\n",
    "df = df.drop_duplicates()\n",
    "after = len(df)\n",
    "print(f\"Dropped {before - after} duplicate rows.\")\n",
    "\n",
    "# 8Ô∏è‚É£ Quick check after cleaning\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\" CLEANING & FEATURE ENGINEERING COMPLETE \")\n",
    "print(\"=\"*30)\n",
    "print(f\"After basic cleaning shape: {df.shape}\")\n",
    "\n",
    "print(\"\\nMissing values (raw count):\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "# === ADDED: Missing Value Percentage View ===\n",
    "print(\"\\nMissing values (percentage):\")\n",
    "missing_pct = (df.isna().sum() / len(df) * 100).sort_values(ascending=False)\n",
    "print(missing_pct[missing_pct > 0])\n",
    "# === END ADDED ===\n",
    "\n",
    "print(\"\\nDataFrame head:\")\n",
    "display(df.head())\n",
    "# print(df['Delivery_Days'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6847d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# üìä START OF EXPLORATORY DATA ANALYSIS (EDA)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\" STARTING EDA \")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# üîπ Define column lists\n",
    "num_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "# === ADDED: Exclude new date features from 'num_cols' for general stats ===\n",
    "date_num_features = ['Order_Month', 'Order_Day_of_Week', 'Delivery_Days']\n",
    "for col in ['Transport_Cost'] + date_num_features:\n",
    "    if col in num_cols:\n",
    "        num_cols.remove(col)\n",
    "# === END ADDED ===\n",
    "        \n",
    "cat_cols = df.select_dtypes(include='object').columns.tolist()\n",
    "# === ADDED: Add boolean 'Is_Weekend' to cat_cols for analysis ===\n",
    "if 'Order_Is_Weekend' in df.columns:\n",
    "    cat_cols.append('Order_Is_Weekend')\n",
    "# === END ADDED ===\n",
    "\n",
    "print(f\"Numeric features identified: {num_cols}\")\n",
    "print(f\"Categorical features identified: {cat_cols}\")\n",
    "print(f\"Date-derived features identified: {date_num_features}\")\n",
    "\n",
    "\n",
    "# === ADDED: 1. Target Variable Analysis (Transport_Cost) ===\n",
    "print(\"\\n===== 1. TARGET VARIABLE ANALYSIS: Transport_Cost =====\")\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Original Distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['Transport_Cost'], kde=True, bins=40)\n",
    "plt.title('Distribution of Transport_Cost (Original)')\n",
    "plt.xlabel('Transport_Cost')\n",
    "\n",
    "# Plot 2: Log-Transformed Distribution\n",
    "# We add 1 to handle potential zero values before logging\n",
    "plt.subplot(1, 2, 2)\n",
    "log_target = np.log1p(df['Transport_Cost'])\n",
    "sns.histplot(log_target, kde=True, bins=40, color='green')\n",
    "plt.title('Distribution of log(Transport_Cost + 1)')\n",
    "plt.xlabel('log(Transport_Cost + 1)')\n",
    "\n",
    "plt.suptitle('Target Variable Distribution Analysis', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "print(f\"Skewness of Transport_Cost: {df['Transport_Cost'].skew():.4f}\")\n",
    "print(f\"Skewness of log(Transport_Cost + 1): {log_target.skew():.4f}\")\n",
    "# === END ADDED ===\n",
    "\n",
    "\n",
    "print(\"\\n===== 2. NUMERIC FEATURE ANALYSIS =====\")\n",
    "print(\"===== BASIC NUMERIC STATISTICS =====\")\n",
    "if not num_cols:\n",
    "    print(\"No numeric columns found to describe (excluding target/dates).\")\n",
    "else:\n",
    "    display(df[num_cols].describe().T)\n",
    "\n",
    "    print(\"\\n===== SKEWNESS =====\")\n",
    "    display(df[num_cols].skew())\n",
    "\n",
    "# üîπ Numeric distributions + boxplots\n",
    "# (Your original loop)\n",
    "# === MODIFIED: Added a check for empty list ===\n",
    "print(\"\\nGenerating numeric distribution plots...\")\n",
    "analysis_num_cols = num_cols + ['Delivery_Days'] # Add Delivery_Days back for plotting\n",
    "if 'Transport_Cost' not in analysis_num_cols:\n",
    "    analysis_num_cols.append('Transport_Cost') # Add Target back for plotting\n",
    "    \n",
    "for col in analysis_num_cols:\n",
    "    if col in df.columns:\n",
    "        plt.figure(figsize=(12,4))\n",
    "        \n",
    "        plt.subplot(1,2,1)\n",
    "        sns.histplot(df[col], kde=True, bins=30)\n",
    "        plt.title(f'{col} distribution')\n",
    "        \n",
    "        plt.subplot(1,2,2)\n",
    "        sns.boxplot(x=df[col])\n",
    "        plt.title(f'{col} boxplot')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Warning: Column '{col}' not found for plotting.\")\n",
    "\n",
    "\n",
    "print(\"\\n===== 3. CORRELATION ANALYSIS =====\")\n",
    "# üîπ Correlation heatmap\n",
    "# (Your original code)\n",
    "plt.figure(figsize=(10,8))\n",
    "corr = df[num_cols + ['Transport_Cost', 'Delivery_Days']].corr()\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n===== 4. CATEGORICAL FEATURE ANALYSIS =====\")\n",
    "# üîπ Categorical distributions\n",
    "# (Your original loop)\n",
    "print(\"\\nGenerating categorical distribution plots...\")\n",
    "high_cardinality_cols = []\n",
    "for col in cat_cols:\n",
    "    print(f\"\\n===== Column: {col} =====\")\n",
    "    print(df[col].value_counts(dropna=False))\n",
    "    \n",
    "    nunique = df[col].nunique()\n",
    "    if nunique > 20:\n",
    "        high_cardinality_cols.append(col)\n",
    "        print(f\"SKIPPING countplot for {col} (High Cardinality: {nunique} unique values)\")\n",
    "        continue\n",
    "        \n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.countplot(y=col, data=df, order=df[col].value_counts().index)\n",
    "    plt.title(f'Count of {col}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# === ADDED: 4a. High-Cardinality Column Summary ===\n",
    "print(\"\\n===== 4a. HIGH-CARDINALITY CATEGORICAL SUMMARY =====\")\n",
    "if high_cardinality_cols:\n",
    "    print(f\"High-cardinality features detected: {high_cardinality_cols}\")\n",
    "    for col in high_cardinality_cols:\n",
    "        print(f\"\\n--- Top 10 values for: {col} ---\")\n",
    "        print(df[col].value_counts(dropna=False).head(10))\n",
    "        print(f\"...and {df[col].nunique() - 10} other unique values.\")\n",
    "else:\n",
    "    print(\"No high-cardinality categorical features detected (threshold > 20).\")\n",
    "# === END ADDED ===\n",
    "\n",
    "\n",
    "print(\"\\n===== 5. BIVARIATE ANALYSIS (FEATURES vs. TARGET) =====\")\n",
    "# üîπ Numeric features vs target\n",
    "# (Your original loop)\n",
    "print(\"\\nGenerating numeric features vs. Transport_Cost...\")\n",
    "for col in num_cols + ['Delivery_Days']:\n",
    "    if col in df.columns:\n",
    "        plt.figure(figsize=(6,4))\n",
    "        sns.scatterplot(x=df[col], y=df['Transport_Cost'])\n",
    "        plt.title(f'{col} vs Transport_Cost')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# üîπ Categorical features vs target (low-cardinality)\n",
    "# (Your original loop)\n",
    "print(\"\\nGenerating categorical features vs. Transport_Cost...\")\n",
    "for col in cat_cols:\n",
    "    if col in df.columns and df[col].nunique() < 20:\n",
    "        plt.figure(figsize=(10,4))\n",
    "        sns.boxplot(x=col, y='Transport_Cost', data=df)\n",
    "        plt.title(f'{col} vs Transport_Cost')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# === ADDED: 5a. Date-Derived Features vs. Target ===\n",
    "print(\"\\nGenerating date-derived features vs. Transport_Cost...\")\n",
    "date_features_to_plot = ['Order_Month', 'Order_Day_of_Week', 'Order_Is_Weekend']\n",
    "for col in date_features_to_plot:\n",
    "    if col in df.columns:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        sns.boxplot(x=col, y='Transport_Cost', data=df)\n",
    "        plt.title(f'{col} vs Transport_Cost')\n",
    "        if col == 'Order_Day_of_Week':\n",
    "            plt.xticks(ticks=range(7), labels=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "# === END ADDED ===\n",
    "\n",
    "\n",
    "print(\"\\n===== 6. OUTLIER DETECTION =====\")\n",
    "# üîπ Outlier detection (Z-score)\n",
    "# (Your original code)\n",
    "# === MODIFIED: Added nan_policy='omit' to handle missing values gracefully ===\n",
    "try:\n",
    "    z_scores = df[num_cols + ['Transport_Cost', 'Delivery_Days']].apply(lambda x: zscore(x, nan_policy='omit'))\n",
    "    outliers = (abs(z_scores) > 3).sum()\n",
    "    print(\"\\n===== NUMBER OF OUTLIERS PER COLUMN (Z-score > 3) =====\")\n",
    "    print(outliers[outliers > 0].sort_values(ascending=False))\n",
    "except ValueError as e:\n",
    "    print(f\"Could not calculate Z-scores, likely due to all-NaN column. Error: {e}\")\n",
    "# === END MODIFIED ===\n",
    "\n",
    "\n",
    "print(\"\\n===== 7. MISSING VALUE VISUALIZATION =====\")\n",
    "# üîπ Missing value visualization\n",
    "# (Your original code)\n",
    "print(\"\\nGenerating missing value matrix...\")\n",
    "msno.matrix(df)\n",
    "plt.title('Missing Value Matrix')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nGenerating missing value bar chart...\")\n",
    "msno.bar(df)\n",
    "plt.title('Missing Value Bar Chart')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "df['Is_Negative_Delivery'] = (df['Delivery_Days'] < 0).astype(int)\n",
    "\n",
    "# Create a boxplot to see the relationship\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(x='Is_Negative_Delivery', y='Transport_Cost', data=df)\n",
    "plt.title('Transport Cost vs. Delivery Type (0=Normal, 1=Negative)')\n",
    "plt.show()\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.scatterplot(x='Delivery_Days', y='Transport_Cost', data=df, hue='Is_Negative_Delivery', alpha=0.5)\n",
    "plt.title('Transport Cost vs. Delivery Days')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Create a temporary dataframe of just the bad rows\n",
    "df_negative_cost = df[df['Transport_Cost'] < 0].copy()\n",
    "\n",
    "# Create a dataframe of the good rows for comparison\n",
    "df_positive_cost = df[df['Transport_Cost'] >= 0].copy()\n",
    "\n",
    "print(f\"Found {len(df_negative_cost)} rows with negative cost.\")\n",
    "\n",
    "# --- WHAT TO CHECK ---\n",
    "\n",
    "# 1. Are they all from one supplier or hospital?\n",
    "print(\"\\nTop Suppliers in negative-cost rows:\")\n",
    "print(df_negative_cost['Supplier_Name'].value_counts().head())\n",
    "\n",
    "print(\"\\nTop Suppliers in positive-cost rows:\")\n",
    "print(df_positive_cost['Supplier_Name'].value_counts().head())\n",
    "\n",
    "# 2. Do they have other weird values?\n",
    "# print(\"\\nNumeric stats for negative-cost rows:\")\n",
    "display(df_negative_cost[numeric_features].describe())\n",
    "\n",
    "print(\"\\nNumeric stats for positive-cost rows:\")\n",
    "display(df_positive_cost[numeric_features].describe())\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\" EDA COMPLETE \")\n",
    "print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ad723c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing script started...\n",
      "Filtered 1964 rows with bad data (negative cost or delivery days).\n",
      "Features for modeling: ['Supplier_Reliability', 'Equipment_Type', 'Equipment_Value', 'Base_Transport_Fee', 'CrossBorder_Shipping', 'Urgent_Shipping', 'Installation_Service', 'Transport_Method', 'Fragile_Equipment', 'Hospital_Info', 'Rural_Hospital', 'Delivery_Days', 'Order_Month', 'Order_Day_of_Week', 'Order_Is_Weekend', 'Equipment_Volume']\n",
      "Baseline RMSE (test, original-scale): 141785.8334\n",
      "Training set shape: (2428, 16)\n",
      "Test set shape: (608, 16)\n",
      "\n",
      "Fitting preprocessor on X_train...\n",
      "Preprocessing complete.\n",
      "Processed X_train shape: (2428, 48)\n",
      "Processed X_test shape: (608, 48)\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocessing script started...\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: PRE-SPLIT (Data Cleaning & Feature Engineering)\n",
    "# These actions are applied to the whole dataset before splitting.\n",
    "# ==============================================================================\n",
    "\n",
    "# 1. Filter Bad Data\n",
    "# EDA Finding: We found impossible values like Transport_Cost < 0 and Delivery_Days < 0.\n",
    "# Action: Remove these rows entirely.\n",
    "initial_rows = len(df)\n",
    "df = df[df['Transport_Cost'] >= 0]\n",
    "df = df[df['Delivery_Days'] >= 0]\n",
    "print(f\"Filtered {initial_rows - len(df)} rows with bad data (negative cost or delivery days).\")\n",
    "\n",
    "# 2. Feature Engineering\n",
    "# EDA Finding: Equipment_Height & Equipment_Width were highly correlated (0.77).\n",
    "# Action: Combine them into a single 'Equipment_Volume' feature.\n",
    "df['Equipment_Volume'] = df['Equipment_Height'] * df['Equipment_Width']\n",
    "\n",
    "# 3. Log-Transform Skewed Features\n",
    "# EDA Finding: Equipment_Value (skew=24) and our new Equipment_Volume\n",
    "# (derived from skewed features) are extremely right-skewed.\n",
    "# Action: Apply np.log1p to normalize them.\n",
    "df['Equipment_Value'] = np.log1p(df['Equipment_Value'])\n",
    "df['Equipment_Volume'] = np.log1p(df['Equipment_Volume'])\n",
    "\n",
    "# 4. Define Target (y) and Features (X)\n",
    "# EDA Finding: Target 'Transport_Cost' is extremely skewed (skew=30).\n",
    "# Action: Use np.log1p on the target. We will predict the log, then convert back.\n",
    "y = df['Transport_Cost'] # this y is in log-space\n",
    "\n",
    "# Action: Define X by dropping the target, original engineered columns, \n",
    "# and high-cardinality/redundant/ID columns identified in the EDA.\n",
    "X = df.drop(columns=[\n",
    "    # Target\n",
    "    'Transport_Cost',\n",
    "    \n",
    "    # Replaced by Equipment_Volume\n",
    "    'Equipment_Height',\n",
    "    'Equipment_Width',\n",
    "    \n",
    "    # Redundant (corr 0.90 with Value)\n",
    "    'Equipment_Weight',\n",
    "    \n",
    "    # High-Cardinality IDs / Unused\n",
    "    'Hospital_Id',\n",
    "    'Supplier_Name',\n",
    "    'Hospital_Location',\n",
    "    \n",
    "    # Replaced by date features\n",
    "    'Order_Placed_Date',\n",
    "    'Delivery_Date'\n",
    "])\n",
    "\n",
    "print(f\"Features for modeling: {X.columns.tolist()}\")\n",
    "\n",
    "# 5. Train-Test Split\n",
    "# Action: Split the data into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "train_mean = y_train.mean()\n",
    "\n",
    "# 1Ô∏è‚É£ Baseline RMSE on the training set (log-space)\n",
    "# y_train_pred_baseline = np.full_like(y_train, train_mean)\n",
    "# baseline_rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred_baseline))\n",
    "# print(f\"Baseline RMSE (train, log-space): {baseline_rmse_train:.4f}\")\n",
    "\n",
    "# 2Ô∏è‚É£ Baseline RMSE on the test set (using train mean as predictor) ‚Äî log-space\n",
    "y_test_pred_baseline = np.full_like(y_test, train_mean)\n",
    "# baseline_rmse_test_log = np.sqrt(mean_squared_error(y_test, y_test_pred_baseline))\n",
    "# print(f\"Baseline RMSE (test, log-space): {baseline_rmse_test_log:.4f}\")\n",
    "\n",
    "# 3Ô∏è‚É£ Baseline RMSE in original (Transport_Cost) scale\n",
    "# now we are not taking log of y so no need of expm1\n",
    "y_test_actual_orig = y_test\n",
    "y_test_baseline_pred_orig = y_test_pred_baseline\n",
    "baseline_rmse_test_orig = np.sqrt(mean_squared_error(y_test_actual_orig, y_test_baseline_pred_orig))\n",
    "print(f\"Baseline RMSE (test, original-scale): {baseline_rmse_test_orig:.4f}\")\n",
    "\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: POST-SPLIT (Pipelines & ColumnTransformer)\n",
    "# This prevents data leakage. We FIT on X_train, then TRANSFORM X_train and X_test.\n",
    "# ==============================================================================\n",
    "\n",
    "# 1. Define Feature Lists\n",
    "# Action: Separate our final columns into numeric and categorical lists.\n",
    "\n",
    "numeric_features = [\n",
    "    'Supplier_Reliability',\n",
    "    'Equipment_Value',      # Already log-transformed\n",
    "    'Base_Transport_Fee',\n",
    "    'Delivery_Days',\n",
    "    'Equipment_Volume'      # Already log-transformed\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'Equipment_Type',\n",
    "    'CrossBorder_Shipping',\n",
    "    'Urgent_Shipping',\n",
    "    'Installation_Service',\n",
    "    'Transport_Method',\n",
    "    'Fragile_Equipment',\n",
    "    'Hospital_Info',\n",
    "    'Rural_Hospital',\n",
    "    'Order_Month',\n",
    "    'Order_Day_of_Week',\n",
    "    'Order_Is_Weekend'\n",
    "]\n",
    "\n",
    "# 2. Create the Numeric Pipeline\n",
    "# EDA Finding: Numeric features had missing values (e.g., Supplier_Reliability)\n",
    "# and were on different scales.\n",
    "# Action: Impute missing values with the median (robust to outliers)\n",
    "# and then scale all features.\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# 3. Create the Categorical Pipeline\n",
    "# EDA Finding: Categorical features had missing values (e.g., Transport_Method,\n",
    "# Rural_Hospital) and need to be converted to numbers.\n",
    "# Action: Impute missing values with the most frequent value and then\n",
    "# one-hot encode. 'handle_unknown='ignore'' ensures our model doesn't\n",
    "# crash if it sees a new category in the test data.\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# 4. Create the Full Preprocessor\n",
    "# Action: Combine the numeric and categorical pipelines using ColumnTransformer.\n",
    "# This single object will handle all preprocessing for us.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'  # Drop any columns we didn't explicitly list\n",
    ")\n",
    "\n",
    "# 5. Apply the Preprocessor\n",
    "# Action: FIT the preprocessor on X_train ONLY (to learn medians, modes, etc.)\n",
    "# and then TRANSFORM both X_train and X_test.\n",
    "# This gives us our final, model-ready datasets.\n",
    "\n",
    "print(\"\\nFitting preprocessor on X_train...\")\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test) # what if we have missig data in testing set?\n",
    "\n",
    "print(\"Preprocessing complete.\")\n",
    "print(f\"Processed X_train shape: {X_train_processed.shape}\")\n",
    "print(f\"Processed X_test shape: {X_test_processed.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0a25e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0dbad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----- 1) Set up 5-Fold -----\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# ----- 2) Create the pipeline -----\n",
    "lr_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),  # your ColumnTransformer\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "# ----- 3) Run 5-Fold Cross-Validation -----\n",
    "cv_scores = cross_val_score(\n",
    "    lr_pipeline,\n",
    "    X_train,       # raw training features\n",
    "    y_train,       # ORIGINAL target (not log)\n",
    "    cv=kf,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "avg_rmse = np.abs(cv_scores).mean()\n",
    "print(f\"Linear Regression 5-Fold Avg. RMSE (original scale): {avg_rmse:.4f}\")\n",
    "\n",
    "# ----- 4) Fit final model on full training data -----\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "print(\"Linear Regression final model trained on full training set.\")\n",
    "\n",
    "# ----- 5) Predict on test set -----\n",
    "y_test_pred = lr_pipeline.predict(X_test)\n",
    "\n",
    "# ----- 6) Compute RMSE on test set -----\n",
    "rmse_test = np.sqrt(np.mean((y_test - y_test_pred) ** 2))\n",
    "print(f\"Test RMSE (original scale): {rmse_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe61007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----- 1) Set up 5-Fold -----\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# ----- 2) Create the pipeline -----\n",
    "poly_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),         # your ColumnTransformer\n",
    "    ('poly', PolynomialFeatures()),         # will tune degree\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "# ----- 3) Set up GridSearch for hyperparameter tuning -----\n",
    "param_grid = {\n",
    "    'poly__degree': [2, 3]   # you can expand to 5 if dataset is small\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    poly_pipeline,\n",
    "    param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=kf,\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "# ----- 4) Run GridSearch -----\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best degree\n",
    "best_degree = grid_search.best_params_['poly__degree']\n",
    "best_rmse = -grid_search.best_score_\n",
    "print(f\"Best polynomial degree: {best_degree}\")\n",
    "print(f\"Best CV RMSE (original scale): {best_rmse:.4f}\")\n",
    "\n",
    "# ----- 5) Fit final model on full training data -----\n",
    "final_poly_model = grid_search.best_estimator_\n",
    "final_poly_model.fit(X_train, y_train)\n",
    "print(\"Polynomial Regression final model trained on full training set.\")\n",
    "\n",
    "# ----- 6) Predict on test set -----\n",
    "y_test_pred = final_poly_model.predict(X_test)\n",
    "\n",
    "# ----- 7) Compute RMSE on test set -----\n",
    "rmse_test = np.sqrt(np.mean((y_test - y_test_pred) ** 2))\n",
    "print(f\"Test RMSE (original scale): {rmse_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2959af42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----- 1) Create the pipeline -----\n",
    "ridge_poly_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),  # your ColumnTransformer\n",
    "    ('poly', PolynomialFeatures()),  # polynomial expansion\n",
    "    ('ridge', Ridge())               # ridge regression\n",
    "])\n",
    "\n",
    "# ----- 2) Set hyperparameter grid -----\n",
    "param_grid = {\n",
    "    'poly__degree': [2,3],      # best degree from CV\n",
    "    'ridge__alpha': [0.1,0.01,10,100,1]     # best alpha from CV\n",
    "}\n",
    "\n",
    "# ----- 3) Grid Search with 5-Fold CV -----\n",
    "grid_search = GridSearchCV(\n",
    "    ridge_poly_pipeline,\n",
    "    param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=5,\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "# ----- 4) Fit on training data -----\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# ----- 5) Best hyperparameters -----\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best CV RMSE (original scale):\", -grid_search.best_score_)\n",
    "\n",
    "# ----- 6) Predict on test set -----\n",
    "y_test_pred = grid_search.predict(X_test)\n",
    "print(\"Max predicted value:\", max(y_test_pred))\n",
    "\n",
    "# ----- 7) Compute RMSE on test set -----\n",
    "rmse_test = np.sqrt(np.mean((y_test - y_test_pred) ** 2))\n",
    "print(f\"Test RMSE (original scale): {rmse_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba86e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----- 1) Create the pipeline -----\n",
    "lasso_poly_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),      # your ColumnTransformer\n",
    "    ('poly', PolynomialFeatures()),      # polynomial expansion\n",
    "    ('lasso', Lasso(max_iter=100000))   # Lasso regression\n",
    "])\n",
    "\n",
    "# ----- 2) Set hyperparameter grid -----\n",
    "param_grid = {\n",
    "    'poly__degree': [2],                     # best degree from CV\n",
    "    'lasso__alpha': [0.001, 0.01, 0.1, 1, 10]  # regularization strengths\n",
    "}\n",
    "\n",
    "# ----- 3) Grid Search with 5-Fold CV -----\n",
    "grid_search = GridSearchCV(\n",
    "    lasso_poly_pipeline,\n",
    "    param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# ----- 4) Fit on training data -----\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# ----- 5) Best hyperparameters -----\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best CV RMSE (original scale):\", -grid_search.best_score_)\n",
    "\n",
    "# ----- 6) Predict on test set -----\n",
    "y_test_pred = grid_search.predict(X_test)\n",
    "print(\"Max predicted value:\", max(y_test_pred))\n",
    "\n",
    "# ----- 7) Compute RMSE on test set -----\n",
    "rmse_test = np.sqrt(np.mean((y_test - y_test_pred) ** 2))\n",
    "print(f\"Test RMSE (original scale): {rmse_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099a12f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----- 1) CV splitter -----\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# ----- 2) Pipeline: preprocessor -> poly -> elastic net -----\n",
    "enet_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),                       # your ColumnTransformer\n",
    "    ('poly', PolynomialFeatures(include_bias=False)),     # polynomial expansion\n",
    "    ('enet', ElasticNet(max_iter=20000, random_state=42)) # ElasticNet regression\n",
    "])\n",
    "\n",
    "# ----- 3) Hyperparameter grid -----\n",
    "param_grid = {\n",
    "    'poly__degree': [2],                     # best degree from CV\n",
    "    'enet__alpha': [0.1, 1, 10, 100, 1000], # regularization strengths\n",
    "    'enet__l1_ratio': [0.01, 0.1, 0.2, 0.5, 0.8]  # L1/L2 mix\n",
    "}\n",
    "\n",
    "# ----- 4) GridSearchCV -----\n",
    "grid_search = GridSearchCV(\n",
    "    enet_pipeline,\n",
    "    param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=kf,\n",
    "    n_jobs=1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# ----- 5) Run grid search -----\n",
    "print(\"Starting GridSearchCV for ElasticNet + PolynomialFeatures ...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# ----- 6) Best params & CV score -----\n",
    "best_params = grid_search.best_params_\n",
    "best_cv_rmse = -grid_search.best_score_\n",
    "print(f\"\\nBest hyperparameters: {best_params}\")\n",
    "print(f\"Best CV RMSE (original scale): {best_cv_rmse:.4f}\")\n",
    "\n",
    "# ----- 7) Final model (best estimator) -----\n",
    "final_enet = grid_search.best_estimator_\n",
    "\n",
    "# ----- 8) Evaluate on test set -----\n",
    "y_test_pred = final_enet.predict(X_test)\n",
    "\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "print(f\"\\nTest RMSE (original scale): {rmse_test:.4f}\")\n",
    "\n",
    "# Optional: inspect predictions\n",
    "print(\"Predictions on test set: min/median/max:\", np.min(y_test_pred), np.median(y_test_pred), np.max(y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e282f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# ----- 1) CV splitter -----\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# ----- 2) XGB pipeline (preprocessor -> xgb) -----\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('xgb', XGBRegressor(objective='reg:squarederror',\n",
    "                         random_state=42,\n",
    "                         n_jobs=-1,\n",
    "                         tree_method='hist'))  # 'hist' is faster for larger data\n",
    "])\n",
    "\n",
    "# ----- 3) Hyperparameter grid (example) -----\n",
    "param_grid = {\n",
    "    'xgb__n_estimators': [100, 300],\n",
    "    'xgb__max_depth': [3, 6],\n",
    "    'xgb__learning_rate': [0.01, 0.1],\n",
    "    'xgb__subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# ----- 4) GridSearchCV -----\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=kf,\n",
    "    n_jobs=1,   # use 1 in notebooks to avoid multiprocessing cwd issues\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# ----- 5) Run grid search -----\n",
    "print(\"Starting GridSearchCV for XGBoost...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# ----- 6) Best params and CV score -----\n",
    "best_params = grid_search.best_params_\n",
    "best_cv_rmse = -grid_search.best_score_\n",
    "print(\"\\nBest hyperparameters:\", best_params)\n",
    "print(f\"Best CV RMSE (original scale): {best_cv_rmse:.4f}\")\n",
    "\n",
    "# ----- 7) Final model (best estimator) -----\n",
    "final_xgb = grid_search.best_estimator_\n",
    "\n",
    "# ----- 8) Evaluate on test set -----\n",
    "y_test_pred = final_xgb.predict(X_test)\n",
    "\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "print(f\"\\nTest RMSE (original scale): {rmse_test:.4f}\")\n",
    "\n",
    "# Optional: inspect predictions\n",
    "print(\"Predictions on test set: min/median/max:\", np.min(y_test_pred), np.median(y_test_pred), np.max(y_test_pred))\n",
    "\n",
    "# ----- 9) Feature importances mapped to feature names -----\n",
    "pre = final_xgb.named_steps['preprocessor']\n",
    "ohe = pre.named_transformers_['cat'].named_steps['onehot']\n",
    "num_names = numeric_features\n",
    "cat_names = list(ohe.get_feature_names_out(categorical_features))\n",
    "feature_names = np.concatenate([num_names, cat_names])\n",
    "\n",
    "xgb_model = final_xgb.named_steps['xgb']\n",
    "importances = xgb_model.feature_importances_\n",
    "\n",
    "if len(importances) == len(feature_names):\n",
    "    fi_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    print(\"\\nTop 20 XGBoost feature importances:\")\n",
    "    print(fi_df.head(20).to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nFeature importance length does not match derived feature name length. Skipping feature-name mapping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b9d237",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----- 1) CV splitter -----\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# ----- 2) LightGBM pipeline -----\n",
    "lgb_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('lgb', LGBMRegressor(random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# ----- 3) Hyperparameter grid -----\n",
    "param_grid = {\n",
    "    'lgb__n_estimators': [100, 300],\n",
    "    'lgb__max_depth': [4, 8],\n",
    "    'lgb__learning_rate': [0.01, 0.1],\n",
    "    'lgb__num_leaves': [31, 63]\n",
    "}\n",
    "\n",
    "# ----- 4) GridSearchCV -----\n",
    "grid = GridSearchCV(\n",
    "    estimator=lgb_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=kf,\n",
    "    n_jobs=1,     # safer in notebooks; use >1 or -1 in scripts\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# ----- 5) Run grid search -----\n",
    "print(\"Starting GridSearchCV for LightGBM...\")\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# ----- 6) Best params and CV score -----\n",
    "best_params = grid.best_params_\n",
    "best_cv_rmse = -grid.best_score_\n",
    "print(\"Best params:\", best_params)\n",
    "print(f\"Best CV RMSE (original scale): {best_cv_rmse:.4f}\")\n",
    "\n",
    "# ----- 7) Final model (best estimator) -----\n",
    "final_lgb = grid.best_estimator_\n",
    "\n",
    "# ----- 8) Evaluate on test set -----\n",
    "y_test_pred = final_lgb.predict(X_test)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "print(f\"Test RMSE (original scale): {rmse_test:.4f}\")\n",
    "print(\"Predictions on test set: min/median/max:\", np.min(y_test_pred), np.median(y_test_pred), np.max(y_test_pred))\n",
    "\n",
    "# ----- 9) Feature importances mapped to feature names -----\n",
    "pre = final_lgb.named_steps['preprocessor']\n",
    "ohe = pre.named_transformers_['cat'].named_steps['onehot']\n",
    "num_names = numeric_features\n",
    "cat_names = list(ohe.get_feature_names_out(categorical_features))\n",
    "feature_names = np.concatenate([num_names, cat_names])\n",
    "\n",
    "lgb_model = final_lgb.named_steps['lgb']\n",
    "importances = lgb_model.feature_importances_\n",
    "\n",
    "if len(importances) == len(feature_names):\n",
    "    fi_df = pd.DataFrame({'feature': feature_names, 'importance': importances}) \\\n",
    "             .sort_values('importance', ascending=False)\n",
    "    print(\"\\nTop 20 LightGBM feature importances:\")\n",
    "    print(fi_df.head(20).to_string(index=False))\n",
    "else:\n",
    "    print(\"Warning: feature importance length != feature name length. Skipping mapping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0522d23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----- 1) Pipeline -----\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('rf', RandomForestRegressor(random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# ----- 2) Expanded parameter grid -----\n",
    "param_dist = {\n",
    "    'rf__n_estimators': [100, 300, 500, 700],\n",
    "    'rf__max_depth': [None, 10, 20, 30],\n",
    "    'rf__min_samples_split': [2, 5, 10],\n",
    "    'rf__min_samples_leaf': [1, 2, 4],\n",
    "    'rf__max_features': ['sqrt', 'log2', 0.5, 1.0]\n",
    "}\n",
    "\n",
    "# ----- 3) 5-Fold CV -----\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# ----- 4) Randomized Search -----\n",
    "random_search = RandomizedSearchCV(\n",
    "    rf_pipeline,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=40,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=kf,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# ----- 5) Train -----\n",
    "print(\"üöÄ Starting RandomizedSearchCV for Random Forest...\")\n",
    "random_search.fit(X_train, y_train)\n",
    "print(\"‚úÖ RandomizedSearchCV complete.\")\n",
    "\n",
    "# ----- 6) Evaluate -----\n",
    "best_model = random_search.best_estimator_\n",
    "print(\"üîπ Best model selected. Predicting on test set...\")\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# ----- 7) Final Results -----\n",
    "print(\"\\n===== FINAL RESULTS =====\")\n",
    "print(\"‚úÖ Best Parameters:\", random_search.best_params_)\n",
    "print(f\"‚úÖ CV RMSE: {-random_search.best_score_:.4f}\")\n",
    "print(f\"‚úÖ Test RMSE (original scale): {rmse_test:.2f}\")\n",
    "\n",
    "# ----- 8) Optional: inspect prediction range -----\n",
    "print(\"Predictions on test set: min/median/max:\", np.min(y_pred), np.median(y_pred), np.max(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9c055b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\" TRAINING FINAL MODEL ON ALL DATA \")\n",
    "print(\"=\"*30)\n",
    "\n",
    "print(\"You've found the best parameters. Now, we'll train a new model using\")\n",
    "print(\"these parameters on the *entire* dataset (X and y) to create\")\n",
    "print(\"the final, production-ready model.\")\n",
    "\n",
    "# --- 1. Re-define the unfitted preprocessor ---\n",
    "# We MUST do this to get a fresh, unfitted preprocessor\n",
    "# so it can be properly fitted on the *full* X dataset.\n",
    "\n",
    "# Define Feature Lists (as before)\n",
    "numeric_features = [\n",
    "    'Supplier_Reliability', 'Equipment_Value', 'Base_Transport_Fee',\n",
    "    'Delivery_Days', 'Equipment_Volume'\n",
    "]\n",
    "categorical_features = [\n",
    "    'Equipment_Type', 'CrossBorder_Shipping', 'Urgent_Shipping',\n",
    "    'Installation_Service', 'Transport_Method', 'Fragile_Equipment',\n",
    "    'Hospital_Info', 'Rural_Hospital', 'Order_Month',\n",
    "    'Order_Day_of_Week', 'Order_Is_Weekend'\n",
    "]\n",
    "\n",
    "# Create the Numeric Pipeline (unfitted)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Create the Categorical Pipeline (unfitted)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Create the Full Preprocessor (unfitted)\n",
    "final_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# --- 2. Create the final, unfitted XGBoost pipeline ---\n",
    "# This pipeline contains the unfitted preprocessor and an unfitted model\n",
    "final_model_pipeline = Pipeline([\n",
    "    ('preprocessor', final_preprocessor),\n",
    "    ('xgb', XGBRegressor(objective='reg:squarederror',\n",
    "                         random_state=42,\n",
    "                         n_jobs=-1,\n",
    "                         tree_method='hist'))\n",
    "])\n",
    "\n",
    "# --- 3. Get best parameters from your grid search ---\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"\\nUsing best parameters: {best_params}\")\n",
    "\n",
    "# --- 4. Set the best parameters on the new pipeline ---\n",
    "final_model_pipeline.set_params(**best_params)\n",
    "\n",
    "# --- 5. Fit the final pipeline on ALL data (X, y) ---\n",
    "# This will fit the preprocessor (imputers, scalers) on ALL X\n",
    "# and then train the XGBoost model on ALL X and y.\n",
    "print(\"Fitting final model on the entire (X, y) dataset...\")\n",
    "final_model_pipeline.fit(X, y)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "print(\"The 'final_model_pipeline' object is now your fully-trained model,\")\n",
    "print(\"ready to be saved and used for predictions.\")\n",
    "\n",
    "# --- 6. (Optional) Save your final model ---\n",
    "# You can now save this model to a file for later use.\n",
    "# import joblib\n",
    "# joblib.dump(final_model_pipeline, 'final_xgb_model.pkl')\n",
    "# print(\"\\nFinal model saved to 'final_xgb_model.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d386a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nTraining final Ridge(polynomial) model on all data...\")\n",
    "\n",
    "# === 1. Feature groups ===\n",
    "numeric_features = [\n",
    "    'Supplier_Reliability', 'Equipment_Value', 'Base_Transport_Fee',\n",
    "    'Delivery_Days', 'Equipment_Volume'\n",
    "]\n",
    "categorical_features = [\n",
    "    'Equipment_Type', 'CrossBorder_Shipping', 'Urgent_Shipping',\n",
    "    'Installation_Service', 'Transport_Method', 'Fragile_Equipment',\n",
    "    'Hospital_Info', 'Rural_Hospital', 'Order_Month',\n",
    "    'Order_Day_of_Week', 'Order_Is_Weekend'\n",
    "]\n",
    "\n",
    "# === 2. Define transformers ===\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# === 3. Combine them ===\n",
    "final_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# === 4. Build final pipeline with best parameters ===\n",
    "final_ridge_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', final_preprocessor),\n",
    "    ('poly', PolynomialFeatures(degree=3, include_bias=False)),  # best poly__degree = 3\n",
    "    ('ridge', Ridge(alpha=100))                                 # best ridge__alpha = 100\n",
    "])\n",
    "\n",
    "# === 5. Fit on full dataset ===\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "final_ridge_pipeline.fit(X, y)\n",
    "\n",
    "print(\"\\n‚úÖ Final Ridge (poly=3, alpha=100) model trained on entire dataset.\")\n",
    "print(\"You can now use `final_ridge_pipeline.predict(new_X)` for predictions.\")\n",
    "print(\"Remember: predictions will be in log1p space; use np.expm1() to convert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a2dead",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nTraining final ElasticNet(polynomial) model on all data...\")\n",
    "\n",
    "# === 1. Feature groups ===\n",
    "numeric_features = [\n",
    "    'Supplier_Reliability', 'Equipment_Value', 'Base_Transport_Fee',\n",
    "    'Delivery_Days', 'Equipment_Volume'\n",
    "]\n",
    "categorical_features = [\n",
    "    'Equipment_Type', 'CrossBorder_Shipping', 'Urgent_Shipping',\n",
    "    'Installation_Service', 'Transport_Method', 'Fragile_Equipment',\n",
    "    'Hospital_Info', 'Rural_Hospital', 'Order_Month',\n",
    "    'Order_Day_of_Week', 'Order_Is_Weekend'\n",
    "]\n",
    "\n",
    "# === 2. Define transformers ===\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# === 3. Combine them ===\n",
    "final_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# === 4. Build final pipeline with best parameters ===\n",
    "final_enet_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', final_preprocessor),\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),  # best poly__degree = 2\n",
    "    ('enet', ElasticNet(alpha=0.1, l1_ratio=0.01, max_iter=10000))  # best enet parameters\n",
    "])\n",
    "\n",
    "# === 5. Fit on full dataset ===\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "final_enet_pipeline.fit(X, y)\n",
    "\n",
    "print(\"\\n‚úÖ Final ElasticNet (poly=2, alpha=0.1, l1_ratio=0.01) model trained on entire dataset.\")\n",
    "print(\"You can now use `final_enet_pipeline.predict(new_X)` for predictions.\")\n",
    "print(\"Remember: predictions will be in log1p space; use np.expm1() to convert if needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff336d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"\\nTraining final Random Forest model on all data...\")\n",
    "\n",
    "# === 1. Feature groups ===\n",
    "numeric_features = [\n",
    "    'Supplier_Reliability', 'Equipment_Value', 'Base_Transport_Fee',\n",
    "    'Delivery_Days', 'Equipment_Volume'\n",
    "]\n",
    "categorical_features = [\n",
    "    'Equipment_Type', 'CrossBorder_Shipping', 'Urgent_Shipping',\n",
    "    'Installation_Service', 'Transport_Method', 'Fragile_Equipment',\n",
    "    'Hospital_Info', 'Rural_Hospital', 'Order_Month',\n",
    "    'Order_Day_of_Week', 'Order_Is_Weekend'\n",
    "]\n",
    "\n",
    "# === 2. Define transformers ===\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# === 3. Combine them ===\n",
    "final_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# === 4. Build final pipeline with best RF hyperparameters ===\n",
    "final_rf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', final_preprocessor),\n",
    "    ('rf', RandomForestRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=30,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=4,\n",
    "        max_features=1.0,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# === 5. Fit on full dataset ===\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "final_rf_pipeline.fit(X, y)\n",
    "\n",
    "print(\"\\n‚úÖ Final Random Forest model trained on entire dataset.\")\n",
    "print(\"You can now use `final_rf_pipeline.predict(new_X)` for predictions.\")\n",
    "print(\"Remember: predictions will be in log1p space; use np.expm1() to convert if needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690c252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_features(df_raw):\n",
    "    \"\"\"\n",
    "    Applies all manual cleaning and feature engineering\n",
    "    to match the data used for model training.\n",
    "    \n",
    "    Takes a raw DataFrame (like test.csv) and returns\n",
    "    a DataFrame ready for the model pipeline.\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid changing the original data\n",
    "    df = df_raw.copy()\n",
    "    \n",
    "    # 1. Clean column names (from your training script)\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    # 2. Clean all string/object columns (from your training script)\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "        df[col] = df[col].replace({'': np.nan, 'nan': np.nan, 'NaN': np.nan})\n",
    "\n",
    "    # 3. Normalize Yes/No columns (from your training script)\n",
    "    yes_no_cols = ['CrossBorder_Shipping', 'Urgent_Shipping', 'Installation_Service',\n",
    "                   'Fragile_Equipment', 'Rural_Hospital']\n",
    "    for col in yes_no_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].replace({\n",
    "                'YES': 'Yes', 'yes': 'Yes', 'Y': 'Yes', 'y': 'Yes',\n",
    "                'NO': 'No', 'no': 'No', 'N': 'No', 'n': 'No'\n",
    "            })\n",
    "\n",
    "    # 4. Convert date columns (from your training script)\n",
    "    df['Order_Placed_Date'] = pd.to_datetime(df['Order_Placed_Date'], errors='coerce')\n",
    "    df['Delivery_Date'] = pd.to_datetime(df['Delivery_Date'], errors='coerce')\n",
    "\n",
    "    # 5. Engineer Date Features (from your training script)\n",
    "    df['Delivery_Days'] = (df['Delivery_Date'] - df['Order_Placed_Date']).dt.days\n",
    "    df['Delivery_Days'] = pd.to_numeric(df['Delivery_Days'], errors='coerce')\n",
    "    \n",
    "    df['Order_Month'] = df['Order_Placed_Date'].dt.month\n",
    "    df['Order_Day_of_Week'] = df['Order_Placed_Date'].dt.dayofweek\n",
    "    df['Order_Is_Weekend'] = df['Order_Day_of_Week'].isin([5, 6])\n",
    "    \n",
    "    # 6. Handle bad data (CRITICAL FIX)\n",
    "    # Instead of dropping rows, we set bad data to NaN.\n",
    "    # Your pipeline's imputer will then handle it.\n",
    "    df.loc[df['Delivery_Days'] < 0, 'Delivery_Days'] = np.nan\n",
    "    # let us print how many nans were set\n",
    "    num_bad_delivery_days = df['Delivery_Days'].isna().sum()\n",
    "    print(f\"Set {num_bad_delivery_days} invalid Delivery_Days to NaN.\")\n",
    "\n",
    "    # 7. Engineer Volume Feature (from your training script)\n",
    "    df['Equipment_Volume'] = df['Equipment_Height'] * df['Equipment_Width']\n",
    "\n",
    "    # 8. Log-Transform Skewed Features (from your training script)\n",
    "    # The model was trained on these log-transformed features.\n",
    "    df['Equipment_Value'] = np.log1p(df['Equipment_Value'])\n",
    "    df['Equipment_Volume'] = np.log1p(df['Equipment_Volume'])\n",
    "    \n",
    "    # 9. Return the feature-engineered DataFrame\n",
    "    # The pipeline will select the columns it needs from this.\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41acd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume 'final_model_pipeline' is your trained model object from the previous step\n",
    "# import joblib\n",
    "# final_model_pipeline = joblib.load('final_xgb_model.pkl') # If you saved it\n",
    "\n",
    "# 1. Load your new, raw test data\n",
    "print(\"Loading new test data...\")\n",
    "# I'm using 'test.csv' as the example filename\n",
    "df_new_test = pd.read_csv('../data/test.csv') \n",
    "\n",
    "# 2. Save IDs for the final submission\n",
    "# We need to map our predictions back to the original IDs\n",
    "submission_ids = df_new_test['Hospital_Id']\n",
    "\n",
    "# 3. Apply the *exact same* feature engineering\n",
    "print(\"Applying feature engineering to new data...\")\n",
    "X_new_prepared = prepare_features(df_new_test)\n",
    "\n",
    "# 4. Get predictions\n",
    "# The pipeline will handle the rest:\n",
    "# - Selects the correct columns\n",
    "# - Imputes missing values (using 'median'/'most_frequent' from training)\n",
    "# - Scales numeric features (using 'scaler' from training)\n",
    "# - One-hot encodes categorical features (using 'onehot' from training)\n",
    "# - Runs the XGBoost model\n",
    "print(\"Getting predictions from the final model...\")\n",
    "log_predictions = final_model_pipeline.predict(X_new_prepared)\n",
    "\n",
    "# 5. Convert predictions back from log-scale!\n",
    "# Remember, you trained on log(Transport_Cost + 1)\n",
    "final_predictions = log_predictions  # No need to expm1 since we didn't log-transform y\n",
    "\n",
    "# 6. Create the final submission file\n",
    "submission_df = pd.DataFrame({\n",
    "    'Hospital_Id': submission_ids,\n",
    "    'Transport_Cost': final_predictions\n",
    "})\n",
    "\n",
    "# Display the first few predictions\n",
    "print(\"\\nFinal Predictions:\")\n",
    "display(submission_df.head())\n",
    "\n",
    "# Save to CSV\n",
    "submission_df.to_csv('submission1.csv', index=False)\n",
    "print(\"Submission file 'submission.csv' created successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MlLab)",
   "language": "python",
   "name": "mllab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
