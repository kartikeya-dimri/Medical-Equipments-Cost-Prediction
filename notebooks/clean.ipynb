{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad8d2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# core\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# data + plotting\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "from scipy.stats import zscore\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "# sklearn (preprocessing / pipeline / model selection / metrics)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# classical models (if you use them elsewhere)\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# gradient boosting / lightgbm / xgboost\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# utilities\n",
    "import joblib   # optional: save/load pipeline\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4617a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "\n",
    "# 1ï¸âƒ£ Load Data\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv('../data/train.csv')\n",
    "df.columns = df.columns.str.strip()\n",
    "display(df.head())\n",
    "print(f\"Initial data shape: {df.shape}\")\n",
    "\n",
    "# 2ï¸âƒ£ Clean all string/object columns: strip spaces, replace blanks with NaN\n",
    "print(\"Cleaning string columns...\")\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    df[col] = df[col].astype(str).str.strip()\n",
    "    df[col] = df[col].replace({'': np.nan, 'nan': np.nan, 'NaN': np.nan})\n",
    "\n",
    "# 3ï¸âƒ£ Normalize Yes/No columns to consistent \"Yes\"/\"No\"\n",
    "print(\"Normalizing Yes/No columns...\")\n",
    "yes_no_cols = ['CrossBorder_Shipping', 'Urgent_Shipping', 'Installation_Service',\n",
    "               'Fragile_Equipment', 'Rural_Hospital']\n",
    "for col in yes_no_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace({\n",
    "            'YES': 'Yes', 'yes': 'Yes', 'Y': 'Yes', 'y': 'Yes',\n",
    "            'NO': 'No', 'no': 'No', 'N': 'No', 'n': 'No'\n",
    "        })\n",
    "\n",
    "# 4ï¸âƒ£ Convert date columns to datetime\n",
    "print(\"Converting date columns...\")\n",
    "df['Order_Placed_Date'] = pd.to_datetime(df['Order_Placed_Date'], errors='coerce')\n",
    "df['Delivery_Date'] = pd.to_datetime(df['Delivery_Date'], errors='coerce')\n",
    "\n",
    "# 5ï¸âƒ£ Create new feature: Delivery_Days (difference in days)\n",
    "print(\"Engineering Delivery_Days feature...\")\n",
    "df['Delivery_Days'] = (df['Delivery_Date'] - df['Order_Placed_Date']).dt.days\n",
    "df['Delivery_Days'] = pd.to_numeric(df['Delivery_Days'], errors='coerce')\n",
    "\n",
    "# === ADDED: Date Feature Engineering ===\n",
    "print(\"Engineering more date features...\")\n",
    "df['Order_Month'] = df['Order_Placed_Date'].dt.month\n",
    "df['Order_Day_of_Week'] = df['Order_Placed_Date'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "df['Order_Is_Weekend'] = df['Order_Day_of_Week'].isin([5, 6])\n",
    "# === END ADDED ===\n",
    "\n",
    "# 6ï¸âƒ£ (Original) delete initial date rows\n",
    "# df = df.dropna(subset=['Order_Placed_Date', 'Delivery_Date'])\n",
    "\n",
    "# 7ï¸âƒ£ Drop exact duplicate rows\n",
    "print(\"Dropping duplicates...\")\n",
    "before = len(df)\n",
    "df = df.drop_duplicates()\n",
    "after = len(df)\n",
    "print(f\"Dropped {before - after} duplicate rows.\")\n",
    "\n",
    "# 8ï¸âƒ£ Quick check after cleaning\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\" CLEANING & FEATURE ENGINEERING COMPLETE \")\n",
    "print(\"=\"*30)\n",
    "print(f\"After basic cleaning shape: {df.shape}\")\n",
    "\n",
    "print(\"\\nMissing values (raw count):\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "# === ADDED: Missing Value Percentage View ===\n",
    "print(\"\\nMissing values (percentage):\")\n",
    "missing_pct = (df.isna().sum() / len(df) * 100).sort_values(ascending=False)\n",
    "print(missing_pct[missing_pct > 0])\n",
    "# === END ADDED ===\n",
    "\n",
    "print(\"\\nDataFrame head:\")\n",
    "display(df.head())\n",
    "# print(df['Delivery_Days'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6847d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# ðŸ“Š START OF EXPLORATORY DATA ANALYSIS (EDA)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\" STARTING EDA \")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# ðŸ”¹ Define column lists\n",
    "num_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "# === ADDED: Exclude new date features from 'num_cols' for general stats ===\n",
    "date_num_features = ['Order_Month', 'Order_Day_of_Week', 'Delivery_Days']\n",
    "for col in ['Transport_Cost'] + date_num_features:\n",
    "    if col in num_cols:\n",
    "        num_cols.remove(col)\n",
    "# === END ADDED ===\n",
    "        \n",
    "cat_cols = df.select_dtypes(include='object').columns.tolist()\n",
    "# === ADDED: Add boolean 'Is_Weekend' to cat_cols for analysis ===\n",
    "if 'Order_Is_Weekend' in df.columns:\n",
    "    cat_cols.append('Order_Is_Weekend')\n",
    "# === END ADDED ===\n",
    "\n",
    "print(f\"Numeric features identified: {num_cols}\")\n",
    "print(f\"Categorical features identified: {cat_cols}\")\n",
    "print(f\"Date-derived features identified: {date_num_features}\")\n",
    "\n",
    "\n",
    "# === ADDED: 1. Target Variable Analysis (Transport_Cost) ===\n",
    "print(\"\\n===== 1. TARGET VARIABLE ANALYSIS: Transport_Cost =====\")\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Original Distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['Transport_Cost'], kde=True, bins=40)\n",
    "plt.title('Distribution of Transport_Cost (Original)')\n",
    "plt.xlabel('Transport_Cost')\n",
    "\n",
    "# Plot 2: Log-Transformed Distribution\n",
    "# We add 1 to handle potential zero values before logging\n",
    "plt.subplot(1, 2, 2)\n",
    "log_target = np.log1p(df['Transport_Cost'])\n",
    "sns.histplot(log_target, kde=True, bins=40, color='green')\n",
    "plt.title('Distribution of log(Transport_Cost + 1)')\n",
    "plt.xlabel('log(Transport_Cost + 1)')\n",
    "\n",
    "plt.suptitle('Target Variable Distribution Analysis', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "print(f\"Skewness of Transport_Cost: {df['Transport_Cost'].skew():.4f}\")\n",
    "print(f\"Skewness of log(Transport_Cost + 1): {log_target.skew():.4f}\")\n",
    "# === END ADDED ===\n",
    "\n",
    "\n",
    "print(\"\\n===== 2. NUMERIC FEATURE ANALYSIS =====\")\n",
    "print(\"===== BASIC NUMERIC STATISTICS =====\")\n",
    "if not num_cols:\n",
    "    print(\"No numeric columns found to describe (excluding target/dates).\")\n",
    "else:\n",
    "    display(df[num_cols].describe().T)\n",
    "\n",
    "    print(\"\\n===== SKEWNESS =====\")\n",
    "    display(df[num_cols].skew())\n",
    "\n",
    "# ðŸ”¹ Numeric distributions + boxplots\n",
    "# (Your original loop)\n",
    "# === MODIFIED: Added a check for empty list ===\n",
    "print(\"\\nGenerating numeric distribution plots...\")\n",
    "analysis_num_cols = num_cols + ['Delivery_Days'] # Add Delivery_Days back for plotting\n",
    "if 'Transport_Cost' not in analysis_num_cols:\n",
    "    analysis_num_cols.append('Transport_Cost') # Add Target back for plotting\n",
    "    \n",
    "for col in analysis_num_cols:\n",
    "    if col in df.columns:\n",
    "        plt.figure(figsize=(12,4))\n",
    "        \n",
    "        plt.subplot(1,2,1)\n",
    "        sns.histplot(df[col], kde=True, bins=30)\n",
    "        plt.title(f'{col} distribution')\n",
    "        \n",
    "        plt.subplot(1,2,2)\n",
    "        sns.boxplot(x=df[col])\n",
    "        plt.title(f'{col} boxplot')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Warning: Column '{col}' not found for plotting.\")\n",
    "\n",
    "\n",
    "print(\"\\n===== 3. CORRELATION ANALYSIS =====\")\n",
    "# ðŸ”¹ Correlation heatmap\n",
    "# (Your original code)\n",
    "plt.figure(figsize=(10,8))\n",
    "corr = df[num_cols + ['Transport_Cost', 'Delivery_Days']].corr()\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n===== 4. CATEGORICAL FEATURE ANALYSIS =====\")\n",
    "# ðŸ”¹ Categorical distributions\n",
    "# (Your original loop)\n",
    "print(\"\\nGenerating categorical distribution plots...\")\n",
    "high_cardinality_cols = []\n",
    "for col in cat_cols:\n",
    "    print(f\"\\n===== Column: {col} =====\")\n",
    "    print(df[col].value_counts(dropna=False))\n",
    "    \n",
    "    nunique = df[col].nunique()\n",
    "    if nunique > 20:\n",
    "        high_cardinality_cols.append(col)\n",
    "        print(f\"SKIPPING countplot for {col} (High Cardinality: {nunique} unique values)\")\n",
    "        continue\n",
    "        \n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.countplot(y=col, data=df, order=df[col].value_counts().index)\n",
    "    plt.title(f'Count of {col}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# === ADDED: 4a. High-Cardinality Column Summary ===\n",
    "print(\"\\n===== 4a. HIGH-CARDINALITY CATEGORICAL SUMMARY =====\")\n",
    "if high_cardinality_cols:\n",
    "    print(f\"High-cardinality features detected: {high_cardinality_cols}\")\n",
    "    for col in high_cardinality_cols:\n",
    "        print(f\"\\n--- Top 10 values for: {col} ---\")\n",
    "        print(df[col].value_counts(dropna=False).head(10))\n",
    "        print(f\"...and {df[col].nunique() - 10} other unique values.\")\n",
    "else:\n",
    "    print(\"No high-cardinality categorical features detected (threshold > 20).\")\n",
    "# === END ADDED ===\n",
    "\n",
    "\n",
    "print(\"\\n===== 5. BIVARIATE ANALYSIS (FEATURES vs. TARGET) =====\")\n",
    "# ðŸ”¹ Numeric features vs target\n",
    "# (Your original loop)\n",
    "print(\"\\nGenerating numeric features vs. Transport_Cost...\")\n",
    "for col in num_cols + ['Delivery_Days']:\n",
    "    if col in df.columns:\n",
    "        plt.figure(figsize=(6,4))\n",
    "        sns.scatterplot(x=df[col], y=df['Transport_Cost'])\n",
    "        plt.title(f'{col} vs Transport_Cost')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# ðŸ”¹ Categorical features vs target (low-cardinality)\n",
    "# (Your original loop)\n",
    "print(\"\\nGenerating categorical features vs. Transport_Cost...\")\n",
    "for col in cat_cols:\n",
    "    if col in df.columns and df[col].nunique() < 20:\n",
    "        plt.figure(figsize=(10,4))\n",
    "        sns.boxplot(x=col, y='Transport_Cost', data=df)\n",
    "        plt.title(f'{col} vs Transport_Cost')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# === ADDED: 5a. Date-Derived Features vs. Target ===\n",
    "print(\"\\nGenerating date-derived features vs. Transport_Cost...\")\n",
    "date_features_to_plot = ['Order_Month', 'Order_Day_of_Week', 'Order_Is_Weekend']\n",
    "for col in date_features_to_plot:\n",
    "    if col in df.columns:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        sns.boxplot(x=col, y='Transport_Cost', data=df)\n",
    "        plt.title(f'{col} vs Transport_Cost')\n",
    "        if col == 'Order_Day_of_Week':\n",
    "            plt.xticks(ticks=range(7), labels=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "# === END ADDED ===\n",
    "\n",
    "\n",
    "print(\"\\n===== 6. OUTLIER DETECTION =====\")\n",
    "# ðŸ”¹ Outlier detection (Z-score)\n",
    "# (Your original code)\n",
    "# === MODIFIED: Added nan_policy='omit' to handle missing values gracefully ===\n",
    "try:\n",
    "    z_scores = df[num_cols + ['Transport_Cost', 'Delivery_Days']].apply(lambda x: zscore(x, nan_policy='omit'))\n",
    "    outliers = (abs(z_scores) > 3).sum()\n",
    "    print(\"\\n===== NUMBER OF OUTLIERS PER COLUMN (Z-score > 3) =====\")\n",
    "    print(outliers[outliers > 0].sort_values(ascending=False))\n",
    "except ValueError as e:\n",
    "    print(f\"Could not calculate Z-scores, likely due to all-NaN column. Error: {e}\")\n",
    "# === END MODIFIED ===\n",
    "\n",
    "\n",
    "print(\"\\n===== 7. MISSING VALUE VISUALIZATION =====\")\n",
    "# ðŸ”¹ Missing value visualization\n",
    "# (Your original code)\n",
    "print(\"\\nGenerating missing value matrix...\")\n",
    "msno.matrix(df)\n",
    "plt.title('Missing Value Matrix')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nGenerating missing value bar chart...\")\n",
    "msno.bar(df)\n",
    "plt.title('Missing Value Bar Chart')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\" EDA COMPLETE \")\n",
    "print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7793e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===== 1. INVESTIGATING NEGATIVE DURATION =====\")\n",
    "# We must use the original 'df' before any rows are dropped\n",
    "try:\n",
    "    neg_days = df[df['Delivery_Days'] < 0]['Delivery_Days']\n",
    "    print(f\"Found {len(neg_days)} negative duration rows.\")\n",
    "    \n",
    "    print(\"\\n--- Top 10 most common negative values: ---\")\n",
    "    print(neg_days.value_counts().head(10))\n",
    "    \n",
    "    print(\"\\n--- Stats for negative values (min, max, mean): ---\")\n",
    "    print(neg_days.describe())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error checking Delivery_Days: {e}\")\n",
    "    print(\"Hint: Make sure 'df' is your original DataFrame and 'Delivery_Days' is created.\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n===== 2. INVESTIGATING NEGATIVE COST =====\")\n",
    "try:\n",
    "    neg_costs = df[df['Transport_Cost'] < 0]['Transport_Cost']\n",
    "    print(f\"Found {len(neg_costs)} negative cost rows.\")\n",
    "    \n",
    "    print(\"\\n--- Top 10 most common negative values: ---\")\n",
    "    print(neg_costs.value_counts().head(10))\n",
    "    \n",
    "    print(\"\\n--- Stats for negative values (min, max, mean): ---\")\n",
    "    print(neg_costs.describe())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error checking Transport_Cost: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8566a161",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===== 3. INVESTIGATING WEIGHT vs. VALUE =====\")\n",
    "try:\n",
    "    # Calculate correlation\n",
    "    correlation = df['Equipment_Weight'].corr(df['Equipment_Value'])\n",
    "    print(f\"Correlation (Weight vs. Value): {correlation:.4f}\")\n",
    "\n",
    "    # Check for missing/zero values in each\n",
    "    weight_zeros = (df['Equipment_Weight'] == 0).sum()\n",
    "    weight_nans = df['Equipment_Weight'].isna().sum()\n",
    "    weight_missing_pct = (weight_zeros + weight_nans) / len(df) * 100\n",
    "    \n",
    "    value_zeros = (df['Equipment_Value'] == 0).sum()\n",
    "    value_nans = df['Equipment_Value'].isna().sum()\n",
    "    value_missing_pct = (value_zeros + value_nans) / len(df) * 100\n",
    "\n",
    "    print(f\"\\n--- Missing Data Stats ---\")\n",
    "    print(f\"Equipment_Weight: {weight_missing_pct:.2f}% missing (as 0 or NaN)\")\n",
    "    print(f\"Equipment_Value:  {value_missing_pct:.2f}% missing (as 0 or NaN)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error checking Weight vs. Value: {e}\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n===== 4. INVESTIGATING HEIGHT vs. WIDTH =====\")\n",
    "try:\n",
    "    # Calculate correlation\n",
    "    correlation = df['Equipment_Height'].corr(df['Equipment_Width'])\n",
    "    print(f\"Correlation (Height vs. Width): {correlation:.4f}\")\n",
    "\n",
    "    # Check for missing/zero values in each\n",
    "    height_zeros = (df['Equipment_Height'] == 0).sum()\n",
    "    height_nans = df['Equipment_Height'].isna().sum()\n",
    "    height_missing_pct = (height_zeros + height_nans) / len(df) * 100\n",
    "    \n",
    "    width_zeros = (df['Equipment_Width'] == 0).sum()\n",
    "    width_nans = df['Equipment_Width'].isna().sum()\n",
    "    width_missing_pct = (width_zeros + width_nans) / len(df) * 100\n",
    "\n",
    "    print(f\"\\n--- Missing Data Stats ---\")\n",
    "    print(f\"Equipment_Height: {height_missing_pct:.2f}% missing (as 0 or NaN)\")\n",
    "    print(f\"Equipment_Width:  {width_missing_pct:.2f}% missing (as 0 or NaN)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error checking Height vs. Width: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ad723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\" ROBUST PREPROCESSING SCRIPT - MEDICAL EQUIPMENT TRANSPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: PRE-SPLIT DATA CLEANING & FEATURE ENGINEERING\n",
    "# (Assuming 'df' is loaded from cell 7f4617a9, with 'Delivery_Days' created)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n[1/10] Repairing impossible negative values (Problem 1)...\")\n",
    "# Based on EDA, negative values are typos/swapped dates, not missing data.\n",
    "# We will use .abs() to repair them and preserve all rows.\n",
    "neg_days = (df['Delivery_Days'] < 0).sum()\n",
    "if neg_days > 0:\n",
    "    df['Delivery_Days'] = df['Delivery_Days'].abs()\n",
    "    print(f\"   âœ“ Repaired {neg_days} rows with negative 'Delivery_Days' using .abs()\")\n",
    "else:\n",
    "    print(\"   âœ“ No negative 'Delivery_Days' found.\")\n",
    "\n",
    "# Note: 'Transport_Cost' is repaired in Step 4\n",
    "\n",
    "# ==============================================================================\n",
    "print(\"\\n[2/10] Feature Engineering - Volume (Problem 4)...\")\n",
    "# Combine correlated Height & Width into a single 'Volume' feature.\n",
    "# This solves multicollinearity and simplifies imputation.\n",
    "df['Equipment_Volume'] = df['Equipment_Height'] * df['Equipment_Width']\n",
    "print(f\"   âœ“ Created 'Equipment_Volume' from Height * Width\")\n",
    "\n",
    "# ==============================================================================\n",
    "print(\"\\n[3/10] Log-transforming heavily skewed features...\")\n",
    "# Apply log-transform to normalize extreme right-skewed features\n",
    "df['Equipment_Value'] = np.log1p(df['Equipment_Value'])\n",
    "df['Equipment_Volume'] = np.log1p(df['Equipment_Volume'])\n",
    "print(f\"   âœ“ Log-transformed 'Equipment_Value'\")\n",
    "print(f\"   âœ“ Log-transformed 'Equipment_Volume'\")\n",
    "\n",
    "# ==============================================================================\n",
    "print(\"\\n[4/10] Defining target variable (y) (Problem 2)...\")\n",
    "# Repair negative 'Transport_Cost' typos FIRST, then log-transform.\n",
    "neg_costs = (df['Transport_Cost'] < 0).sum()\n",
    "if neg_costs > 0:\n",
    "    print(f\"   âœ“ Repairing {neg_costs} rows with negative 'Transport_Cost' using .abs()\")\n",
    "\n",
    "# Apply .abs() to fix typos, then np.log1p() to normalize the target\n",
    "y = np.log1p(df['Transport_Cost'].abs())\n",
    "\n",
    "print(f\"   âœ“ Target (y) created using log1p(abs(Transport_Cost))\")\n",
    "print(f\"   âœ“ Target shape: {y.shape}\")\n",
    "\n",
    "# ==============================================================================\n",
    "print(\"\\n[5/10] Selecting features (X) for modeling (Problem 3 & 4)...\")\n",
    "# Define the final feature set, dropping redundant/replaced columns.\n",
    "drop_cols = [\n",
    "    # Target\n",
    "    'Transport_Cost',\n",
    "    \n",
    "    # Replaced by Equipment_Volume\n",
    "    'Equipment_Height', \n",
    "    'Equipment_Width',\n",
    "    \n",
    "    # Redundant & less reliable (Problem 3)\n",
    "    'Equipment_Weight',\n",
    "    \n",
    "    # High-cardinality IDs / Unused\n",
    "    'Hospital_Id',\n",
    "    'Supplier_Name', \n",
    "    'Hospital_Location',\n",
    "    \n",
    "    # Replaced by engineered date features\n",
    "    'Order_Placed_Date',\n",
    "    'Delivery_Date'\n",
    "]\n",
    "\n",
    "X = df.drop(columns=drop_cols)\n",
    "print(f\"   âœ“ Dropped {len(drop_cols)} columns (incl. Weight, Height, Width)\")\n",
    "print(f\"   âœ“ Remaining features: {X.shape[1]}\")\n",
    "\n",
    "# ==============================================================================\n",
    "print(\"\\n[6/10] Train-test split (80/20)...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "print(f\"   âœ“ Training set: {X_train.shape}\")\n",
    "print(f\"   âœ“ Test set:     {X_test.shape}\")\n",
    "\n",
    "# ==============================================================================\n",
    "print(\"\\n[7/10] Baseline performance (predicting mean)...\")\n",
    "train_mean = y_train.mean()\n",
    "y_test_pred_baseline = np.full_like(y_test, train_mean)\n",
    "\n",
    "# Baseline RMSE in log-space\n",
    "baseline_rmse_log = np.sqrt(mean_squared_error(y_test, y_test_pred_baseline))\n",
    "print(f\"   âœ“ Baseline RMSE (log-space):    {baseline_rmse_log:.4f}\")\n",
    "\n",
    "# Baseline RMSE in original scale\n",
    "y_test_actual_orig = np.expm1(y_test)\n",
    "y_test_baseline_orig = np.expm1(y_test_pred_baseline)\n",
    "baseline_rmse_orig = np.sqrt(mean_squared_error(\n",
    "    y_test_actual_orig, y_test_baseline_orig\n",
    "))\n",
    "print(f\"   âœ“ Baseline RMSE (original-scale): ${baseline_rmse_orig:,.2f}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: POST-SPLIT PIPELINES (Prevents Data Leakage)\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" BUILDING PREPROCESSING PIPELINES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n[8/10] Configuring feature transformers (Problem 5)...\")\n",
    "\n",
    "# --- Numeric Features ---\n",
    "# NOTE: 'Equipment_Weight' is GONE. 'Equipment_Volume' is IN.\n",
    "numeric_features = [\n",
    "    'Supplier_Reliability', \n",
    "    'Equipment_Value',      # Log-transformed\n",
    "    'Base_Transport_Fee', \n",
    "    'Delivery_Days',        # Repaired\n",
    "    'Equipment_Volume'      # Engineered & Log-transformed\n",
    "]\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Robust to outliers\n",
    "    ('scaler', StandardScaler())                    # Standardize\n",
    "])\n",
    "print(f\"   âœ“ Numeric features ({len(numeric_features)}): median imputation + scaling\")\n",
    "\n",
    "# --- Categorical Features ---\n",
    "categorical_features = [\n",
    "    'Equipment_Type',\n",
    "    'CrossBorder_Shipping',\n",
    "    'Urgent_Shipping',\n",
    "    'Installation_Service',\n",
    "    'Transport_Method',\n",
    "    'Fragile_Equipment',\n",
    "    'Hospital_Info',\n",
    "    'Rural_Hospital',\n",
    "    'Order_Month',\n",
    "    'Order_Day_of_Week',\n",
    "    'Order_Is_Weekend'\n",
    "]\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')), # Mode for categoricals\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "print(f\"   âœ“ Categorical features ({len(categorical_features)}): mode imputation + one-hot\")\n",
    "\n",
    "# ==============================================================================\n",
    "print(\"\\n[9/10] Assembling ColumnTransformer...\")\n",
    "# This preprocessor is the \"heart\" of your model pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'  # Drop any columns we didn't explicitly list\n",
    ")\n",
    "print(f\"   âœ“ ColumnTransformer configured\")\n",
    "\n",
    "# ==============================================================================\n",
    "print(\"\\n[10/10] Applying preprocessing (fitting on train, transforming both)...\")\n",
    "\n",
    "# FIT on training data only\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# TRANSFORM both sets\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"   âœ“ Training set processed: {X_train_processed.shape}\")\n",
    "print(f\"   âœ“ Test set processed:     {X_test_processed.shape}\")\n",
    "\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" PREPROCESSING COMPLETE! âœ“\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nKey Decisions Implemented:\")\n",
    "print(\"   1. âœ“ REPAIRED all negative costs/durations using .abs()\")\n",
    "print(\"   2. âœ“ KEPT all 5,000 training rows (no data loss)\")\n",
    "print(\"   3. âœ“ DROPPED 'Equipment_Weight' (redundant with 'Value')\")\n",
    "print(\"   4. âœ“ COMBINED 'Height'/'Width' into 'Equipment_Volume'\")\n",
    "print(\"   5. âœ“ USED robust 'median'/'mode' imputation\")\n",
    "print(\"   6. âœ“ LOG-TRANSFORMED target and skewed features\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0a25e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0dbad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----- 1) Set up 5-Fold -----\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# ----- 2) Create the pipeline -----\n",
    "lr_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),  # your ColumnTransformer\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "# ----- 3) Run 5-Fold Cross-Validation -----\n",
    "cv_scores = cross_val_score(\n",
    "    lr_pipeline,\n",
    "    X_train,       # raw training features\n",
    "    y_train,       # log-transformed target\n",
    "    cv=kf,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "avg_rmse = np.abs(cv_scores).mean()\n",
    "print(f\"Linear Regression 5-Fold Avg. RMSE (log-space): {avg_rmse:.4f}\")\n",
    "\n",
    "# ----- 4) Fit final model on full training data -----\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "# print(\"Linear Regression final model trained on full training set.\")\n",
    "\n",
    "# ----- 5) Predict on test set -----\n",
    "y_test_pred_log = lr_pipeline.predict(X_test)\n",
    "y_test_pred_orig = np.expm1(y_test_pred_log)  # back to original scale\n",
    "\n",
    "rmse_test_log = np.sqrt(np.mean((y_test - y_test_pred_log)**2))\n",
    "rmse_test_orig = np.sqrt(np.mean((np.expm1(y_test) - y_test_pred_orig)**2))\n",
    "\n",
    "print(f\"Test RMSE (log-space)      : {rmse_test_log:.4f}\")  \n",
    "print(f\"Test RMSE (original scale) : {rmse_test_orig:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4273a8c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317ee06b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe61007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----- 1) Set up 5-Fold -----\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# ----- 2) Create the pipeline -----\n",
    "poly_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),         # your ColumnTransformer\n",
    "    ('poly', PolynomialFeatures()),         # will tune degree\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "# ----- 3) Set up GridSearch for hyperparameter tuning -----\n",
    "param_grid = {\n",
    "    'poly__degree': [2,3]   # you can expand to 5 if dataset is small\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    poly_pipeline,\n",
    "    param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=kf,\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "# ----- 4) Run GridSearch -----\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best degree\n",
    "best_degree = grid_search.best_params_['poly__degree']\n",
    "best_rmse = -grid_search.best_score_\n",
    "print(f\"Best polynomial degree: {best_degree}\")\n",
    "print(f\"Best CV RMSE (log-space): {best_rmse:.4f}\")\n",
    "\n",
    "# ----- 5) Fit final model on full training data -----\n",
    "final_poly_model = grid_search.best_estimator_\n",
    "final_poly_model.fit(X_train, y_train)\n",
    "print(\"Polynomial Regression final model trained on full training set.\")\n",
    "\n",
    "# ----- 6) Predict on test set -----\n",
    "y_test_pred_log = final_poly_model.predict(X_test)\n",
    "y_test_pred_orig = np.expm1(y_test_pred_log)  # back to original scale\n",
    "\n",
    "rmse_test_log = np.sqrt(np.mean((y_test - y_test_pred_log)**2))\n",
    "rmse_test_orig = np.sqrt(np.mean((np.expm1(y_test) - y_test_pred_orig)**2))\n",
    "\n",
    "print(f\"Test RMSE (log-space)      : {rmse_test_log:.4f}\")\n",
    "print(f\"Test RMSE (original scale) : {rmse_test_orig:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2959af42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----- 1) Create the pipeline -----\n",
    "ridge_poly_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),  # your ColumnTransformer\n",
    "    ('poly', PolynomialFeatures()),  # polynomial expansion\n",
    "    ('ridge', Ridge())               # ridge regression\n",
    "])\n",
    "\n",
    "# ----- 2) Set hyperparameter grid -----\n",
    "param_grid = {\n",
    "    'poly__degree': [2,3],      # try degrees 1, 2, 3, 4, 5\n",
    "    'ridge__alpha': [ 0.01,0.1,1,10,100]  # try different regularization strengths\n",
    "}\n",
    "\n",
    "# ----- 3) Grid Search with 5-Fold CV -----\n",
    "grid_search = GridSearchCV(\n",
    "    ridge_poly_pipeline,\n",
    "    param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=5,\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "# ----- 4) Fit on training data -----\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# ----- 5) Best hyperparameters -----\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best CV RMSE (log-space):\", -grid_search.best_score_)\n",
    "\n",
    "# ----- 6) Predict on test set -----\n",
    "y_test_pred_log = grid_search.predict(X_test)\n",
    "y_test_pred_orig = np.expm1(y_test_pred_log)\n",
    "\n",
    "rmse_test_log = np.sqrt(np.mean((y_test - y_test_pred_log)**2))\n",
    "rmse_test_orig = np.sqrt(np.mean((np.expm1(y_test) - y_test_pred_orig)**2))\n",
    "\n",
    "print(f\"Test RMSE (log-space)      : {rmse_test_log:.4f}\")\n",
    "print(f\"Test RMSE (original scale) : {rmse_test_orig:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba86e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----- 1) Create the pipeline -----\n",
    "lasso_poly_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),  # your ColumnTransformer\n",
    "    ('poly', PolynomialFeatures()),  # polynomial expansion\n",
    "    ('lasso', Lasso(max_iter=10000)) # Lasso regression\n",
    "])\n",
    "\n",
    "# ----- 2) Set hyperparameter grid -----\n",
    "param_grid = {\n",
    "    'poly__degree': [2,3],       # try degrees 1, 2, 3\n",
    "    'lasso__alpha': [0.001, 0.01, 0.1, 1]  # regularization strengths\n",
    "}\n",
    "\n",
    "# ----- 3) Grid Search with 5-Fold CV -----\n",
    "grid_search = GridSearchCV(\n",
    "    lasso_poly_pipeline,\n",
    "    param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# ----- 4) Fit on training data -----\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# ----- 5) Best hyperparameters -----\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best CV RMSE (log-space):\", -grid_search.best_score_)\n",
    "\n",
    "# ----- 6) Predict on test set -----\n",
    "y_test_pred_log = grid_search.predict(X_test)\n",
    "y_test_pred_orig = np.expm1(y_test_pred_log)\n",
    "\n",
    "rmse_test_log = np.sqrt(np.mean((y_test - y_test_pred_log)**2))\n",
    "rmse_test_orig = np.sqrt(np.mean((np.expm1(y_test) - y_test_pred_orig)**2))\n",
    "\n",
    "print(f\"Test RMSE (log-space)      : {rmse_test_log:.4f}\")\n",
    "print(f\"Test RMSE (original scale) : {rmse_test_orig:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099a12f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----- 1) CV splitter -----\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# ----- 2) Pipeline: preprocessor -> poly -> elastic net -----\n",
    "enet_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),                       # your ColumnTransformer\n",
    "    ('poly', PolynomialFeatures(include_bias=False)),     # polynomial expansion (tune degree)\n",
    "    ('enet', ElasticNet(max_iter=20000, random_state=42)) # ElasticNet regression\n",
    "])\n",
    "\n",
    "# ----- 3) Hyperparameter grid -----\n",
    "param_grid = {\n",
    "    'poly__degree': [2,3],                      # try degrees 1..3 (increase carefully)\n",
    "    'enet__alpha': [0.01,0.1, 1],   # regularization strengths\n",
    "    'enet__l1_ratio': [0.01, 0.1,0.2]               # mix between L1 (1.0) and L2 (0.0)\n",
    "}\n",
    "\n",
    "# ----- 4) GridSearchCV (use n_jobs=1 in notebooks to avoid multiprocessing cwd issues) -----\n",
    "grid_search = GridSearchCV(\n",
    "    enet_pipeline,\n",
    "    param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=kf,\n",
    "    n_jobs=1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# ----- 5) Run grid search -----\n",
    "print(\"Starting GridSearchCV for ElasticNet + PolynomialFeatures ...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# ----- 6) Best params & CV score -----\n",
    "best_params = grid_search.best_params_\n",
    "best_cv_rmse = -grid_search.best_score_\n",
    "print(f\"\\nBest hyperparameters: {best_params}\")\n",
    "print(f\"Best CV RMSE (log-space): {best_cv_rmse:.4f}\")\n",
    "\n",
    "# ----- 7) Final model (best estimator) -----\n",
    "final_enet = grid_search.best_estimator_\n",
    "\n",
    "# ----- 8) Evaluate on test set -----\n",
    "y_test_pred_log = final_enet.predict(X_test)\n",
    "y_test_pred_orig = np.expm1(y_test_pred_log)\n",
    "\n",
    "rmse_test_log = np.sqrt(mean_squared_error(y_test, y_test_pred_log))\n",
    "rmse_test_orig = np.sqrt(mean_squared_error(np.expm1(y_test), y_test_pred_orig))\n",
    "\n",
    "print(f\"\\nTest RMSE (log-space)      : {rmse_test_log:.4f}\")\n",
    "print(f\"Test RMSE (original scale) : {rmse_test_orig:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e282f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----- 1) CV splitter -----\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# ----- 2) XGB pipeline (preprocessor -> xgb) -----\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('xgb', XGBRegressor(objective='reg:squarederror',\n",
    "                         random_state=42,\n",
    "                         n_jobs=-1,\n",
    "                         tree_method='hist'))  # 'hist' is faster for larger data\n",
    "])\n",
    "\n",
    "# ----- 3) Hyperparameter grid (example) -----\n",
    "param_grid = {\n",
    "    'xgb__n_estimators': [100, 300],\n",
    "    'xgb__max_depth': [3, 6],\n",
    "    'xgb__learning_rate': [0.01, 0.1],\n",
    "    'xgb__subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# ----- 4) GridSearchCV -----\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=kf,\n",
    "    n_jobs=1,   # use 1 in notebooks to avoid multiprocessing cwd issues\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# ----- 5) Run grid search -----\n",
    "print(\"Starting GridSearchCV for XGBoost...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# ----- 6) Best params and CV score -----\n",
    "best_params = grid_search.best_params_\n",
    "best_cv_rmse = -grid_search.best_score_\n",
    "print(\"\\nBest hyperparameters:\", best_params)\n",
    "print(f\"Best CV RMSE (log-space): {best_cv_rmse:.4f}\")\n",
    "\n",
    "# ----- 7) Final model (best estimator) -----\n",
    "final_xgb = grid_search.best_estimator_\n",
    "\n",
    "# ----- 8) Evaluate on test set -----\n",
    "y_test_pred_log = final_xgb.predict(X_test)\n",
    "y_test_pred_orig = np.expm1(y_test_pred_log)\n",
    "\n",
    "rmse_test_log = np.sqrt(mean_squared_error(y_test, y_test_pred_log))\n",
    "rmse_test_orig = np.sqrt(mean_squared_error(np.expm1(y_test), y_test_pred_orig))\n",
    "\n",
    "print(f\"\\nTest RMSE (log-space)      : {rmse_test_log:.4f}\")\n",
    "print(f\"Test RMSE (original scale) : {rmse_test_orig:.2f}\")\n",
    "\n",
    "# ----- 9) (Optional) Feature importances mapped to feature names -----\n",
    "# This extracts names from the preprocessor (numeric + one-hot cat names)\n",
    "pre = final_xgb.named_steps['preprocessor']\n",
    "ohe = pre.named_transformers_['cat'].named_steps['onehot']\n",
    "num_names = numeric_features\n",
    "cat_names = list(ohe.get_feature_names_out(categorical_features))\n",
    "feature_names = np.concatenate([num_names, cat_names])\n",
    "\n",
    "# xgboost stores feature importances by index (0..n-1)\n",
    "xgb_model = final_xgb.named_steps['xgb']\n",
    "importances = xgb_model.feature_importances_\n",
    "\n",
    "# If shapes mismatch (e.g., due to different handling), ensure lengths match before creating df\n",
    "if len(importances) == len(feature_names):\n",
    "    fi_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    print(\"\\nTop 20 XGBoost feature importances:\")\n",
    "    print(fi_df.head(20).to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nFeature importance length does not match derived feature name length. Skipping feature-name mapping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca04eea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "print(\"ðŸš€ Starting GridSearchCV for XGBoost...\")\n",
    "\n",
    "# --- 1. CV splitter ---\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# --- 2. XGB pipeline (preprocessor -> xgb) ---\n",
    "# 'preprocessor' is the robust one we built\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('xgb', XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# --- 3. Hyperparameter grid ---\n",
    "# This is a focused grid to start with\n",
    "param_grid = {\n",
    "    'xgb__n_estimators': [100, 300, 500],\n",
    "    'xgb__max_depth': [3, 5, 7],\n",
    "    'xgb__learning_rate': [0.01, 0.1]\n",
    "}\n",
    "\n",
    "# --- 4. GridSearchCV ---\n",
    "# We use n_jobs=-1 to use all your CPU cores!\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=kf,\n",
    "    n_jobs=-1,  # Use all cores to speed this up\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# --- 5. Run grid search on the training data ---\n",
    "# (X_train and y_train are from our robust script)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# --- 6. Best params & CV score ---\n",
    "print(\"\\nâœ… GridSearch complete!\")\n",
    "best_params = grid_search.best_params_\n",
    "best_cv_rmse = -grid_search.best_score_\n",
    "print(f\"   Best hyperparameters: {best_params}\")\n",
    "print(f\"   Best CV RMSE (log-space): {best_cv_rmse:.4f}\")\n",
    "\n",
    "# --- 7. Evaluate on test set ---\n",
    "final_xgb = grid_search.best_estimator_\n",
    "y_test_pred_log = final_xgb.predict(X_test)\n",
    "y_test_pred_orig = np.expm1(y_test_pred_log)\n",
    "\n",
    "rmse_test_log = np.sqrt(mean_squared_error(y_test, y_test_pred_log))\n",
    "rmse_test_orig = np.sqrt(mean_squared_error(np.expm1(y_test), y_test_pred_orig))\n",
    "\n",
    "print(f\"\\n   Test RMSE (log-space)      : {rmse_test_log:.4f}\")\n",
    "print(f\"   Test RMSE (original scale) : {rmse_test_orig:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4c0302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "print(\"\\nTraining final XGBoost model on all data...\")\n",
    "print(\"--- THIS IS YOUR NEW BEST PERFORMING MODEL ---\")\n",
    "\n",
    "# === 1. Feature groups ===\n",
    "# (From our robust script)\n",
    "numeric_features = [\n",
    "    'Supplier_Reliability', 'Equipment_Value', 'Base_Transport_Fee',\n",
    "    'Delivery_Days', 'Equipment_Volume'\n",
    "]\n",
    "categorical_features = [\n",
    "    'Equipment_Type', 'CrossBorder_Shipping', 'Urgent_Shipping',\n",
    "    'Installation_Service', 'Transport_Method', 'Fragile_Equipment',\n",
    "    'Hospital_Info', 'Rural_Hospital', 'Order_Month',\n",
    "    'Order_Day_of_Week', 'Order_Is_Weekend'\n",
    "]\n",
    "\n",
    "# === 2. Define transformers ===\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# === 3. Combine them ===\n",
    "final_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# === 4. Build final pipeline with BEST hyperparameters ===\n",
    "# (These are the winning params from your last grid search)\n",
    "print(\"   âœ“ Using best params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500}\")\n",
    "\n",
    "final_xgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', final_preprocessor),\n",
    "    ('xgb', XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        learning_rate=0.1,    # Best param\n",
    "        max_depth=3,          # Best param\n",
    "        n_estimators=500      # Best param\n",
    "    ))\n",
    "])\n",
    "\n",
    "# === 5. Fit on full dataset ===\n",
    "# (X and y are from our robust preprocessing script)\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "final_xgb_pipeline.fit(X, y)\n",
    "\n",
    "print(\"\\nâœ… Final (BEST) XGBoost model trained on entire dataset.\")\n",
    "print(\"You can now use 'final_xgb_pipeline' for predictions.\")\n",
    "print(\"Remember: predictions will be in log1p space; use np.expm1() to convert if needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b9d237",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "lgb_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('lgb', LGBMRegressor(random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'lgb__n_estimators': [100, 300],\n",
    "    'lgb__max_depth': [4, 8],\n",
    "    'lgb__learning_rate': [0.01, 0.1],\n",
    "    'lgb__num_leaves': [31, 63]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=lgb_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=kf,\n",
    "    n_jobs=1,     # safer in notebooks; use >1 or -1 in script environments\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(\"Starting GridSearchCV for LightGBM...\")\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "best_cv_rmse = -grid.best_score_\n",
    "print(f\"Best CV RMSE (log-space): {best_cv_rmse:.4f}\")\n",
    "\n",
    "# Final model: best estimator already includes the preprocessor\n",
    "final_lgb = grid.best_estimator_\n",
    "\n",
    "# Evaluate on test set\n",
    "y_test_pred_log = final_lgb.predict(X_test)\n",
    "y_test_pred_orig = np.expm1(y_test_pred_log)\n",
    "\n",
    "rmse_test_log = np.sqrt(mean_squared_error(y_test, y_test_pred_log))\n",
    "rmse_test_orig = np.sqrt(mean_squared_error(np.expm1(y_test), y_test_pred_orig))\n",
    "\n",
    "print(f\"Test RMSE (log-space): {rmse_test_log:.4f}\")\n",
    "print(f\"Test RMSE (original scale): {rmse_test_orig:.2f}\")\n",
    "\n",
    "# Optional: feature importances mapped to names (if preprocessor produces matching columns)\n",
    "pre = final_lgb.named_steps['preprocessor']\n",
    "ohe = pre.named_transformers_['cat'].named_steps['onehot']\n",
    "num_names = numeric_features\n",
    "cat_names = list(ohe.get_feature_names_out(categorical_features))\n",
    "feature_names = np.concatenate([num_names, cat_names])\n",
    "\n",
    "lgb_model = final_lgb.named_steps['lgb']\n",
    "importances = lgb_model.feature_importances_\n",
    "\n",
    "if len(importances) == len(feature_names):\n",
    "    fi_df = pd.DataFrame({'feature': feature_names, 'importance': importances}) \\\n",
    "             .sort_values('importance', ascending=False)\n",
    "    print(fi_df.head(20).to_string(index=False))\n",
    "else:\n",
    "    print(\"Warning: feature importance length != feature name length. Skipping mapping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0522d23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 1) Pipeline -----\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('rf', RandomForestRegressor(random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# ----- 2) Expanded parameter grid -----\n",
    "param_dist = {\n",
    "    'rf__n_estimators': [100, 300, 500, 700],\n",
    "    'rf__max_depth': [None, 10, 20, 30],\n",
    "    'rf__min_samples_split': [2, 5, 10],\n",
    "    'rf__min_samples_leaf': [1, 2, 4],\n",
    "    'rf__max_features': ['sqrt', 'log2', 0.5, 1.0]\n",
    "}\n",
    "\n",
    "# ----- 3) 5-Fold CV -----\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# ----- 4) Randomized Search -----\n",
    "random_search = RandomizedSearchCV(\n",
    "    rf_pipeline,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10000,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=kf,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,  # shows progress for each combination\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# ----- 5) Train -----\n",
    "print(\"ðŸš€ Starting RandomizedSearchCV for Random Forest...\")\n",
    "random_search.fit(X_train, y_train)\n",
    "print(\"âœ… RandomizedSearchCV complete.\")\n",
    "\n",
    "# ----- 6) Evaluate -----\n",
    "best_model = random_search.best_estimator_\n",
    "print(\"ðŸ”¹ Best model selected. Predicting on test set...\")\n",
    "y_pred_log = best_model.predict(X_test)\n",
    "y_pred_orig = np.expm1(y_pred_log)\n",
    "\n",
    "rmse_log = np.sqrt(mean_squared_error(y_test, y_pred_log))\n",
    "rmse_orig = np.sqrt(mean_squared_error(np.expm1(y_test), y_pred_orig))\n",
    "\n",
    "# ----- 7) Final Results -----\n",
    "print(\"\\n===== FINAL RESULTS =====\")\n",
    "print(\"âœ… Best Parameters:\", random_search.best_params_)\n",
    "print(f\"âœ… CV RMSE (log-space): {-random_search.best_score_:.4f}\")\n",
    "print(f\"âœ… Test RMSE (log-space): {rmse_log:.4f}\")\n",
    "print(f\"âœ… Test RMSE (original scale): {rmse_orig:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9c055b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\" TRAINING FINAL MODEL ON ALL DATA \")\n",
    "print(\"=\"*30)\n",
    "\n",
    "print(\"You've found the best parameters. Now, we'll train a new model using\")\n",
    "print(\"these parameters on the *entire* dataset (X and y) to create\")\n",
    "print(\"the final, production-ready model.\")\n",
    "\n",
    "# --- 1. Re-define the unfitted preprocessor ---\n",
    "# We MUST do this to get a fresh, unfitted preprocessor\n",
    "# so it can be properly fitted on the *full* X dataset.\n",
    "\n",
    "# Define Feature Lists (as before)\n",
    "numeric_features = [\n",
    "    'Supplier_Reliability', 'Equipment_Value', 'Base_Transport_Fee',\n",
    "    'Delivery_Days', 'Equipment_Volume'\n",
    "]\n",
    "categorical_features = [\n",
    "    'Equipment_Type', 'CrossBorder_Shipping', 'Urgent_Shipping',\n",
    "    'Installation_Service', 'Transport_Method', 'Fragile_Equipment',\n",
    "    'Hospital_Info', 'Rural_Hospital', 'Order_Month',\n",
    "    'Order_Day_of_Week', 'Order_Is_Weekend'\n",
    "]\n",
    "\n",
    "# Create the Numeric Pipeline (unfitted)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Create the Categorical Pipeline (unfitted)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Create the Full Preprocessor (unfitted)\n",
    "final_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# --- 2. Create the final, unfitted XGBoost pipeline ---\n",
    "# This pipeline contains the unfitted preprocessor and an unfitted model\n",
    "final_model_pipeline = Pipeline([\n",
    "    ('preprocessor', final_preprocessor),\n",
    "    ('xgb', XGBRegressor(objective='reg:squarederror',\n",
    "                         random_state=42,\n",
    "                         n_jobs=-1,\n",
    "                         tree_method='hist'))\n",
    "])\n",
    "\n",
    "# --- 3. Get best parameters from your grid search ---\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"\\nUsing best parameters: {best_params}\")\n",
    "\n",
    "# --- 4. Set the best parameters on the new pipeline ---\n",
    "final_model_pipeline.set_params(**best_params)\n",
    "\n",
    "# --- 5. Fit the final pipeline on ALL data (X, y) ---\n",
    "# This will fit the preprocessor (imputers, scalers) on ALL X\n",
    "# and then train the XGBoost model on ALL X and y.\n",
    "print(\"Fitting final model on the entire (X, y) dataset...\")\n",
    "final_model_pipeline.fit(X, y)\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "print(\"The 'final_model_pipeline' object is now your fully-trained model,\")\n",
    "print(\"ready to be saved and used for predictions.\")\n",
    "\n",
    "# --- 6. (Optional) Save your final model ---\n",
    "# You can now save this model to a file for later use.\n",
    "# import joblib\n",
    "# joblib.dump(final_model_pipeline, 'final_xgb_model.pkl')\n",
    "# print(\"\\nFinal model saved to 'final_xgb_model.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4f2727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fc0c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining final Random Forest model on all data...\")\n",
    "\n",
    "# === 1. Feature groups ===\n",
    "numeric_features = [\n",
    "    'Supplier_Reliability', 'Equipment_Value', 'Base_Transport_Fee',\n",
    "    'Delivery_Days', 'Equipment_Volume'\n",
    "]\n",
    "categorical_features = [\n",
    "    'Equipment_Type', 'CrossBorder_Shipping', 'Urgent_Shipping',\n",
    "    'Installation_Service', 'Transport_Method', 'Fragile_Equipment',\n",
    "    'Hospital_Info', 'Rural_Hospital', 'Order_Month',\n",
    "    'Order_Day_of_Week', 'Order_Is_Weekend'\n",
    "]\n",
    "\n",
    "# === 2. Define transformers ===\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# === 3. Combine them ===\n",
    "final_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# === 4. Build final pipeline with best RF hyperparameters ===\n",
    "final_rf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', final_preprocessor),\n",
    "    ('rf', RandomForestRegressor(\n",
    "        n_estimators=700,\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        max_features=1.0,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# === 5. Fit on full dataset ===\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "final_rf_pipeline.fit(X, y)\n",
    "\n",
    "print(\"\\nâœ… Final Random Forest model trained on entire dataset.\")\n",
    "print(\"You can now use final_rf_pipeline.predict(new_X) for predictions.\")\n",
    "print(\"Remember: predictions will be in log1p space; use np.expm1() to convert if needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a74a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining final Polynomial Regression (Degree 2) model on all data...\")\n",
    "\n",
    "# === 1. Feature groups ===\n",
    "# (These remain the same as your robust script)\n",
    "numeric_features = [\n",
    "    'Supplier_Reliability', 'Equipment_Value', 'Base_Transport_Fee',\n",
    "    'Delivery_Days', 'Equipment_Volume'\n",
    "]\n",
    "categorical_features = [\n",
    "    'Equipment_Type', 'CrossBorder_Shipping', 'Urgent_Shipping',\n",
    "    'Installation_Service', 'Transport_Method', 'Fragile_Equipment',\n",
    "    'Hospital_Info', 'Rural_Hospital', 'Order_Month',\n",
    "    'Order_Day_of_Week', 'Order_Is_Weekend'\n",
    "]\n",
    "\n",
    "# === 2. Define transformers ===\n",
    "# (These remain the same)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# === 3. Combine them ===\n",
    "# (This remains the same)\n",
    "final_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# === 4. Build final pipeline with PolynomialFeatures (Degree 2) ===\n",
    "# THIS IS THE MODIFIED SECTION\n",
    "final_poly_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', final_preprocessor),\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('model', LinearRegression(n_jobs=-1)) # Using LinearRegression as the model\n",
    "])\n",
    "\n",
    "# === 5. Fit on full dataset ===\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "# Fit the new polynomial pipeline\n",
    "final_poly_pipeline.fit(X, y)\n",
    "\n",
    "print(\"\\nâœ… Final Polynomial Regression (Degree 2) model trained on entire dataset.\")\n",
    "print(\"You can now use final_poly_pipeline.predict(new_X) for predictions.\")\n",
    "print(\"Remember: predictions will be in log1p space; use np.expm1() to convert if needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1b966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining final Ridge(polynomial) model on all data...\")\n",
    "\n",
    "# === 1. Feature groups ===\n",
    "numeric_features = [\n",
    "    'Supplier_Reliability', 'Equipment_Value', 'Base_Transport_Fee',\n",
    "    'Delivery_Days', 'Equipment_Volume'\n",
    "]\n",
    "categorical_features = [\n",
    "    'Equipment_Type', 'CrossBorder_Shipping', 'Urgent_Shipping',\n",
    "    'Installation_Service', 'Transport_Method', 'Fragile_Equipment',\n",
    "    'Hospital_Info', 'Rural_Hospital', 'Order_Month',\n",
    "    'Order_Day_of_Week', 'Order_Is_Weekend'\n",
    "]\n",
    "\n",
    "# === 2. Define transformers ===\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# === 3. Combine them ===\n",
    "final_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# === 4. Build final pipeline with best parameters ===\n",
    "final_ridge_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', final_preprocessor),\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),  # best poly__degree = 3\n",
    "    ('ridge', Ridge(alpha=100))                                 # best ridge__alpha = 100\n",
    "])\n",
    "\n",
    "# === 5. Fit on full dataset ===\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "final_ridge_pipeline.fit(X, y)\n",
    "\n",
    "print(\"\\nâœ… Final Ridge (poly=3, alpha=100) model trained on entire dataset.\")\n",
    "print(\"You can now use final_ridge_pipeline.predict(new_X) for predictions.\")\n",
    "print(\"Remember: predictions will be in log1p space; use np.expm1() to convert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690c252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(df_raw):\n",
    "    \"\"\"\n",
    "    Applies all manual cleaning and feature engineering\n",
    "    to match the data used for model training.\n",
    "    \n",
    "    Takes a raw DataFrame (like test.csv) and returns\n",
    "    a DataFrame ready for the model pipeline's .predict() method.\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid changing the original data\n",
    "    df = df_raw.copy()\n",
    "    \n",
    "    # 1. Clean column names (from your training script)\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    # 2. Clean all string/object columns (from your training script)\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "        df[col] = df[col].replace({'': np.nan, 'nan': np.nan, 'NaN': np.nan})\n",
    "\n",
    "    # 3. Normalize Yes/No columns (from your training script)\n",
    "    yes_no_cols = ['CrossBorder_Shipping', 'Urgent_Shipping', 'Installation_Service',\n",
    "                   'Fragile_Equipment', 'Rural_Hospital']\n",
    "    for col in yes_no_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].replace({\n",
    "                'YES': 'Yes', 'yes': 'Yes', 'Y': 'Yes', 'y': 'Yes',\n",
    "                'NO': 'No', 'no': 'No', 'N': 'No', 'n': 'No'\n",
    "            })\n",
    "\n",
    "    # 4. Convert date columns (from your training script)\n",
    "    df['Order_Placed_Date'] = pd.to_datetime(df['Order_Placed_Date'], errors='coerce')\n",
    "    df['Delivery_Date'] = pd.to_datetime(df['Delivery_Date'], errors='coerce')\n",
    "\n",
    "    # 5. Engineer Date Features (from your training script)\n",
    "    df['Delivery_Days'] = (df['Delivery_Date'] - df['Order_Placed_Date']).dt.days\n",
    "    \n",
    "    # 6. ==== OUR ROBUST FIX (Problem 1) ====\n",
    "    # Apply .abs() to fix any negative durations (55% of test data)\n",
    "    # This preserves the magnitude instead of imputing with median.\n",
    "    num_bad_delivery_days = (df['Delivery_Days'] < 0).sum()\n",
    "    df['Delivery_Days'] = df['Delivery_Days'].abs()\n",
    "    print(f\"   âœ“ Repaired {num_bad_delivery_days} invalid Delivery_Days using .abs().\")\n",
    "    \n",
    "    # Continue engineering date features\n",
    "    df['Order_Month'] = df['Order_Placed_Date'].dt.month\n",
    "    df['Order_Day_of_Week'] = df['Order_Placed_Date'].dt.dayofweek\n",
    "    df['Order_Is_Weekend'] = df['Order_Day_of_Week'].isin([5, 6])\n",
    "    \n",
    "    # 7. ==== OUR ROBUST FIX (Problem 4) ====\n",
    "    # Engineer Volume Feature\n",
    "    df['Equipment_Volume'] = df['Equipment_Height'] * df['Equipment_Width']\n",
    "\n",
    "    # 8. ==== OUR ROBUST FIX (Log-transform) ====\n",
    "    # Log-Transform Skewed Features\n",
    "    # The model was trained on these log-transformed features.\n",
    "    df['Equipment_Value'] = np.log1p(df['Equipment_Value'])\n",
    "    df['Equipment_Volume'] = np.log1p(df['Equipment_Volume'])\n",
    "    \n",
    "    # 9. Return the feature-engineered DataFrame\n",
    "    # The pipeline will automatically select the columns it needs\n",
    "    # (numeric_features, categorical_features) and drop the rest.\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41acd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume 'final_model_pipeline' is your trained model object from the previous step\n",
    "# import joblib\n",
    "# final_model_pipeline = joblib.load('final_xgb_model.pkl') # If you saved it\n",
    "\n",
    "# 1. Load your new, raw test data\n",
    "print(\"Loading new test data...\")\n",
    "# I'm using 'test.csv' as the example filename\n",
    "df_new_test = pd.read_csv('../data/test.csv') \n",
    "\n",
    "# 2. Save IDs for the final submission\n",
    "# We need to map our predictions back to the original IDs\n",
    "submission_ids = df_new_test['Hospital_Id']\n",
    "\n",
    "# 3. Apply the *exact same* feature engineering\n",
    "print(\"Applying feature engineering to new data...\")\n",
    "X_new_prepared = prepare_features(df_new_test)\n",
    "\n",
    "# 4. Get predictions\n",
    "# The pipeline will handle the rest:\n",
    "# - Selects the correct columns\n",
    "# - Imputes missing values (using 'median'/'most_frequent' from training)\n",
    "# - Scales numeric features (using 'scaler' from training)\n",
    "# - One-hot encodes categorical features (using 'onehot' from training)\n",
    "# - Runs the XGBoost model\n",
    "print(\"Getting predictions from the final model...\")\n",
    "log_predictions = final_xgb_pipeline.predict(X_new_prepared)\n",
    "\n",
    "# 5. Convert predictions back from log-scale!\n",
    "# Remember, you trained on log(Transport_Cost + 1)\n",
    "final_predictions = np.expm1(log_predictions)\n",
    "\n",
    "# 6. Create the final submission file\n",
    "submission_df = pd.DataFrame({\n",
    "    'Hospital_Id': submission_ids,\n",
    "    'Transport_Cost': final_predictions\n",
    "})\n",
    "\n",
    "# Display the first few predictions\n",
    "print(\"\\nFinal Predictions:\")\n",
    "display(submission_df.head())\n",
    "\n",
    "# Save to CSV\n",
    "submission_df.to_csv('submission1.csv', index=False)\n",
    "print(\"Submission file 'submission.csv' created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fb9f5e",
   "metadata": {},
   "source": [
    "This is 2nd Attempt : Not to be included with above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "df412888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 1: IMPORTS ===\n",
    "\n",
    "# core\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# data + plotting\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# sklearn (preprocessing / pipeline / model selection / metrics)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "# NEW: Import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# models\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d8ae8204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Initial data shape: (5000, 20)\n",
      "Cleaning string columns...\n",
      "Normalizing Yes/No columns...\n",
      "Converting date columns...\n",
      "Engineering Delivery_Days and date features...\n",
      "\n",
      "âœ… Initial load and feature engineering complete.\n"
     ]
    }
   ],
   "source": [
    "# === CELL 2: INITIAL DATA LOAD & CLEANING ===\n",
    "\n",
    "print(\"Loading data...\")\n",
    "try:\n",
    "    df = pd.read_csv('../data/train.csv')\n",
    "    df.columns = df.columns.str.strip()\n",
    "    print(f\"Initial data shape: {df.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading '../data/train.csv'. Make sure the file is in the correct path.\")\n",
    "    print(e)\n",
    "\n",
    "# Clean all string/object columns\n",
    "print(\"Cleaning string columns...\")\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    df[col] = df[col].astype(str).str.strip()\n",
    "    df[col] = df[col].replace({'': np.nan, 'nan': np.nan, 'NaN': np.nan})\n",
    "\n",
    "# Normalize Yes/No columns\n",
    "print(\"Normalizing Yes/No columns...\")\n",
    "yes_no_cols = ['CrossBorder_Shipping', 'Urgent_Shipping', 'Installation_Service',\n",
    "               'Fragile_Equipment', 'Rural_Hospital']\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    if col in yes_no_cols:\n",
    "        df[col] = df[col].replace({\n",
    "            'YES': 'Yes', 'yes': 'Yes', 'Y': 'Yes', 'y': 'Yes',\n",
    "            'NO': 'No', 'no': 'No', 'N': 'No', 'n': 'No'\n",
    "        })\n",
    "\n",
    "# Convert date columns\n",
    "print(\"Converting date columns...\")\n",
    "df['Order_Placed_Date'] = pd.to_datetime(df['Order_Placed_Date'], errors='coerce')\n",
    "df['Delivery_Date'] = pd.to_datetime(df['Delivery_Date'], errors='coerce')\n",
    "\n",
    "# Create new features\n",
    "print(\"Engineering Delivery_Days and date features...\")\n",
    "df['Delivery_Days'] = (df['Delivery_Date'] - df['Order_Placed_Date']).dt.days\n",
    "df['Order_Month'] = df['Order_Placed_Date'].dt.month\n",
    "df['Order_Day_of_Week'] = df['Order_Placed_Date'].dt.dayofweek\n",
    "df['Order_Is_Weekend'] = df['Order_Day_of_Week'].isin([5, 6])\n",
    "\n",
    "print(\"\\nâœ… Initial load and feature engineering complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9be0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 3: V4 ROBUST PREPROCESSING (USING ROBUSTSCALER) ===\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\" V4 - ROBUST PREPROCESSING (NO CLIPPING, USING ROBUSTSCALER)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: PRE-SPLIT DATA CLEANING & FEATURE ENGINEERING\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n[1/10] Repairing impossible negative values...\")\n",
    "# We STILL do this. This is correct.\n",
    "df['Delivery_Days'] = df['Delivery_Days'].abs()\n",
    "df['Transport_Cost'] = df['Transport_Cost'].abs()\n",
    "print(\"   âœ“ Repaired negative costs and durations using .abs()\")\n",
    "\n",
    "# ==============================================================================\n",
    "print(\"\\n[2/10] Engineering features...\")\n",
    "df['Equipment_Volume'] = df['Equipment_Height'] * df['Equipment_Width']\n",
    "print(\"   âœ“ Created 'Equipment_Volume'\")\n",
    "\n",
    "# ==============================================================================\n",
    "print(\"\\n[3/10] Log-transforming skewed features...\")\n",
    "# We no longer clip, just log-transform\n",
    "df['Equipment_Value'] = np.log1p(df['Equipment_Value'])\n",
    "df['Equipment_Volume'] = np.log1p(df['Equipment_Volume'])\n",
    "print(f\"   âœ“ Log-transformed 'Equipment_Value' and 'Equipment_Volume'\")\n",
    "\n",
    "# ==============================================================================\n",
    "print(\"\\n[4/10] Defining target variable (y)...\")\n",
    "# We no longer clip, just log-transform the repaired target\n",
    "y = np.log1p(df['Transport_Cost'])\n",
    "print(f\"   âœ“ Target (y) created using log1p(abs(Transport_Cost))\")\n",
    "\n",
    "# ==============================================================================\n",
    "print(\"\\n[5/10] Selecting features (X) for modeling...\")\n",
    "drop_cols = [\n",
    "    'Transport_Cost', 'Equipment_Height', 'Equipment_Width', 'Equipment_Weight',\n",
    "    'Hospital_Id', 'Supplier_Name', 'Hospital_Location',\n",
    "    'Order_Placed_Date', 'Delivery_Date'\n",
    "]\n",
    "X = df.drop(columns=drop_cols)\n",
    "print(f\"   âœ“ Selected {X.shape[1]} features.\")\n",
    "\n",
    "# ==============================================================================\n",
    "print(\"\\n[6/10] Train-test split (80/20)...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "print(f\"   âœ“ Training set: {X_train.shape}\")\n",
    "print(f\"   âœ“ Test set:     {X_test.shape}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: POST-SPLIT PIPELINES\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" BUILDING V4 ROBUST PIPELINES (USING ROBUSTSCALER)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n[8/10] Configuring feature transformers...\")\n",
    "\n",
    "# --- Numeric Features ---\n",
    "numeric_features = [\n",
    "    'Supplier_Reliability', 'Equipment_Value', 'Base_Transport_Fee',\n",
    "    'Delivery_Days', 'Equipment_Volume'\n",
    "]\n",
    "# === CRITICAL CHANGE: USE ROBUSTSCALER ===\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', RobustScaler()) # Use RobustScaler instead of StandardScaler\n",
    "])\n",
    "print(f\"   âœ“ Numeric features: median imputation + RobustScaler\")\n",
    "\n",
    "# --- Categorical Features ---\n",
    "categorical_features = [\n",
    "    'Equipment_Type', 'CrossBorder_Shipping', 'Urgent_Shipping',\n",
    "    'Installation_Service', 'Transport_Method', 'Fragile_Equipment',\n",
    "    'Hospital_Info', 'Rural_Hospital', 'Order_Month',\n",
    "    'Order_Day_of_Week', 'Order_Is_Weekend'\n",
    "]\n",
    "# Use the 'Missing' category strategy\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "print(f\"   âœ“ Categorical features: imputing NaNs as 'Missing' + one-hot\")\n",
    "\n",
    "# ==============================================================================\n",
    "print(\"\\n[9/10] Assembling ColumnTransformer...\")\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "print(f\"   âœ“ ColumnTransformer 'preprocessor' configured\")\n",
    "\n",
    "# ==============================================================================\n",
    "print(\"\\n[10/10] Applying preprocessing...\")\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "print(f\"   âœ“ V4 Preprocessing complete.\")\n",
    "print(f\"   âœ“ Training set processed: {X_train_processed.shape}\")\n",
    "print(f\"   âœ“ Test set processed:     {X_test_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068ca677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# === CELL 4 (V8): GRIDSEARCHCV FOR A SIMPLE RIDGE MODEL ===\n",
    "\n",
    "print(\"ðŸš€ Starting V8 GridSearchCV for Ridge (Focusing on EXTREME SIMPLICITY)...\")\n",
    "\n",
    "# --- 1. CV splitter ---\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# --- 2. Ridge pipeline ---\n",
    "# 'preprocessor' is your V4 preprocessor (with RobustScaler)\n",
    "# This MUST be the V4 preprocessor object, already in your memory\n",
    "ridge_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor), \n",
    "    ('ridge', Ridge(random_state=42)) # Simple, robust linear model\n",
    "])\n",
    "\n",
    "# --- 3. V8 Hyperparameter grid (Just tune regularization strength) ---\n",
    "param_grid = {\n",
    "    'ridge__alpha': [1, 10, 50, 100, 200, 500, 1000] # Test a wide range of L2 regularization\n",
    "}\n",
    "\n",
    "# --- 4. GridSearchCV ---\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=ridge_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=kf,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# --- 5. Run grid search on the V4 TRAINING DATA ---\n",
    "# (X_train and y_train are from your V4 preprocessing cell)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# --- 6. Best params & CV score ---\n",
    "print(\"\\nâœ… V8 (Ridge) GridSearch complete!\")\n",
    "best_params = grid_search.best_params_\n",
    "best_cv_rmse = -grid_search.best_score_\n",
    "print(f\"   Best hyperparameters: {best_params}\")\n",
    "print(f\"   Best CV RMSE (log-space): {best_cv_rmse:.4f}\")\n",
    "\n",
    "# --- 7. Evaluate on V4 test set ---\n",
    "final_ridge_v8 = grid_search.best_estimator_\n",
    "y_test_pred_log = final_ridge_v8.predict(X_test)\n",
    "y_test_pred_orig = np.expm1(y_test_pred_log)\n",
    "\n",
    "rmse_test_log = np.sqrt(mean_squared_error(y_test, y_test_pred_log))\n",
    "rmse_test_orig = np.sqrt(mean_squared_error(np.expm1(y_test), y_test_pred_orig))\n",
    "\n",
    "print(f\"\\n   Test RMSE (log-space)      : {rmse_test_log:.4f}\")\n",
    "print(f\"   Test RMSE (original scale) : {rmse_test_orig:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef10034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 4 (V6): GRIDSEARCHCV FOR EXTREME ROBUSTNESS ===\n",
    "\n",
    "print(\"ðŸš€ Starting V6 GridSearchCV for XGBoost (Focusing on EXTREME ROBUSTNESS)...\")\n",
    "\n",
    "# --- 1. CV splitter ---\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# --- 2. XGB pipeline ---\n",
    "# 'preprocessor' is your V4 preprocessor (with RobustScaler)\n",
    "# This MUST be the V4 preprocessor object, already in your memory\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor), \n",
    "    ('xgb', XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        learning_rate=0.05 # Slower learning rate for more stability\n",
    "    ))\n",
    "])\n",
    "\n",
    "# --- 3. V6 Hyperparameter grid (Shallow, Regularized) ---\n",
    "param_grid = {\n",
    "    'xgb__n_estimators': [1000, 1500],\n",
    "    'xgb__max_depth': [3],               # Force shallow trees\n",
    "    'xgb__reg_alpha': [10, 50, 100],     # Aggressive L1\n",
    "    'xgb__reg_lambda': [10, 50, 100]     # Aggressive L2\n",
    "}\n",
    "\n",
    "# --- 4. GridSearchCV ---\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=kf,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# --- 5. Run grid search on the V4 TRAINING DATA ---\n",
    "# (X_train and y_train are from your V4 preprocessing cell)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# --- 6. Best params & CV score ---\n",
    "print(\"\\nâœ… V6 GridSearch complete!\")\n",
    "best_params = grid_search.best_params_\n",
    "best_cv_rmse = -grid_search.best_score_\n",
    "print(f\"   Best hyperparameters: {best_params}\")\n",
    "print(f\"   Best CV RMSE (log-space): {best_cv_rmse:.4f}\")\n",
    "\n",
    "# --- 7. Evaluate on V4 test set ---\n",
    "final_xgb_bulletproof_v6 = grid_search.best_estimator_\n",
    "y_test_pred_log = final_xgb_bulletproof_v6.predict(X_test)\n",
    "y_test_pred_orig = np.expm1(y_test_pred_log)\n",
    "\n",
    "rmse_test_log = np.sqrt(mean_squared_error(y_test, y_test_pred_log))\n",
    "rmse_test_orig = np.sqrt(mean_squared_error(np.expm1(y_test), y_test_pred_orig))\n",
    "\n",
    "print(f\"\\n   Test RMSE (log-space)      : {rmse_test_log:.4f}\")\n",
    "print(f\"   Test RMSE (original scale) : {rmse_test_orig:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec00bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 5: DEFINE THE V4 'PREPARE_FEATURES' FUNCTION ===\n",
    "\n",
    "# This function is now simpler: no clipping!\n",
    "def prepare_features_V4(df_raw):\n",
    "    \"\"\"\n",
    "    Applies all V4 (robust) manual cleaning and feature engineering.\n",
    "    No clipping is needed as the RobustScaler pipeline handles outliers.\n",
    "    \"\"\"\n",
    "    df = df_raw.copy()\n",
    "    \n",
    "    # 1. Clean column names\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    # 2. Clean all string/object columns\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "        df[col] = df[col].replace({'': np.nan, 'nan': np.nan, 'NaN': np.nan})\n",
    "\n",
    "    # 3. Normalize Yes/No columns\n",
    "    yes_no_cols = ['CrossBorder_Shipping', 'Urgent_Shipping', 'Installation_Service',\n",
    "                   'Fragile_Equipment', 'Rural_Hospital']\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        if col in yes_no_cols:\n",
    "            df[col] = df[col].replace({\n",
    "                'YES': 'Yes', 'yes': 'Yes', 'Y': 'Yes', 'y': 'Yes',\n",
    "                'NO': 'No', 'no': 'No', 'N': 'No', 'n': 'No'\n",
    "            })\n",
    "\n",
    "    # 4. Convert date columns\n",
    "    df['Order_Placed_Date'] = pd.to_datetime(df['Order_Placed_Date'], errors='coerce')\n",
    "    df['Delivery_Date'] = pd.to_datetime(df['Delivery_Date'], errors='coerce')\n",
    "\n",
    "    # 5. Engineer Date Features\n",
    "    df['Delivery_Days'] = (df['Delivery_Date'] - df['Order_Placed_Date']).dt.days\n",
    "    df['Delivery_Days'] = df['Delivery_Days'].abs() # V4 fix\n",
    "    df['Order_Month'] = df['Order_Placed_Date'].dt.month\n",
    "    df['Order_Day_of_Week'] = df['Order_Placed_Date'].dt.dayofweek\n",
    "    df['Order_Is_Weekend'] = df['Order_Day_of_Week'].isin([5, 6])\n",
    "    \n",
    "    # 7. ==== V4: ENGINEERING & LOG-TRANSFORM ====\n",
    "    df['Equipment_Volume'] = df['Equipment_Height'] * df['Equipment_Width']\n",
    "    df['Equipment_Value'] = np.log1p(df['Equipment_Value'])\n",
    "    df['Equipment_Volume'] = np.log1p(df['Equipment_Volume'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"   âœ“ V4 `prepare_features_V4` function created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf0bcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 6: TRAIN YOUR FINAL, BEST V8 (RIDGE) MODEL ON ALL V4 DATA ===\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "print(\"\\nTraining final V8 (Ridge) model on all V4 data...\")\n",
    "print(\"--- THIS IS OUR 'ROBUST & SIMPLE' MODEL ---\")\n",
    "\n",
    "# === 1. Feature groups (from our V4 robust script) ===\n",
    "numeric_features = [\n",
    "    'Supplier_Reliability', 'Equipment_Value', 'Base_Transport_Fee',\n",
    "    'Delivery_Days', 'Equipment_Volume'\n",
    "]\n",
    "categorical_features = [\n",
    "    'Equipment_Type', 'CrossBorder_Shipping', 'Urgent_Shipping',\n",
    "    'Installation_Service', 'Transport_Method', 'Fragile_Equipment',\n",
    "    'Hospital_Info', 'Rural_Hospital', 'Order_Month',\n",
    "    'Order_Day_of_Week', 'Order_Is_Weekend'\n",
    "]\n",
    "\n",
    "# === 2. Define V4 transformers (with RobustScaler) ===\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', RobustScaler()) # CRITICAL: Use RobustScaler\n",
    "])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# === 3. Combine them ===\n",
    "final_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# === 4. Build final pipeline with BEST V8 hyperparameters ===\n",
    "# (This automatically uses the 'best_params' variable from Cell 4)\n",
    "print(f\"   âœ“ Using best params from V8 GridSearch: {best_params}\")\n",
    "\n",
    "final_ridge_v8_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', final_preprocessor),\n",
    "    ('ridge', Ridge(\n",
    "        alpha=best_params['ridge__alpha'], # Unpacks {'ridge__alpha': 10}\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# === 5. Fit on full V4 dataset ===\n",
    "# (X and y are from Cell 3)\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "final_ridge_v8_pipeline.fit(X, y)\n",
    "\n",
    "print(\"\\nâœ… Final (V8 RIDGE) model trained on entire dataset.\")\n",
    "print(\"The 'final_ridge_v8_pipeline' object is ready for prediction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0926a8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 6: TRAIN YOUR FINAL, BEST V4 MODEL ON ALL V4 DATA ===\n",
    "\n",
    "print(\"\\nTraining final V4 XGBoost model on all V4 data...\")\n",
    "print(\"--- THIS IS YOUR NEW BEST/MOST STABLE MODEL ---\")\n",
    "\n",
    "# === 1. Feature groups (from our V4 robust script) ===\n",
    "numeric_features = [\n",
    "    'Supplier_Reliability', 'Equipment_Value', 'Base_Transport_Fee',\n",
    "    'Delivery_Days', 'Equipment_Volume'\n",
    "]\n",
    "categorical_features = [\n",
    "    'Equipment_Type', 'CrossBorder_Shipping', 'Urgent_Shipping',\n",
    "    'Installation_Service', 'Transport_Method', 'Fragile_Equipment',\n",
    "    'Hospital_Info', 'Rural_Hospital', 'Order_Month',\n",
    "    'Order_Day_of_Week', 'Order_Is_Weekend'\n",
    "]\n",
    "\n",
    "# === 2. Define V4 transformers (with RobustScaler) ===\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', RobustScaler()) # CRITICAL: Use RobustScaler\n",
    "])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# === 3. Combine them ===\n",
    "final_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# === 4. Build final pipeline with BEST V4 hyperparameters ===\n",
    "# (This automatically uses the 'best_params' variable from Cell 4)\n",
    "print(f\"   âœ“ Using best params from V4 GridSearch: {best_params}\")\n",
    "\n",
    "# Map the 'xgb__' keys to the model's expected keys\n",
    "model_params = {\n",
    "    key.replace('xgb__', ''): value \n",
    "    for key, value in best_params.items()\n",
    "}\n",
    "\n",
    "final_xgb_robust_pipeline_V4 = Pipeline(steps=[\n",
    "    ('preprocessor', final_preprocessor),\n",
    "    ('xgb', XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        **model_params # Unpacks {'learning_rate': 0.1, 'max_depth': 4, ...}\n",
    "    ))\n",
    "])\n",
    "\n",
    "# === 5. Fit on full V4 dataset ===\n",
    "# (X and y are from Cell 3)\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "final_xgb_robust_pipeline_V4.fit(X, y)\n",
    "\n",
    "print(\"\\nâœ… Final (V4 ROBUST) XGBoost model trained on entire dataset.\")\n",
    "print(\"The 'final_xgb_robust_pipeline_V4' object is ready for prediction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0357e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 7: GENERATE YOUR FINAL V4 SUBMISSION ===\n",
    "\n",
    "# 1. Load your new, raw test data\n",
    "print(\"Loading new test data (test.csv)...\")\n",
    "df_new_test = pd.read_csv('../data/test.csv') \n",
    "\n",
    "# 2. Save IDs\n",
    "submission_ids = df_new_test['Hospital_Id']\n",
    "\n",
    "# 3. Apply the *V4* feature engineering\n",
    "print(\"Applying V4 (RobustScaler) feature engineering...\")\n",
    "# This uses the 'prepare_features_V4' function you defined in Cell 5\n",
    "X_new_prepared = prepare_features_V4(df_new_test) \n",
    "\n",
    "# 4. Get predictions FROM THE ROBUST V4 MODEL\n",
    "print(\"Getting predictions from the final V4 XGBoost model...\")\n",
    "# This uses the 'final_xgb_robust_pipeline_V4' model from Cell 6\n",
    "log_predictions = final_xgb_robust_pipeline_V4.predict(X_new_prepared)\n",
    "\n",
    "# 5. Convert predictions back from log-scale\n",
    "final_predictions = np.expm1(log_predictions)\n",
    "\n",
    "# 6. Create the final submission file\n",
    "submission_df = pd.DataFrame({\n",
    "    'Hospital_Id': submission_ids,\n",
    "    'Transport_Cost': final_predictions\n",
    "})\n",
    "\n",
    "# Display the first few predictions\n",
    "print(\"\\nFinal Predictions:\")\n",
    "display(submission_df.head())\n",
    "\n",
    "# Save to CSV\n",
    "submission_df.to_csv('submission_XGB_V4_RobustScaler.csv', index=False)\n",
    "print(\"\\nâœ… Submission file 'submission_XGB_V4_RobustScaler.csv' created successfully.\")\n",
    "print(\"THIS IS THE ONE. UPLOAD THIS FILE TO KAGGLE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a36c626",
   "metadata": {},
   "source": [
    "This is Attempt 3 for Dumb Models check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12be3d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 1: V4 ROBUST PREPROCESSING (Run this again) ===\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\" V4 - ROBUST PREPROCESSING (USING ROBUSTSCALER)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: PRE-SPLIT DATA CLEANING & FEATURE ENGINEERING\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n[1/10] Loading and cleaning data...\")\n",
    "try:\n",
    "    df = pd.read_csv('../data/train.csv')\n",
    "    df.columns = df.columns.str.strip()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading '../data/train.csv'. Make sure the file is in the correct path.\")\n",
    "    print(e)\n",
    "\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    df[col] = df[col].astype(str).str.strip()\n",
    "    df[col] = df[col].replace({'': np.nan, 'nan': np.nan, 'NaN': np.nan})\n",
    "    \n",
    "yes_no_cols = ['CrossBorder_Shipping', 'Urgent_Shipping', 'Installation_Service',\n",
    "               'Fragile_Equipment', 'Rural_Hospital']\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    if col in yes_no_cols:\n",
    "        df[col] = df[col].replace({\n",
    "            'YES': 'Yes', 'yes': 'Yes', 'Y': 'Yes', 'y': 'Yes',\n",
    "            'NO': 'No', 'no': 'No', 'N': 'No', 'n': 'No'\n",
    "        })\n",
    "\n",
    "df['Order_Placed_Date'] = pd.to_datetime(df['Order_Placed_Date'], errors='coerce')\n",
    "df['Delivery_Date'] = pd.to_datetime(df['Delivery_Date'], errors='coerce')\n",
    "\n",
    "df['Delivery_Days'] = (df['Delivery_Date'] - df['Order_Placed_Date']).dt.days\n",
    "df['Order_Month'] = df['Order_Placed_Date'].dt.month\n",
    "df['Order_Day_of_Week'] = df['Order_Placed_Date'].dt.dayofweek\n",
    "df['Order_Is_Weekend'] = df['Order_Day_of_Week'].isin([5, 6])\n",
    "print(\"   âœ“ Initial load and cleaning complete.\")\n",
    "\n",
    "print(\"\\n[2/10] Repairing negative values...\")\n",
    "df['Delivery_Days'] = df['Delivery_Days'].abs()\n",
    "df['Transport_Cost'] = df['Transport_Cost'].abs()\n",
    "print(\"   âœ“ Repaired negative costs and durations.\")\n",
    "\n",
    "print(\"\\n[3/10] Engineering & Log-transforming features...\")\n",
    "df['Equipment_Volume'] = df['Equipment_Height'] * df['Equipment_Width']\n",
    "df['Equipment_Value'] = np.log1p(df['Equipment_Value'])\n",
    "df['Equipment_Volume'] = np.log1p(df['Equipment_Volume'])\n",
    "print(\"   âœ“ Engineered and log-transformed features.\")\n",
    "\n",
    "print(\"\\n[4/10] Defining target variable (y)...\")\n",
    "y = np.log1p(df['Transport_Cost'])\n",
    "print(f\"   âœ“ Target (y) created.\")\n",
    "\n",
    "print(\"\\n[5/10] Selecting features (X)...\")\n",
    "drop_cols = [\n",
    "    'Transport_Cost', 'Equipment_Height', 'Equipment_Width', 'Equipment_Weight',\n",
    "    'Hospital_Id', 'Supplier_Name', 'Hospital_Location',\n",
    "    'Order_Placed_Date', 'Delivery_Date'\n",
    "]\n",
    "X = df.drop(columns=drop_cols)\n",
    "\n",
    "print(\"\\n[6/10] Train-test split (80/20)...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: POST-SPLIT PIPELINES\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" BUILDING V4 ROBUST PIPELINES (USING ROBUSTSCALER)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# --- Numeric Features ---\n",
    "numeric_features = [\n",
    "    'Supplier_Reliability', 'Equipment_Value', 'Base_Transport_Fee',\n",
    "    'Delivery_Days', 'Equipment_Volume'\n",
    "]\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', RobustScaler()) \n",
    "])\n",
    "\n",
    "# --- Categorical Features ---\n",
    "categorical_features = [\n",
    "    'Equipment_Type', 'CrossBorder_Shipping', 'Urgent_Shipping',\n",
    "    'Installation_Service', 'Transport_Method', 'Fragile_Equipment',\n",
    "    'Hospital_Info', 'Rural_Hospital', 'Order_Month',\n",
    "    'Order_Day_of_Week', 'Order_Is_Weekend'\n",
    "]\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# --- V4 Preprocessor (This is the one we will modify) ---\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# --- V4 Prepare_features function (This is the one we will use) ---\n",
    "def prepare_features_V4(df_raw):\n",
    "    df = df_raw.copy()\n",
    "    df.columns = df.columns.str.strip()\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "        df[col] = df[col].replace({'': np.nan, 'nan': np.nan, 'NaN': np.nan})\n",
    "    yes_no_cols = ['CrossBorder_Shipping', 'Urgent_Shipping', 'Installation_Service', 'Fragile_Equipment', 'Rural_Hospital']\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        if col in yes_no_cols:\n",
    "            df[col] = df[col].replace({'YES': 'Yes', 'yes': 'Yes', 'Y': 'Yes', 'y': 'Yes', 'NO': 'No', 'no': 'No', 'N': 'No', 'n': 'No'})\n",
    "    df['Order_Placed_Date'] = pd.to_datetime(df['Order_Placed_Date'], errors='coerce')\n",
    "    df['Delivery_Date'] = pd.to_datetime(df['Delivery_Date'], errors='coerce')\n",
    "    df['Delivery_Days'] = (df['Delivery_Date'] - df['Order_Placed_Date']).dt.days\n",
    "    df['Delivery_Days'] = df['Delivery_Days'].abs()\n",
    "    df['Order_Month'] = df['Order_Placed_Date'].dt.month\n",
    "    df['Order_Day_of_Week'] = df['Order_Placed_Date'].dt.dayofweek\n",
    "    df['Order_Is_Weekend'] = df['Order_Day_of_Week'].isin([5, 6])\n",
    "    df['Equipment_Volume'] = df['Equipment_Height'] * df['Equipment_Width']\n",
    "    df['Equipment_Value'] = np.log1p(df['Equipment_Value'])\n",
    "    df['Equipment_Volume'] = np.log1p(df['Equipment_Volume'])\n",
    "    return df\n",
    "\n",
    "print(\"   âœ“ V4 Preprocessor and V4 Prepare_Features function are in memory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d86fd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 2: PROBE 1 (SUPPLIER_RELIABILITY) ===\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "print(\"ðŸš€ Starting Probe 1: Supplier_Reliability\")\n",
    "\n",
    "# 1. Create a preprocessor for ONLY this feature\n",
    "probe_1_features = ['Supplier_Reliability']\n",
    "probe_1_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', RobustScaler()) \n",
    "])\n",
    "probe_1_preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', probe_1_transformer, probe_1_features)],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# 2. Build the final pipeline\n",
    "probe_1_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', probe_1_preprocessor),\n",
    "    ('ridge', Ridge(alpha=10)) # Use alpha=10 from our V8 test\n",
    "])\n",
    "\n",
    "# 3. Fit on ALL V4 data\n",
    "print(\"Fitting Probe 1 model on all data...\")\n",
    "probe_1_pipeline.fit(X, y)\n",
    "\n",
    "# 4. Load test data and apply V4 preparation\n",
    "print(\"Loading and preparing test data...\")\n",
    "df_new_test = pd.read_csv('../data/test.csv') \n",
    "submission_ids = df_new_test['Hospital_Id']\n",
    "X_new_prepared = prepare_features_V4(df_new_test)\n",
    "\n",
    "# 5. Get predictions\n",
    "print(\"Getting predictions...\")\n",
    "log_predictions = probe_1_pipeline.predict(X_new_prepared)\n",
    "final_predictions = np.expm1(log_predictions)\n",
    "\n",
    "# 6. Create submission file\n",
    "submission_df = pd.DataFrame({'Hospital_Id': submission_ids, 'Transport_Cost': final_predictions})\n",
    "submission_df.to_csv('submission_PROBE_1_Reliability.csv', index=False)\n",
    "print(\"âœ… 'submission_PROBE_1_Reliability.csv' created. PLEASE SUBMIT THIS.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61d73d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 3: PROBE 2 (BASE_TRANSPORT_FEE) ===\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "print(\"ðŸš€ Starting Probe 2: Base_Transport_Fee\")\n",
    "\n",
    "# 1. Create a preprocessor for ONLY this feature\n",
    "probe_2_features = ['Base_Transport_Fee']\n",
    "probe_2_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', RobustScaler()) \n",
    "])\n",
    "probe_2_preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', probe_2_transformer, probe_2_features)],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# 2. Build the final pipeline\n",
    "probe_2_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', probe_2_preprocessor),\n",
    "    ('ridge', Ridge(alpha=10))\n",
    "])\n",
    "\n",
    "# 3. Fit on ALL V4 data\n",
    "print(\"Fitting Probe 2 model on all data...\")\n",
    "probe_2_pipeline.fit(X, y)\n",
    "\n",
    "# 4. Load test data (already prepared)\n",
    "print(\"Using prepared test data...\")\n",
    "# (X_new_prepared and submission_ids are already in memory from Cell 2)\n",
    "\n",
    "# 5. Get predictions\n",
    "print(\"Getting predictions...\")\n",
    "log_predictions = probe_2_pipeline.predict(X_new_prepared)\n",
    "final_predictions = np.expm1(log_predictions)\n",
    "\n",
    "# 6. Create submission file\n",
    "submission_df = pd.DataFrame({'Hospital_Id': submission_ids, 'Transport_Cost': final_predictions})\n",
    "submission_df.to_csv('submission_PROBE_2_BaseFee.csv', index=False)\n",
    "print(\"âœ… 'submission_PROBE_2_BaseFee.csv' created. PLEASE SUBMIT THIS.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361c6f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 4: PROBE 3 (DELIVERY_DAYS) ===\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "print(\"ðŸš€ Starting Probe 3: Delivery_Days\")\n",
    "\n",
    "# 1. Create a preprocessor for ONLY this feature\n",
    "probe_3_features = ['Delivery_Days']\n",
    "probe_3_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', RobustScaler()) \n",
    "])\n",
    "probe_3_preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', probe_3_transformer, probe_3_features)],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# 2. Build the final pipeline\n",
    "probe_3_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', probe_3_preprocessor),\n",
    "    ('ridge', Ridge(alpha=10))\n",
    "])\n",
    "\n",
    "# 3. Fit on ALL V4 data\n",
    "print(\"Fitting Probe 3 model on all data...\")\n",
    "probe_3_pipeline.fit(X, y)\n",
    "\n",
    "# 4. Load test data (already prepared)\n",
    "print(\"Using prepared test data...\")\n",
    "# (X_new_prepared and submission_ids are already in memory from Cell 2)\n",
    "\n",
    "# 5. Get predictions\n",
    "print(\"Getting predictions...\")\n",
    "log_predictions = probe_3_pipeline.predict(X_new_prepared)\n",
    "final_predictions = np.expm1(log_predictions)\n",
    "\n",
    "# 6. Create submission file\n",
    "submission_df = pd.DataFrame({'Hospital_Id': submission_ids, 'Transport_Cost': final_predictions})\n",
    "submission_df.to_csv('submission_PROBE_3_DeliveryDays.csv', index=False)\n",
    "print(\"âœ… 'submission_PROBE_3_DeliveryDays.csv' created. PLEASE SUBMIT THIS.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24228b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "4th Attempt of removing above features and retrying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270b15eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 1: V10 \"SAFE FEATURES ONLY\" PREPROCESSING ===\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\" V10 - 'SAFE FEATURES ONLY' PREPROCESSING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: PRE-SPLIT DATA CLEANING & FEATURE ENGINEERING\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n[1/10] Loading and cleaning data...\")\n",
    "try:\n",
    "    df = pd.read_csv('../data/train.csv')\n",
    "    df.columns = df.columns.str.strip()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading '../data/train.csv'. Make sure the file is in the correct path.\")\n",
    "    print(e)\n",
    "\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    df[col] = df[col].astype(str).str.strip()\n",
    "    df[col] = df[col].replace({'': np.nan, 'nan': np.nan, 'NaN': np.nan})\n",
    "    \n",
    "yes_no_cols = ['CrossBorder_Shipping', 'Urgent_Shipping', 'Installation_Service',\n",
    "               'Fragile_Equipment', 'Rural_Hospital']\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    if col in yes_no_cols:\n",
    "        df[col] = df[col].replace({\n",
    "            'YES': 'Yes', 'yes': 'Yes', 'Y': 'Yes', 'y': 'Yes',\n",
    "            'NO': 'No', 'no': 'No', 'N': 'No', 'n': 'No'\n",
    "        })\n",
    "\n",
    "df['Order_Placed_Date'] = pd.to_datetime(df['Order_Placed_Date'], errors='coerce')\n",
    "df['Delivery_Date'] = pd.to_datetime(df['Delivery_Date'], errors='coerce')\n",
    "\n",
    "df['Delivery_Days'] = (df['Delivery_Date'] - df['Order_Placed_Date']).dt.days\n",
    "df['Order_Month'] = df['Order_Placed_Date'].dt.month\n",
    "df['Order_Day_of_Week'] = df['Order_Placed_Date'].dt.dayofweek\n",
    "df['Order_Is_Weekend'] = df['Order_Day_of_Week'].isin([5, 6])\n",
    "print(\"   âœ“ Initial load and cleaning complete.\")\n",
    "\n",
    "print(\"\\n[2/10] Repairing negative costs/durations...\")\n",
    "# We still repair Delivery_Days in case it's used for something (it's not, but good practice)\n",
    "df['Delivery_Days'] = df['Delivery_Days'].abs() \n",
    "df['Transport_Cost'] = df['Transport_Cost'].abs()\n",
    "print(\"   âœ“ Repaired negative costs and durations.\")\n",
    "\n",
    "print(\"\\n[3/10] Engineering & Log-transforming 'safe' features...\")\n",
    "df['Equipment_Volume'] = df['Equipment_Height'] * df['Equipment_Width']\n",
    "df['Equipment_Value'] = np.log1p(df['Equipment_Value'])\n",
    "df['Equipment_Volume'] = np.log1p(df['Equipment_Volume'])\n",
    "print(\"   âœ“ Engineered and log-transformed 'safe' features (Value and Volume).\")\n",
    "\n",
    "print(\"\\n[4/10] Defining target variable (y)...\")\n",
    "y = np.log1p(df['Transport_Cost'])\n",
    "print(f\"   âœ“ Target (y) created.\")\n",
    "\n",
    "print(\"\\n[5/10] Selecting features (X)...\")\n",
    "# We drop the \"bomb\" features from our training data!\n",
    "drop_cols = [\n",
    "    'Transport_Cost', 'Equipment_Height', 'Equipment_Width', 'Equipment_Weight',\n",
    "    'Hospital_Id', 'Supplier_Name', 'Hospital_Location',\n",
    "    'Order_Placed_Date', 'Delivery_Date',\n",
    "    \n",
    "    # === DROPPING THE BOMBS ===\n",
    "    'Supplier_Reliability',\n",
    "    'Base_Transport_Fee',\n",
    "    'Delivery_Days'\n",
    "]\n",
    "X = df.drop(columns=drop_cols)\n",
    "print(f\"   âœ“ 'X' created. 'Bomb' features (Reliability, Base_Fee, Days) have been DROPPED.\")\n",
    "\n",
    "print(\"\\n[6/10] Train-test split (80/20)...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: POST-SPLIT PIPELINES\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" BUILDING V10 'SAFE FEATURES ONLY' PIPELINES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n[8/10] Configuring feature transformers...\")\n",
    "\n",
    "# --- Numeric Features (SAFE FEATURES ONLY) ---\n",
    "numeric_features = [\n",
    "    'Equipment_Value',  # Log-transformed, safe\n",
    "    'Equipment_Volume'  # Log-transformed, safe\n",
    "]\n",
    "# We can use StandardScaler now, it's fine for log-transformed data\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()) \n",
    "])\n",
    "print(f\"   âœ“ Numeric features (SAFE ONLY): median imputation + StandardScaler\")\n",
    "\n",
    "# --- Categorical Features (ALL) ---\n",
    "categorical_features = [\n",
    "    'Equipment_Type', 'CrossBorder_Shipping', 'Urgent_Shipping',\n",
    "    'Installation_Service', 'Transport_Method', 'Fragile_Equipment',\n",
    "    'Hospital_Info', 'Rural_Hospital', 'Order_Month',\n",
    "    'Order_Day_of_Week', 'Order_Is_Weekend'\n",
    "]\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "print(f\"   âœ“ Categorical features: imputing NaNs as 'Missing' + one-hot\")\n",
    "\n",
    "# ==============================================================================\n",
    "print(\"\\n[9/10] Assembling ColumnTransformer...\")\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "print(f\"   âœ“ ColumnTransformer 'preprocessor' configured\")\n",
    "\n",
    "# ==============================================================================\n",
    "print(\"\\n[10/10] Applying preprocessing...\")\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "print(f\"   âœ“ V10 Preprocessing complete.\")\n",
    "print(f\"   âœ“ Training set processed: {X_train_processed.shape}\")\n",
    "print(f\"   âœ“ Test set processed:     {X_test_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5e5980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 2: V10 GRIDSEARCHCV (FOR 'SAFE FEATURES ONLY' DATA) ===\n",
    "\n",
    "print(\"ðŸš€ Starting V10 GridSearchCV for XGBoost (on 'Safe' data)...\")\n",
    "\n",
    "# --- 1. CV splitter ---\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# --- 2. XGB pipeline ---\n",
    "# 'preprocessor' is the new V10 one from Cell 1\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('xgb', XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# --- 3. Hyperparameter grid (we can be less aggressive) ---\n",
    "param_grid = {\n",
    "    'xgb__n_estimators': [300, 500],\n",
    "    'xgb__max_depth': [3, 5, 7],\n",
    "    'xgb__learning_rate': [0.1],\n",
    "    'xgb__reg_alpha': [0.1, 1],  # Less regularization needed now\n",
    "    'xgb__reg_lambda': [1]\n",
    "}\n",
    "\n",
    "# --- 4. GridSearchCV ---\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=kf,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# --- 5. Run grid search on the V10 TRAINING DATA ---\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# --- 6. Best params & CV score ---\n",
    "print(\"\\nâœ… V10 GridSearch complete!\")\n",
    "best_params = grid_search.best_params_\n",
    "best_cv_rmse = -grid_search.best_score_\n",
    "print(f\"   Best hyperparameters: {best_params}\")\n",
    "print(f\"   Best CV RMSE (log-space): {best_cv_rmse:.4f}\")\n",
    "\n",
    "# --- 7. Evaluate on V10 test set ---\n",
    "final_xgb_safe_v10 = grid_search.best_estimator_\n",
    "y_test_pred_log = final_xgb_safe_v10.predict(X_test)\n",
    "y_test_pred_orig = np.expm1(y_test_pred_log)\n",
    "\n",
    "rmse_test_log = np.sqrt(mean_squared_error(y_test, y_test_pred_log))\n",
    "rmse_test_orig = np.sqrt(mean_squared_error(np.expm1(y_test), y_test_pred_orig))\n",
    "\n",
    "print(f\"\\n   Test RMSE (log-space)      : {rmse_test_log:.4f}\")\n",
    "print(f\"   Test RMSE (original scale) : {rmse_test_orig:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c45fd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 3: DEFINE THE V10 'PREPARE_FEATURES' FUNCTION ===\n",
    "\n",
    "# This function ONLY engineers the 'safe' features\n",
    "def prepare_features_V10(df_raw):\n",
    "    \"\"\"\n",
    "    Applies all V10 ('Safe Features Only') manual cleaning and feature engineering.\n",
    "    It does NOT create the 'bomb' features.\n",
    "    \"\"\"\n",
    "    df = df_raw.copy()\n",
    "    \n",
    "    # 1. Clean column names\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    # 2. Clean all string/object columns\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "        df[col] = df[col].replace({'': np.nan, 'nan': np.nan, 'NaN': np.nan})\n",
    "\n",
    "    # 3. Normalize Yes/No columns\n",
    "    yes_no_cols = ['CrossBorder_Shipping', 'Urgent_Shipping', 'Installation_Service',\n",
    "                   'Fragile_Equipment', 'Rural_Hospital']\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        if col in yes_no_cols:\n",
    "            df[col] = df[col].replace({\n",
    "                'YES': 'Yes', 'yes': 'Yes', 'Y': 'Yes', 'y': 'Yes',\n",
    "                'NO': 'No', 'no': 'No', 'N': 'No', 'n': 'No'\n",
    "            })\n",
    "\n",
    "    # 4. Convert date columns\n",
    "    df['Order_Placed_Date'] = pd.to_datetime(df['Order_Placed_Date'], errors='coerce')\n",
    "    df['Delivery_Date'] = pd.to_datetime(df['Delivery_Date'], errors='coerce')\n",
    "\n",
    "    # 5. Engineer ONLY categorical date features\n",
    "    df['Order_Month'] = df['Order_Placed_Date'].dt.month\n",
    "    df['Order_Day_of_Week'] = df['Order_Placed_Date'].dt.dayofweek\n",
    "    df['Order_Is_Weekend'] = df['Order_Day_of_Week'].isin([5, 6])\n",
    "    \n",
    "    # 6. ==== V10: ENGINEERING & LOG-TRANSFORM 'SAFE' FEATURES ====\n",
    "    df['Equipment_Volume'] = df['Equipment_Height'] * df['Equipment_Width']\n",
    "    df['Equipment_Value'] = np.log1p(df['Equipment_Value'])\n",
    "    df['Equipment_Volume'] = np.log1p(df['Equipment_Volume'])\n",
    "    \n",
    "    # We explicitly DO NOT create 'Delivery_Days', 'Supplier_Reliability', or 'Base_Transport_Fee'\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"   âœ“ V10 `prepare_features_V10` ('Safe Features Only') function created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d186239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 4: TRAIN YOUR FINAL, BEST V10 MODEL ON ALL V10 DATA ===\n",
    "\n",
    "print(\"\\nTraining final V10 XGBoost model on all V10 ('Safe') data...\")\n",
    "\n",
    "# === 1. Feature groups (V10 SAFE FEATURES) ===\n",
    "numeric_features = [\n",
    "    'Equipment_Value',\n",
    "    'Equipment_Volume'\n",
    "]\n",
    "categorical_features = [\n",
    "    'Equipment_Type', 'CrossBorder_Shipping', 'Urgent_Shipping',\n",
    "    'Installation_Service', 'Transport_Method', 'Fragile_Equipment',\n",
    "    'Hospital_Info', 'Rural_Hospital', 'Order_Month',\n",
    "    'Order_Day_of_Week', 'Order_Is_Weekend'\n",
    "]\n",
    "\n",
    "# === 2. Define V10 transformers ===\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# === 3. Combine them ===\n",
    "final_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# === 4. Build final pipeline with BEST V10 hyperparameters ===\n",
    "# (This automatically uses the 'best_params' variable from Cell 2)\n",
    "print(f\"   âœ“ Using best params from V10 GridSearch: {best_params}\")\n",
    "\n",
    "model_params = {\n",
    "    key.replace('xgb__', ''): value \n",
    "    for key, value in best_params.items()\n",
    "}\n",
    "\n",
    "final_xgb_safe_pipeline_V10 = Pipeline(steps=[\n",
    "    ('preprocessor', final_preprocessor),\n",
    "    ('xgb', XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        **model_params # Unpacks the best params here\n",
    "    ))\n",
    "])\n",
    "\n",
    "# === 5. Fit on full V10 dataset ===\n",
    "# (X and y are from Cell 1)\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "final_xgb_safe_pipeline_V10.fit(X, y)\n",
    "\n",
    "print(\"\\nâœ… Final (V10 'SAFE') XGBoost model trained on entire dataset.\")\n",
    "print(\"The 'final_xgb_safe_pipeline_V10' object is ready for prediction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722ac2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 5: GENERATE YOUR FINAL V10 SUBMISSION ===\n",
    "\n",
    "# 1. Load your new, raw test data\n",
    "print(\"Loading new test data (test.csv)...\")\n",
    "df_new_test = pd.read_csv('../data/test.csv') \n",
    "\n",
    "# 2. Save IDs\n",
    "submission_ids = df_new_test['Hospital_Id']\n",
    "\n",
    "# 3. Apply the *V10* feature engineering\n",
    "print(\"Applying V10 ('Safe Features Only') feature engineering...\")\n",
    "X_new_prepared = prepare_features_V10(df_new_test) # Using the new V10 function\n",
    "\n",
    "# 4. Get predictions FROM THE ROBUST V10 MODEL\n",
    "print(\"Getting predictions from the final V10 XGBoost model...\")\n",
    "log_predictions = final_xgb_safe_pipeline_V10.predict(X_new_prepared)\n",
    "\n",
    "# 5. Convert predictions back from log-scale\n",
    "final_predictions = np.expm1(log_predictions)\n",
    "\n",
    "# 6. Create the final submission file\n",
    "submission_df = pd.DataFrame({\n",
    "    'Hospital_Id': submission_ids,\n",
    "    'Transport_Cost': final_predictions\n",
    "})\n",
    "\n",
    "# Display the first few predictions\n",
    "print(\"\\nFinal Predictions:\")\n",
    "display(submission_df.head())\n",
    "\n",
    "# Save to CSV\n",
    "submission_df.to_csv('submission_XGB_V10_SafeFeaturesOnly.csv', index=False)\n",
    "print(\"\\nâœ… Submission file 'submission_XGB_V10_SafeFeaturesOnly.csv' created successfully.\")\n",
    "print(\"UPLOAD THIS FILE. This is the ultimate test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b38256",
   "metadata": {},
   "source": [
    "5th Try with LGBM and robustScaler\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6d625462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 1: IMPORTS ===\n",
    "\n",
    "# core\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# data + plotting\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# sklearn (preprocessing / pipeline / model selection / metrics)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "# NEW: Import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# models\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0ea56d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Initial data shape: (5000, 20)\n",
      "Cleaning string columns...\n",
      "Normalizing Yes/No columns...\n",
      "Converting date columns...\n",
      "Engineering Delivery_Days and date features...\n",
      "\n",
      "âœ… Initial load and feature engineering complete.\n"
     ]
    }
   ],
   "source": [
    "# === CELL 2: INITIAL DATA LOAD & CLEANING ===\n",
    "\n",
    "print(\"Loading data...\")\n",
    "try:\n",
    "    df = pd.read_csv('../data/train.csv')\n",
    "    df.columns = df.columns.str.strip()\n",
    "    print(f\"Initial data shape: {df.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading '../data/train.csv'. Make sure the file is in the correct path.\")\n",
    "    print(e)\n",
    "\n",
    "# Clean all string/object columns\n",
    "print(\"Cleaning string columns...\")\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    df[col] = df[col].astype(str).str.strip()\n",
    "    df[col] = df[col].replace({'': np.nan, 'nan': np.nan, 'NaN': np.nan})\n",
    "\n",
    "# Normalize Yes/No columns\n",
    "print(\"Normalizing Yes/No columns...\")\n",
    "yes_no_cols = ['CrossBorder_Shipping', 'Urgent_Shipping', 'Installation_Service',\n",
    "               'Fragile_Equipment', 'Rural_Hospital']\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    if col in yes_no_cols:\n",
    "        df[col] = df[col].replace({\n",
    "            'YES': 'Yes', 'yes': 'Yes', 'Y': 'Yes', 'y': 'Yes',\n",
    "            'NO': 'No', 'no': 'No', 'N': 'No', 'n': 'No'\n",
    "        })\n",
    "\n",
    "# Convert date columns\n",
    "print(\"Converting date columns...\")\n",
    "df['Order_Placed_Date'] = pd.to_datetime(df['Order_Placed_Date'], errors='coerce')\n",
    "df['Delivery_Date'] = pd.to_datetime(df['Delivery_Date'], errors='coerce')\n",
    "\n",
    "# Create new features\n",
    "print(\"Engineering Delivery_Days and date features...\")\n",
    "df['Delivery_Days'] = (df['Delivery_Date'] - df['Order_Placed_Date']).dt.days\n",
    "df['Order_Month'] = df['Order_Placed_Date'].dt.month\n",
    "df['Order_Day_of_Week'] = df['Order_Placed_Date'].dt.dayofweek\n",
    "df['Order_Is_Weekend'] = df['Order_Day_of_Week'].isin([5, 6])\n",
    "\n",
    "print(\"\\nâœ… Initial load and feature engineering complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4977add3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      " V12 - ROBUST PREPROCESSING (USING ROBUSTSCALER + OHE)\n",
      "======================================================================\n",
      "\n",
      "[1/10] Loading and cleaning data...\n",
      "   âœ“ Initial load and cleaning complete.\n",
      "\n",
      "[2/10] Repairing negative values...\n",
      "   âœ“ Repaired negative costs and durations.\n",
      "\n",
      "[3/10] Engineering & Log-transforming features...\n",
      "   âœ“ Engineered and log-transformed features.\n",
      "\n",
      "[4/10] Defining target variable (y)...\n",
      "   âœ“ Target (y) created.\n",
      "\n",
      "[5/10] Selecting features (X)...\n",
      "\n",
      "[6/10] Train-test split (80/20)...\n",
      "\n",
      "======================================================================\n",
      " BUILDING V12 ROBUST PIPELINES (USING ROBUSTSCALER + OHE)\n",
      "======================================================================\n",
      "\n",
      "[8/10] Configuring feature transformers...\n",
      "   âœ“ Numeric features: median imputation + RobustScaler\n",
      "   âœ“ Categorical features: 'Missing' imputation + OneHotEncoder\n",
      "\n",
      "[9/10] Assembling ColumnTransformer...\n",
      "   âœ“ ColumnTransformer 'preprocessor' configured\n",
      "\n",
      "[10/10] Applying preprocessing...\n",
      "   âœ“ V12 Preprocessing complete.\n",
      "   âœ“ Training set processed: (4000, 51)\n",
      "   âœ“ Test set processed:     (1000, 51)\n"
     ]
    }
   ],
   "source": [
    "# === CELL 1: V12 PREPROCESSING (Same as V4/V6) ===\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\" V12 - ROBUST PREPROCESSING (USING ROBUSTSCALER + OHE)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: PRE-SPLIT DATA CLEANING & FEATURE ENGINEERING\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n[1/10] Loading and cleaning data...\")\n",
    "try:\n",
    "    df = pd.read_csv('../data/train.csv')\n",
    "    df.columns = df.columns.str.strip()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading '../data/train.csv'. Make sure the file is in the correct path.\")\n",
    "    print(e)\n",
    "\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    df[col] = df[col].astype(str).str.strip()\n",
    "    df[col] = df[col].replace({'': np.nan, 'nan': np.nan, 'NaN': np.nan})\n",
    "    \n",
    "yes_no_cols = ['CrossBorder_Shipping', 'Urgent_Shipping', 'Installation_Service',\n",
    "               'Fragile_Equipment', 'Rural_Hospital']\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    if col in yes_no_cols:\n",
    "        df[col] = df[col].replace({\n",
    "            'YES': 'Yes', 'yes': 'Yes', 'Y': 'Yes', 'y': 'Yes',\n",
    "            'NO': 'No', 'no': 'No', 'N': 'No', 'n': 'No'\n",
    "        })\n",
    "\n",
    "df['Order_Placed_Date'] = pd.to_datetime(df['Order_Placed_Date'], errors='coerce')\n",
    "df['Delivery_Date'] = pd.to_datetime(df['Delivery_Date'], errors='coerce')\n",
    "\n",
    "df['Delivery_Days'] = (df['Delivery_Date'] - df['Order_Placed_Date']).dt.days\n",
    "df['Order_Month'] = df['Order_Placed_Date'].dt.month\n",
    "df['Order_Day_of_Week'] = df['Order_Placed_Date'].dt.dayofweek\n",
    "df['Order_Is_Weekend'] = df['Order_Day_of_Week'].isin([5, 6])\n",
    "print(\"   âœ“ Initial load and cleaning complete.\")\n",
    "\n",
    "print(\"\\n[2/10] Repairing negative values...\")\n",
    "df['Delivery_Days'] = df['Delivery_Days'].abs()\n",
    "df['Transport_Cost'] = df['Transport_Cost'].abs()\n",
    "print(\"   âœ“ Repaired negative costs and durations.\")\n",
    "\n",
    "print(\"\\n[3/10] Engineering & Log-transforming features...\")\n",
    "df['Equipment_Volume'] = df['Equipment_Height'] * df['Equipment_Width']\n",
    "df['Equipment_Value'] = np.log1p(df['Equipment_Value'])\n",
    "df['Equipment_Volume'] = np.log1p(df['Equipment_Volume'])\n",
    "print(\"   âœ“ Engineered and log-transformed features.\")\n",
    "\n",
    "print(\"\\n[4/10] Defining target variable (y)...\")\n",
    "y = np.log1p(df['Transport_Cost'])\n",
    "print(f\"   âœ“ Target (y) created.\")\n",
    "\n",
    "print(\"\\n[5/10] Selecting features (X)...\")\n",
    "drop_cols = [\n",
    "    'Transport_Cost', 'Equipment_Height', 'Equipment_Width', 'Equipment_Weight',\n",
    "    'Hospital_Id', 'Supplier_Name', 'Hospital_Location',\n",
    "    'Order_Placed_Date', 'Delivery_Date'\n",
    "]\n",
    "X = df.drop(columns=drop_cols)\n",
    "\n",
    "print(\"\\n[6/10] Train-test split (80/20)...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: POST-SPLIT PIPELINES\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" BUILDING V12 ROBUST PIPELINES (USING ROBUSTSCALER + OHE)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n[8/10] Configuring feature transformers...\")\n",
    "\n",
    "# --- Numeric Features ---\n",
    "numeric_features = [\n",
    "    'Supplier_Reliability', 'Equipment_Value', 'Base_Transport_Fee',\n",
    "    'Delivery_Days', 'Equipment_Volume'\n",
    "]\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', RobustScaler()) \n",
    "])\n",
    "print(f\"   âœ“ Numeric features: median imputation + RobustScaler\")\n",
    "\n",
    "# --- Categorical Features ---\n",
    "categorical_features = [\n",
    "    'Equipment_Type', 'CrossBorder_Shipping', 'Urgent_Shipping',\n",
    "    'Installation_Service', 'Transport_Method', 'Fragile_Equipment',\n",
    "    'Hospital_Info', 'Rural_Hospital', 'Order_Month',\n",
    "    'Order_Day_of_Week', 'Order_Is_Weekend'\n",
    "]\n",
    "# We are back to using OneHotEncoder. This is reliable.\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "print(f\"   âœ“ Categorical features: 'Missing' imputation + OneHotEncoder\")\n",
    "\n",
    "# ==============================================================================\n",
    "print(\"\\n[9/10] Assembling ColumnTransformer...\")\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "print(f\"   âœ“ ColumnTransformer 'preprocessor' configured\")\n",
    "\n",
    "# ==============================================================================\n",
    "print(\"\\n[10/10] Applying preprocessing...\")\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "print(f\"   âœ“ V12 Preprocessing complete.\")\n",
    "print(f\"   âœ“ Training set processed: {X_train_processed.shape}\")\n",
    "print(f\"   âœ“ Test set processed:     {X_test_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "22a5044f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting V12 GridSearchCV for XGBoost (Focusing on MAX STABILITY)...\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "\n",
      "âœ… V12 GridSearch complete!\n",
      "   Best hyperparameters: {'xgb__colsample_bytree': 0.7, 'xgb__learning_rate': 0.05, 'xgb__max_depth': 2, 'xgb__n_estimators': 1000, 'xgb__reg_alpha': 10, 'xgb__reg_lambda': 10, 'xgb__subsample': 0.7}\n",
      "   Best CV RMSE (log-space): 0.3771\n",
      "\n",
      "   Test RMSE (log-space)      : 0.3606\n",
      "   Test RMSE (original scale) : 34062.36\n"
     ]
    }
   ],
   "source": [
    "# === CELL 2: V12 GRIDSEARCHCV (FOR \"DUMBER\" XGBOOST) ===\n",
    "\n",
    "print(\"ðŸš€ Starting V12 GridSearchCV for XGBoost (Focusing on MAX STABILITY)...\")\n",
    "\n",
    "# --- 1. CV splitter ---\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# --- 2. XGB pipeline ---\n",
    "# 'preprocessor' is your V12 (RobustScaler + OHE) preprocessor\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor), \n",
    "    ('xgb', XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# --- 3. V12 Hyperparameter grid (Even dumber/more regularized) ---\n",
    "param_grid = {\n",
    "    'xgb__n_estimators': [1000],\n",
    "    'xgb__max_depth': [2],               # FORCE max_depth=2\n",
    "    'xgb__learning_rate': [0.05],\n",
    "    'xgb__subsample': [0.5, 0.7],        # NEW: Use only a fraction of data per tree\n",
    "    'xgb__colsample_bytree': [0.5, 0.7], # NEW: Use only a fraction of features per tree\n",
    "    'xgb__reg_alpha': [10, 100],         # Aggressive L1\n",
    "    'xgb__reg_lambda': [10, 100]         # Aggressive L2\n",
    "}\n",
    "\n",
    "# --- 4. GridSearchCV ---\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=kf,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# --- 5. Run grid search on the V12 TRAINING DATA ---\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# --- 6. Best params & CV score ---\n",
    "print(\"\\nâœ… V12 GridSearch complete!\")\n",
    "best_params = grid_search.best_params_\n",
    "best_cv_rmse = -grid_search.best_score_\n",
    "print(f\"   Best hyperparameters: {best_params}\")\n",
    "print(f\"   Best CV RMSE (log-space): {best_cv_rmse:.4f}\")\n",
    "\n",
    "# --- 7. Evaluate on V12 test set ---\n",
    "final_xgb_v12 = grid_search.best_estimator_\n",
    "y_test_pred_log = final_xgb_v12.predict(X_test)\n",
    "y_test_pred_orig = np.expm1(y_test_pred_log)\n",
    "\n",
    "rmse_test_log = np.sqrt(mean_squared_error(y_test, y_test_pred_log))\n",
    "rmse_test_orig = np.sqrt(mean_squared_error(np.expm1(y_test), y_test_pred_orig))\n",
    "\n",
    "print(f\"\\n   Test RMSE (log-space)      : {rmse_test_log:.4f}\")\n",
    "print(f\"   Test RMSE (original scale) : {rmse_test_orig:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "def69858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ“ V12 `prepare_features_V12` function created.\n"
     ]
    }
   ],
   "source": [
    "# === CELL 3: DEFINE THE V12 'PREPARE_FEATURES' FUNCTION ===\n",
    "\n",
    "# This is the same as V4/V6/V8 - our stable function\n",
    "def prepare_features_V12(df_raw):\n",
    "    \"\"\"\n",
    "    Applies all V12 (robust) manual cleaning and feature engineering.\n",
    "    No clipping is needed as the RobustScaler pipeline handles outliers.\n",
    "    \"\"\"\n",
    "    df = df_raw.copy()\n",
    "    \n",
    "    # 1. Clean column names\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    # 2. Clean all string/object columns\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "        df[col] = df[col].replace({'': np.nan, 'nan': np.nan, 'NaN': np.nan})\n",
    "\n",
    "    # 3. Normalize Yes/No columns\n",
    "    yes_no_cols = ['CrossBorder_Shipping', 'Urgent_Shipping', 'Installation_Service',\n",
    "                   'Fragile_Equipment', 'Rural_Hospital']\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        if col in yes_no_cols:\n",
    "            df[col] = df[col].replace({\n",
    "                'YES': 'Yes', 'yes': 'Yes', 'Y': 'Yes', 'y': 'Yes',\n",
    "                'NO': 'No', 'no': 'No', 'N': 'No', 'n': 'No'\n",
    "            })\n",
    "\n",
    "    # 4. Convert date columns\n",
    "    df['Order_Placed_Date'] = pd.to_datetime(df['Order_Placed_Date'], errors='coerce')\n",
    "    df['Delivery_Date'] = pd.to_datetime(df['Delivery_Date'], errors='coerce')\n",
    "\n",
    "    # 5. Engineer Date Features\n",
    "    df['Delivery_Days'] = (df['Delivery_Date'] - df['Order_Placed_Date']).dt.days\n",
    "    df['Delivery_Days'] = df['Delivery_Days'].abs() \n",
    "    df['Order_Month'] = df['Order_Placed_Date'].dt.month\n",
    "    df['Order_Day_of_Week'] = df['Order_Placed_Date'].dt.dayofweek\n",
    "    df['Order_Is_Weekend'] = df['Order_Day_of_Week'].isin([5, 6])\n",
    "    \n",
    "    # 7. ==== ENGINEERING & LOG-TRANSFORM ====\n",
    "    df['Equipment_Volume'] = df['Equipment_Height'] * df['Equipment_Width']\n",
    "    df['Equipment_Value'] = np.log1p(df['Equipment_Value'])\n",
    "    df['Equipment_Volume'] = np.log1p(df['Equipment_Volume'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"   âœ“ V12 `prepare_features_V12` function created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "765b20ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training final V12 XGBoost model on all V12 data...\n",
      "   âœ“ Using best params from V12 GridSearch: {'xgb__colsample_bytree': 0.7, 'xgb__learning_rate': 0.05, 'xgb__max_depth': 2, 'xgb__n_estimators': 1000, 'xgb__reg_alpha': 10, 'xgb__reg_lambda': 10, 'xgb__subsample': 0.7}\n",
      "X shape: (5000, 16)\n",
      "y shape: (5000,)\n",
      "\n",
      "âœ… Final (V12 \"DUMBER\" XGBOOST) model trained on entire dataset.\n",
      "The 'final_xgb_v12_pipeline' object is ready for prediction.\n"
     ]
    }
   ],
   "source": [
    "# === CELL 4: TRAIN YOUR FINAL, BEST V12 MODEL ON ALL V12 DATA ===\n",
    "\n",
    "print(\"\\nTraining final V12 XGBoost model on all V12 data...\")\n",
    "\n",
    "# === 1. Feature groups (from our V12 script) ===\n",
    "numeric_features = [\n",
    "    'Supplier_Reliability', 'Equipment_Value', 'Base_Transport_Fee',\n",
    "    'Delivery_Days', 'Equipment_Volume'\n",
    "]\n",
    "categorical_features = [\n",
    "    'Equipment_Type', 'CrossBorder_Shipping', 'Urgent_Shipping',\n",
    "    'Installation_Service', 'Transport_Method', 'Fragile_Equipment',\n",
    "    'Hospital_Info', 'Rural_Hospital', 'Order_Month',\n",
    "    'Order_Day_of_Week', 'Order_Is_Weekend'\n",
    "]\n",
    "\n",
    "# === 2. Define V12 transformers (with RobustScaler + OHE) ===\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', RobustScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# === 3. Combine them ===\n",
    "final_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# === 4. Build final pipeline with BEST V12 hyperparameters ===\n",
    "print(f\"   âœ“ Using best params from V12 GridSearch: {best_params}\")\n",
    "\n",
    "model_params = {\n",
    "    key.replace('xgb__', ''): value \n",
    "    for key, value in best_params.items()\n",
    "}\n",
    "\n",
    "final_xgb_v12_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', final_preprocessor),\n",
    "    ('xgb', XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        **model_params # Unpacks the best params here\n",
    "    ))\n",
    "])\n",
    "\n",
    "# === 5. Fit on full V12 dataset ===\n",
    "# (X and y are from Cell 1)\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "final_xgb_v12_pipeline.fit(X, y)\n",
    "\n",
    "print(\"\\nâœ… Final (V12 \\\"DUMBER\\\" XGBOOST) model trained on entire dataset.\")\n",
    "\n",
    "print(\"The 'final_xgb_v12_pipeline' object is ready for prediction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "779a7662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading new test data (test.csv)...\n",
      "Applying V12 (RobustScaler) feature engineering...\n",
      "Getting predictions from the final V12 XGBoost model...\n",
      "\n",
      "Final Predictions:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hospital_Id</th>\n",
       "      <th>Transport_Cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fffe33003400</td>\n",
       "      <td>485.060547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fffe3700330036003600</td>\n",
       "      <td>252.996292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fffe3300390038003400</td>\n",
       "      <td>2019.174194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fffe310030003900</td>\n",
       "      <td>204.991043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fffe3700330031003200</td>\n",
       "      <td>1029.840088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Hospital_Id  Transport_Cost\n",
       "0          fffe33003400      485.060547\n",
       "1  fffe3700330036003600      252.996292\n",
       "2  fffe3300390038003400     2019.174194\n",
       "3      fffe310030003900      204.991043\n",
       "4  fffe3700330031003200     1029.840088"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Submission file 'submission_XGB_V12_Dumber.csv' created successfully.\n",
      "UPLOAD THIS FILE TO KAGGLE!\n"
     ]
    }
   ],
   "source": [
    "# === CELL 5: GENERATE YOUR FINAL V12 SUBMISSION ===\n",
    "\n",
    "# 1. Load your new, raw test data\n",
    "print(\"Loading new test data (test.csv)...\")\n",
    "df_new_test = pd.read_csv('../data/test.csv') \n",
    "\n",
    "# 2. Save IDs\n",
    "submission_ids = df_new_test['Hospital_Id']\n",
    "\n",
    "# 3. Apply the *V12* feature engineering\n",
    "print(\"Applying V12 (RobustScaler) feature engineering...\")\n",
    "X_new_prepared = prepare_features_V12(df_new_test) # Using the new V12 function\n",
    "\n",
    "# 4. Get predictions FROM THE ROBUST V12 MODEL\n",
    "print(\"Getting predictions from the final V12 XGBoost model...\")\n",
    "log_predictions = final_xgb_v12_pipeline.predict(X_new_prepared)\n",
    "\n",
    "# 5. Convert predictions back from log-scale\n",
    "final_predictions = np.expm1(log_predictions)\n",
    "\n",
    "# 6. Create the final submission file\n",
    "submission_df = pd.DataFrame({\n",
    "    'Hospital_Id': submission_ids,\n",
    "    'Transport_Cost': final_predictions\n",
    "})\n",
    "\n",
    "# Display the first few predictions\n",
    "print(\"\\nFinal Predictions:\")\n",
    "display(submission_df.head())\n",
    "\n",
    "# Save to CSV\n",
    "submission_df.to_csv('submission_XGB_V12_Dumber.csv', index=False)\n",
    "print(\"\\nâœ… Submission file 'submission_XGB_V12_Dumber.csv' created successfully.\")\n",
    "print(\"UPLOAD THIS FILE TO KAGGLE!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
