{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad8d2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import PowerTransformer, OneHotEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score , GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from lightgbm import LGBMRegressor # A powerful gradient boosting model\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abbea68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4617a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "\n",
    "# 1ï¸âƒ£ Load Data\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv('../data/train.csv')\n",
    "df.columns = df.columns.str.strip()\n",
    "display(df.head())\n",
    "print(f\"Initial data shape: {df.shape}\")\n",
    "\n",
    "# 2ï¸âƒ£ Clean all string/object columns: strip spaces, replace blanks with NaN\n",
    "print(\"Cleaning string columns...\")\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    df[col] = df[col].astype(str).str.strip()\n",
    "    df[col] = df[col].replace({'': np.nan, 'nan': np.nan, 'NaN': np.nan})\n",
    "\n",
    "# 3ï¸âƒ£ Normalize Yes/No columns to consistent \"Yes\"/\"No\"\n",
    "print(\"Normalizing Yes/No columns...\")\n",
    "yes_no_cols = ['CrossBorder_Shipping', 'Urgent_Shipping', 'Installation_Service',\n",
    "               'Fragile_Equipment', 'Rural_Hospital']\n",
    "for col in yes_no_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace({\n",
    "            'YES': 'Yes', 'yes': 'Yes', 'Y': 'Yes', 'y': 'Yes',\n",
    "            'NO': 'No', 'no': 'No', 'N': 'No', 'n': 'No'\n",
    "        })\n",
    "\n",
    "# 4ï¸âƒ£ Convert date columns to datetime\n",
    "print(\"Converting date columns...\")\n",
    "df['Order_Placed_Date'] = pd.to_datetime(df['Order_Placed_Date'], errors='coerce')\n",
    "df['Delivery_Date'] = pd.to_datetime(df['Delivery_Date'], errors='coerce')\n",
    "\n",
    "# 5ï¸âƒ£ Create new feature: Delivery_Days (difference in days)\n",
    "print(\"Engineering Delivery_Days feature...\")\n",
    "df['Delivery_Days'] = (df['Delivery_Date'] - df['Order_Placed_Date']).dt.days\n",
    "df['Delivery_Days'] = pd.to_numeric(df['Delivery_Days'], errors='coerce')\n",
    "\n",
    "# === ADDED: Date Feature Engineering ===\n",
    "print(\"Engineering more date features...\")\n",
    "df['Order_Month'] = df['Order_Placed_Date'].dt.month\n",
    "df['Order_Day_of_Week'] = df['Order_Placed_Date'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "df['Order_Is_Weekend'] = df['Order_Day_of_Week'].isin([5, 6])\n",
    "# === END ADDED ===\n",
    "\n",
    "# 6ï¸âƒ£ (Original) delete initial date rows\n",
    "# df = df.dropna(subset=['Order_Placed_Date', 'Delivery_Date'])\n",
    "\n",
    "# 7ï¸âƒ£ Drop exact duplicate rows\n",
    "print(\"Dropping duplicates...\")\n",
    "before = len(df)\n",
    "df = df.drop_duplicates()\n",
    "after = len(df)\n",
    "print(f\"Dropped {before - after} duplicate rows.\")\n",
    "\n",
    "# 8ï¸âƒ£ Quick check after cleaning\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\" CLEANING & FEATURE ENGINEERING COMPLETE \")\n",
    "print(\"=\"*30)\n",
    "print(f\"After basic cleaning shape: {df.shape}\")\n",
    "\n",
    "print(\"\\nMissing values (raw count):\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "# === ADDED: Missing Value Percentage View ===\n",
    "print(\"\\nMissing values (percentage):\")\n",
    "missing_pct = (df.isna().sum() / len(df) * 100).sort_values(ascending=False)\n",
    "print(missing_pct[missing_pct > 0])\n",
    "# === END ADDED ===\n",
    "\n",
    "print(\"\\nDataFrame head:\")\n",
    "display(df.head())\n",
    "# print(df['Delivery_Days'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6847d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# ðŸ“Š START OF EXPLORATORY DATA ANALYSIS (EDA)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\" STARTING EDA \")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# ðŸ”¹ Define column lists\n",
    "num_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "# === ADDED: Exclude new date features from 'num_cols' for general stats ===\n",
    "date_num_features = ['Order_Month', 'Order_Day_of_Week', 'Delivery_Days']\n",
    "for col in ['Transport_Cost'] + date_num_features:\n",
    "    if col in num_cols:\n",
    "        num_cols.remove(col)\n",
    "# === END ADDED ===\n",
    "        \n",
    "cat_cols = df.select_dtypes(include='object').columns.tolist()\n",
    "# === ADDED: Add boolean 'Is_Weekend' to cat_cols for analysis ===\n",
    "if 'Order_Is_Weekend' in df.columns:\n",
    "    cat_cols.append('Order_Is_Weekend')\n",
    "# === END ADDED ===\n",
    "\n",
    "print(f\"Numeric features identified: {num_cols}\")\n",
    "print(f\"Categorical features identified: {cat_cols}\")\n",
    "print(f\"Date-derived features identified: {date_num_features}\")\n",
    "\n",
    "\n",
    "# === ADDED: 1. Target Variable Analysis (Transport_Cost) ===\n",
    "print(\"\\n===== 1. TARGET VARIABLE ANALYSIS: Transport_Cost =====\")\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Original Distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['Transport_Cost'], kde=True, bins=40)\n",
    "plt.title('Distribution of Transport_Cost (Original)')\n",
    "plt.xlabel('Transport_Cost')\n",
    "\n",
    "# Plot 2: Log-Transformed Distribution\n",
    "# We add 1 to handle potential zero values before logging\n",
    "plt.subplot(1, 2, 2)\n",
    "log_target = np.log1p(df['Transport_Cost'])\n",
    "sns.histplot(log_target, kde=True, bins=40, color='green')\n",
    "plt.title('Distribution of log(Transport_Cost + 1)')\n",
    "plt.xlabel('log(Transport_Cost + 1)')\n",
    "\n",
    "plt.suptitle('Target Variable Distribution Analysis', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "print(f\"Skewness of Transport_Cost: {df['Transport_Cost'].skew():.4f}\")\n",
    "print(f\"Skewness of log(Transport_Cost + 1): {log_target.skew():.4f}\")\n",
    "# === END ADDED ===\n",
    "\n",
    "\n",
    "print(\"\\n===== 2. NUMERIC FEATURE ANALYSIS =====\")\n",
    "print(\"===== BASIC NUMERIC STATISTICS =====\")\n",
    "if not num_cols:\n",
    "    print(\"No numeric columns found to describe (excluding target/dates).\")\n",
    "else:\n",
    "    display(df[num_cols].describe().T)\n",
    "\n",
    "    print(\"\\n===== SKEWNESS =====\")\n",
    "    display(df[num_cols].skew())\n",
    "\n",
    "# ðŸ”¹ Numeric distributions + boxplots\n",
    "# (Your original loop)\n",
    "# === MODIFIED: Added a check for empty list ===\n",
    "print(\"\\nGenerating numeric distribution plots...\")\n",
    "analysis_num_cols = num_cols + ['Delivery_Days'] # Add Delivery_Days back for plotting\n",
    "if 'Transport_Cost' not in analysis_num_cols:\n",
    "    analysis_num_cols.append('Transport_Cost') # Add Target back for plotting\n",
    "    \n",
    "for col in analysis_num_cols:\n",
    "    if col in df.columns:\n",
    "        plt.figure(figsize=(12,4))\n",
    "        \n",
    "        plt.subplot(1,2,1)\n",
    "        sns.histplot(df[col], kde=True, bins=30)\n",
    "        plt.title(f'{col} distribution')\n",
    "        \n",
    "        plt.subplot(1,2,2)\n",
    "        sns.boxplot(x=df[col])\n",
    "        plt.title(f'{col} boxplot')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Warning: Column '{col}' not found for plotting.\")\n",
    "\n",
    "\n",
    "print(\"\\n===== 3. CORRELATION ANALYSIS =====\")\n",
    "# ðŸ”¹ Correlation heatmap\n",
    "# (Your original code)\n",
    "plt.figure(figsize=(10,8))\n",
    "corr = df[num_cols + ['Transport_Cost', 'Delivery_Days']].corr()\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n===== 4. CATEGORICAL FEATURE ANALYSIS =====\")\n",
    "# ðŸ”¹ Categorical distributions\n",
    "# (Your original loop)\n",
    "print(\"\\nGenerating categorical distribution plots...\")\n",
    "high_cardinality_cols = []\n",
    "for col in cat_cols:\n",
    "    print(f\"\\n===== Column: {col} =====\")\n",
    "    print(df[col].value_counts(dropna=False))\n",
    "    \n",
    "    nunique = df[col].nunique()\n",
    "    if nunique > 20:\n",
    "        high_cardinality_cols.append(col)\n",
    "        print(f\"SKIPPING countplot for {col} (High Cardinality: {nunique} unique values)\")\n",
    "        continue\n",
    "        \n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.countplot(y=col, data=df, order=df[col].value_counts().index)\n",
    "    plt.title(f'Count of {col}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# === ADDED: 4a. High-Cardinality Column Summary ===\n",
    "print(\"\\n===== 4a. HIGH-CARDINALITY CATEGORICAL SUMMARY =====\")\n",
    "if high_cardinality_cols:\n",
    "    print(f\"High-cardinality features detected: {high_cardinality_cols}\")\n",
    "    for col in high_cardinality_cols:\n",
    "        print(f\"\\n--- Top 10 values for: {col} ---\")\n",
    "        print(df[col].value_counts(dropna=False).head(10))\n",
    "        print(f\"...and {df[col].nunique() - 10} other unique values.\")\n",
    "else:\n",
    "    print(\"No high-cardinality categorical features detected (threshold > 20).\")\n",
    "# === END ADDED ===\n",
    "\n",
    "\n",
    "print(\"\\n===== 5. BIVARIATE ANALYSIS (FEATURES vs. TARGET) =====\")\n",
    "# ðŸ”¹ Numeric features vs target\n",
    "# (Your original loop)\n",
    "print(\"\\nGenerating numeric features vs. Transport_Cost...\")\n",
    "for col in num_cols + ['Delivery_Days']:\n",
    "    if col in df.columns:\n",
    "        plt.figure(figsize=(6,4))\n",
    "        sns.scatterplot(x=df[col], y=df['Transport_Cost'])\n",
    "        plt.title(f'{col} vs Transport_Cost')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# ðŸ”¹ Categorical features vs target (low-cardinality)\n",
    "# (Your original loop)\n",
    "print(\"\\nGenerating categorical features vs. Transport_Cost...\")\n",
    "for col in cat_cols:\n",
    "    if col in df.columns and df[col].nunique() < 20:\n",
    "        plt.figure(figsize=(10,4))\n",
    "        sns.boxplot(x=col, y='Transport_Cost', data=df)\n",
    "        plt.title(f'{col} vs Transport_Cost')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# === ADDED: 5a. Date-Derived Features vs. Target ===\n",
    "print(\"\\nGenerating date-derived features vs. Transport_Cost...\")\n",
    "date_features_to_plot = ['Order_Month', 'Order_Day_of_Week', 'Order_Is_Weekend']\n",
    "for col in date_features_to_plot:\n",
    "    if col in df.columns:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        sns.boxplot(x=col, y='Transport_Cost', data=df)\n",
    "        plt.title(f'{col} vs Transport_Cost')\n",
    "        if col == 'Order_Day_of_Week':\n",
    "            plt.xticks(ticks=range(7), labels=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "# === END ADDED ===\n",
    "\n",
    "\n",
    "print(\"\\n===== 6. OUTLIER DETECTION =====\")\n",
    "# ðŸ”¹ Outlier detection (Z-score)\n",
    "# (Your original code)\n",
    "# === MODIFIED: Added nan_policy='omit' to handle missing values gracefully ===\n",
    "try:\n",
    "    z_scores = df[num_cols + ['Transport_Cost', 'Delivery_Days']].apply(lambda x: zscore(x, nan_policy='omit'))\n",
    "    outliers = (abs(z_scores) > 3).sum()\n",
    "    print(\"\\n===== NUMBER OF OUTLIERS PER COLUMN (Z-score > 3) =====\")\n",
    "    print(outliers[outliers > 0].sort_values(ascending=False))\n",
    "except ValueError as e:\n",
    "    print(f\"Could not calculate Z-scores, likely due to all-NaN column. Error: {e}\")\n",
    "# === END MODIFIED ===\n",
    "\n",
    "\n",
    "print(\"\\n===== 7. MISSING VALUE VISUALIZATION =====\")\n",
    "# ðŸ”¹ Missing value visualization\n",
    "# (Your original code)\n",
    "print(\"\\nGenerating missing value matrix...\")\n",
    "msno.matrix(df)\n",
    "plt.title('Missing Value Matrix')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nGenerating missing value bar chart...\")\n",
    "msno.bar(df)\n",
    "plt.title('Missing Value Bar Chart')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\" EDA COMPLETE \")\n",
    "print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ad723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preprocessing script started...\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: PRE-SPLIT (Data Cleaning & Feature Engineering)\n",
    "# These actions are applied to the whole dataset before splitting.\n",
    "# ==============================================================================\n",
    "\n",
    "# 1. Filter Bad Data\n",
    "# EDA Finding: We found impossible values like Transport_Cost < 0 and Delivery_Days < 0.\n",
    "# Action: Remove these rows entirely.\n",
    "initial_rows = len(df)\n",
    "df = df[df['Transport_Cost'] >= 0]\n",
    "df = df[df['Delivery_Days'] >= 0]\n",
    "print(f\"Filtered {initial_rows - len(df)} rows with bad data (negative cost or delivery days).\")\n",
    "\n",
    "# 2. Feature Engineering\n",
    "# EDA Finding: Equipment_Height & Equipment_Width were highly correlated (0.77).\n",
    "# Action: Combine them into a single 'Equipment_Volume' feature.\n",
    "df['Equipment_Volume'] = df['Equipment_Height'] * df['Equipment_Width']\n",
    "\n",
    "# 3. Log-Transform Skewed Features\n",
    "# EDA Finding: Equipment_Value (skew=24) and our new Equipment_Volume\n",
    "# (derived from skewed features) are extremely right-skewed.\n",
    "# Action: Apply np.log1p to normalize them.\n",
    "df['Equipment_Value'] = np.log1p(df['Equipment_Value'])\n",
    "df['Equipment_Volume'] = np.log1p(df['Equipment_Volume'])\n",
    "\n",
    "# 4. Define Target (y) and Features (X)\n",
    "# EDA Finding: Target 'Transport_Cost' is extremely skewed (skew=30).\n",
    "# Action: Use np.log1p on the target. We will predict the log, then convert back.\n",
    "y = np.log1p(df['Transport_Cost'])\n",
    "\n",
    "# Action: Define X by dropping the target, original engineered columns, \n",
    "# and high-cardinality/redundant/ID columns identified in the EDA.\n",
    "X = df.drop(columns=[\n",
    "    # Target\n",
    "    'Transport_Cost',\n",
    "    \n",
    "    # Replaced by Equipment_Volume\n",
    "    'Equipment_Height',\n",
    "    'Equipment_Width',\n",
    "    \n",
    "    # Redundant (corr 0.90 with Value)\n",
    "    'Equipment_Weight',\n",
    "    \n",
    "    # High-Cardinality IDs / Unused\n",
    "    'Hospital_Id',\n",
    "    'Supplier_Name',\n",
    "    'Hospital_Location',\n",
    "    \n",
    "    # Replaced by date features\n",
    "    'Order_Placed_Date',\n",
    "    'Delivery_Date'\n",
    "])\n",
    "\n",
    "print(f\"Features for modeling: {X.columns.tolist()}\")\n",
    "\n",
    "# 5. Train-Test Split\n",
    "# Action: Split the data into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "train_mean = y_train.mean()\n",
    "\n",
    "# 1ï¸âƒ£ Baseline RMSE on the training set (log-space)\n",
    "y_train_pred_baseline = np.full_like(y_train, train_mean)\n",
    "baseline_rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred_baseline))\n",
    "print(f\"Baseline RMSE (train, log-space): {baseline_rmse_train:.4f}\")\n",
    "\n",
    "# 2ï¸âƒ£ Baseline RMSE on the test set (using train mean as predictor) â€” log-space\n",
    "y_test_pred_baseline = np.full_like(y_test, train_mean)\n",
    "baseline_rmse_test_log = np.sqrt(mean_squared_error(y_test, y_test_pred_baseline))\n",
    "print(f\"Baseline RMSE (test, log-space): {baseline_rmse_test_log:.4f}\")\n",
    "\n",
    "# 3ï¸âƒ£ Baseline RMSE in original (Transport_Cost) scale\n",
    "y_test_actual_orig = np.expm1(y_test)\n",
    "y_test_baseline_pred_orig = np.expm1(y_test_pred_baseline)\n",
    "baseline_rmse_test_orig = np.sqrt(mean_squared_error(y_test_actual_orig, y_test_baseline_pred_orig))\n",
    "print(f\"Baseline RMSE (test, original-scale): {baseline_rmse_test_orig:.4f}\")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: POST-SPLIT (Pipelines & ColumnTransformer)\n",
    "# This prevents data leakage. We FIT on X_train, then TRANSFORM X_train and X_test.\n",
    "# ==============================================================================\n",
    "\n",
    "# 1. Define Feature Lists\n",
    "# Action: Separate our final columns into numeric and categorical lists.\n",
    "\n",
    "numeric_features = [\n",
    "    'Supplier_Reliability',\n",
    "    'Equipment_Value',      # Already log-transformed\n",
    "    'Base_Transport_Fee',\n",
    "    'Delivery_Days',\n",
    "    'Equipment_Volume'      # Already log-transformed\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'Equipment_Type',\n",
    "    'CrossBorder_Shipping',\n",
    "    'Urgent_Shipping',\n",
    "    'Installation_Service',\n",
    "    'Transport_Method',\n",
    "    'Fragile_Equipment',\n",
    "    'Hospital_Info',\n",
    "    'Rural_Hospital',\n",
    "    'Order_Month',\n",
    "    'Order_Day_of_Week',\n",
    "    'Order_Is_Weekend'\n",
    "]\n",
    "\n",
    "# 2. Create the Numeric Pipeline\n",
    "# EDA Finding: Numeric features had missing values (e.g., Supplier_Reliability)\n",
    "# and were on different scales.\n",
    "# Action: Impute missing values with the median (robust to outliers)\n",
    "# and then scale all features.\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# 3. Create the Categorical Pipeline\n",
    "# EDA Finding: Categorical features had missing values (e.g., Transport_Method,\n",
    "# Rural_Hospital) and need to be converted to numbers.\n",
    "# Action: Impute missing values with the most frequent value and then\n",
    "# one-hot encode. 'handle_unknown='ignore'' ensures our model doesn't\n",
    "# crash if it sees a new category in the test data.\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# 4. Create the Full Preprocessor\n",
    "# Action: Combine the numeric and categorical pipelines using ColumnTransformer.\n",
    "# This single object will handle all preprocessing for us.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'  # Drop any columns we didn't explicitly list\n",
    ")\n",
    "\n",
    "# 5. Apply the Preprocessor\n",
    "# Action: FIT the preprocessor on X_train ONLY (to learn medians, modes, etc.)\n",
    "# and then TRANSFORM both X_train and X_test.\n",
    "# This gives us our final, model-ready datasets.\n",
    "\n",
    "print(\"\\nFitting preprocessor on X_train...\")\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"Preprocessing complete.\")\n",
    "print(f\"Processed X_train shape: {X_train_processed.shape}\")\n",
    "print(f\"Processed X_test shape: {X_test_processed.shape}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# K-Fold Validation Setup (Bonus)\n",
    "# ==============================================================================\n",
    "# You mentioned K-Fold. The *best* way to use this `preprocessor` is to\n",
    "# put it in a pipeline with your model. This way, the K-Fold cross-validation\n",
    "# will correctly re-fit the preprocessor on each fold, preventing all leakage.\n",
    "\n",
    "# Example (don't run, just for info):\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# 1. Choose a model\n",
    "# model = LinearRegression()\n",
    "\n",
    "# 2. Create the full ML pipeline\n",
    "# full_pipeline = Pipeline(steps=[\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('regressor', model)\n",
    "# ])\n",
    "\n",
    "# 3. Run cross-validation on the *original* X_train and y_train\n",
    "# This is the correct, leak-proof way to do it.\n",
    "# scores = cross_val_score(full_pipeline, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error')\n",
    "# print(f\"K-Fold Scores: {scores}\")\n",
    "# print(f\"Mean RMSE: {np.mean(scores)}\")\n",
    "\n",
    "# 4. Fit the final model\n",
    "# full_pipeline.fit(X_train, y_train)\n",
    "# print(\"Final model pipeline is fitted and ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0a25e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0dbad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----- 1) Set up 5-Fold -----\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# ----- 2) Create the pipeline -----\n",
    "lr_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),  # your ColumnTransformer\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "# ----- 3) Run 5-Fold Cross-Validation -----\n",
    "cv_scores = cross_val_score(\n",
    "    lr_pipeline,\n",
    "    X_train,       # raw training features\n",
    "    y_train,       # log-transformed target\n",
    "    cv=kf,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "avg_rmse = np.abs(cv_scores).mean()\n",
    "print(f\"Linear Regression 5-Fold Avg. RMSE (log-space): {avg_rmse:.4f}\")\n",
    "\n",
    "# ----- 4) Fit final model on full training data -----\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "print(\"Linear Regression final model trained on full training set.\")\n",
    "\n",
    "# ----- 5) Predict on test set -----\n",
    "y_test_pred_log = lr_pipeline.predict(X_test)\n",
    "y_test_pred_orig = np.expm1(y_test_pred_log)  # back to original scale\n",
    "\n",
    "rmse_test_log = np.sqrt(np.mean((y_test - y_test_pred_log)**2))\n",
    "rmse_test_orig = np.sqrt(np.mean((np.expm1(y_test) - y_test_pred_orig)**2))\n",
    "\n",
    "print(f\"Test RMSE (log-space)      : {rmse_test_log:.4f}\")\n",
    "print(f\"Test RMSE (original scale) : {rmse_test_orig:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe61007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----- 1) Set up 5-Fold -----\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# ----- 2) Create the pipeline -----\n",
    "poly_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),         # your ColumnTransformer\n",
    "    ('poly', PolynomialFeatures()),         # will tune degree\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "# ----- 3) Set up GridSearch for hyperparameter tuning -----\n",
    "param_grid = {\n",
    "    'poly__degree': [2, 3, 4, 5]   # you can expand to 5 if dataset is small\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    poly_pipeline,\n",
    "    param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=kf,\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "# ----- 4) Run GridSearch -----\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best degree\n",
    "best_degree = grid_search.best_params_['poly__degree']\n",
    "best_rmse = -grid_search.best_score_\n",
    "print(f\"Best polynomial degree: {best_degree}\")\n",
    "print(f\"Best CV RMSE (log-space): {best_rmse:.4f}\")\n",
    "\n",
    "# ----- 5) Fit final model on full training data -----\n",
    "final_poly_model = grid_search.best_estimator_\n",
    "final_poly_model.fit(X_train, y_train)\n",
    "print(\"Polynomial Regression final model trained on full training set.\")\n",
    "\n",
    "# ----- 6) Predict on test set -----\n",
    "y_test_pred_log = final_poly_model.predict(X_test)\n",
    "y_test_pred_orig = np.expm1(y_test_pred_log)  # back to original scale\n",
    "\n",
    "rmse_test_log = np.sqrt(np.mean((y_test - y_test_pred_log)**2))\n",
    "rmse_test_orig = np.sqrt(np.mean((np.expm1(y_test) - y_test_pred_orig)**2))\n",
    "\n",
    "print(f\"Test RMSE (log-space)      : {rmse_test_log:.4f}\")\n",
    "print(f\"Test RMSE (original scale) : {rmse_test_orig:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2959af42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----- 1) Create the pipeline -----\n",
    "ridge_poly_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),  # your ColumnTransformer\n",
    "    ('poly', PolynomialFeatures()),  # polynomial expansion\n",
    "    ('ridge', Ridge())               # ridge regression\n",
    "])\n",
    "\n",
    "# ----- 2) Set hyperparameter grid -----\n",
    "param_grid = {\n",
    "    'poly__degree': [1, 2, 3, 4, 5],      # try degrees 1, 2, 3, 4, 5\n",
    "    'ridge__alpha': [0.01, 0.1, 1, 10]  # try different regularization strengths\n",
    "}\n",
    "\n",
    "# ----- 3) Grid Search with 5-Fold CV -----\n",
    "grid_search = GridSearchCV(\n",
    "    ridge_poly_pipeline,\n",
    "    param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=5,\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "# ----- 4) Fit on training data -----\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# ----- 5) Best hyperparameters -----\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best CV RMSE (log-space):\", -grid_search.best_score_)\n",
    "\n",
    "# ----- 6) Predict on test set -----\n",
    "y_test_pred_log = grid_search.predict(X_test)\n",
    "y_test_pred_orig = np.expm1(y_test_pred_log)\n",
    "\n",
    "rmse_test_log = np.sqrt(np.mean((y_test - y_test_pred_log)**2))\n",
    "rmse_test_orig = np.sqrt(np.mean((np.expm1(y_test) - y_test_pred_orig)**2))\n",
    "\n",
    "print(f\"Test RMSE (log-space)      : {rmse_test_log:.4f}\")\n",
    "print(f\"Test RMSE (original scale) : {rmse_test_orig:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba86e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----- 1) Create the pipeline -----\n",
    "lasso_poly_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),  # your ColumnTransformer\n",
    "    ('poly', PolynomialFeatures()),  # polynomial expansion\n",
    "    ('lasso', Lasso(max_iter=10000)) # Lasso regression\n",
    "])\n",
    "\n",
    "# ----- 2) Set hyperparameter grid -----\n",
    "param_grid = {\n",
    "    'poly__degree': [1, 2, 3],       # try degrees 1, 2, 3\n",
    "    'lasso__alpha': [0.001, 0.01, 0.1, 1, 10]  # regularization strengths\n",
    "}\n",
    "\n",
    "# ----- 3) Grid Search with 5-Fold CV -----\n",
    "grid_search = GridSearchCV(\n",
    "    lasso_poly_pipeline,\n",
    "    param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# ----- 4) Fit on training data -----\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# ----- 5) Best hyperparameters -----\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best CV RMSE (log-space):\", -grid_search.best_score_)\n",
    "\n",
    "# ----- 6) Predict on test set -----\n",
    "y_test_pred_log = grid_search.predict(X_test)\n",
    "y_test_pred_orig = np.expm1(y_test_pred_log)\n",
    "\n",
    "rmse_test_log = np.sqrt(np.mean((y_test - y_test_pred_log)**2))\n",
    "rmse_test_orig = np.sqrt(np.mean((np.expm1(y_test) - y_test_pred_orig)**2))\n",
    "\n",
    "print(f\"Test RMSE (log-space)      : {rmse_test_log:.4f}\")\n",
    "print(f\"Test RMSE (original scale) : {rmse_test_orig:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099a12f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----- 1) CV splitter -----\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# ----- 2) Pipeline: preprocessor -> poly -> elastic net -----\n",
    "enet_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),                       # your ColumnTransformer\n",
    "    ('poly', PolynomialFeatures(include_bias=False)),     # polynomial expansion (tune degree)\n",
    "    ('enet', ElasticNet(max_iter=20000, random_state=42)) # ElasticNet regression\n",
    "])\n",
    "\n",
    "# ----- 3) Hyperparameter grid -----\n",
    "param_grid = {\n",
    "    'poly__degree': [1, 2, 3],                      # try degrees 1..3 (increase carefully)\n",
    "    'enet__alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1.0],   # regularization strengths\n",
    "    'enet__l1_ratio': [0.2, 0.5, 0.8]               # mix between L1 (1.0) and L2 (0.0)\n",
    "}\n",
    "\n",
    "# ----- 4) GridSearchCV (use n_jobs=1 in notebooks to avoid multiprocessing cwd issues) -----\n",
    "grid_search = GridSearchCV(\n",
    "    enet_pipeline,\n",
    "    param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=kf,\n",
    "    n_jobs=1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# ----- 5) Run grid search -----\n",
    "print(\"Starting GridSearchCV for ElasticNet + PolynomialFeatures ...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# ----- 6) Best params & CV score -----\n",
    "best_params = grid_search.best_params_\n",
    "best_cv_rmse = -grid_search.best_score_\n",
    "print(f\"\\nBest hyperparameters: {best_params}\")\n",
    "print(f\"Best CV RMSE (log-space): {best_cv_rmse:.4f}\")\n",
    "\n",
    "# ----- 7) Final model (best estimator) -----\n",
    "final_enet = grid_search.best_estimator_\n",
    "\n",
    "# ----- 8) Evaluate on test set -----\n",
    "y_test_pred_log = final_enet.predict(X_test)\n",
    "y_test_pred_orig = np.expm1(y_test_pred_log)\n",
    "\n",
    "rmse_test_log = np.sqrt(mean_squared_error(y_test, y_test_pred_log))\n",
    "rmse_test_orig = np.sqrt(mean_squared_error(np.expm1(y_test), y_test_pred_orig))\n",
    "\n",
    "print(f\"\\nTest RMSE (log-space)      : {rmse_test_log:.4f}\")\n",
    "print(f\"Test RMSE (original scale) : {rmse_test_orig:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MlLab)",
   "language": "python",
   "name": "mllab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
