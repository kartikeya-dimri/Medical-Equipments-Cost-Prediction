{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30138410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cell 1: Imports ---\n",
      "✅ Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "# === CELL 1: IMPORTS (V25 - Bare Bones + ElasticNet) ===\n",
    "print(\"--- Cell 1: Imports ---\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder # Using StandardScaler\n",
    "from sklearn.linear_model import ElasticNet # Using ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(\"✅ Libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c444df9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 2: Data Loading & Initial Cleaning ---\n",
      "Loaded train.csv: (5000, 20)\n",
      "Cleaning string columns...\n",
      "Normalizing Yes/No columns...\n",
      "Converting date columns...\n",
      "✅ Basic cleaning complete.\n"
     ]
    }
   ],
   "source": [
    "# === CELL 2: DATA LOADING & INITIAL CLEANING ===\n",
    "print(\"\\n--- Cell 2: Data Loading & Initial Cleaning ---\")\n",
    "try:\n",
    "    # Load the training data from the specified path\n",
    "    df = pd.read_csv('../data/train.csv')\n",
    "    df.columns = df.columns.str.strip() # Remove leading/trailing spaces from column names\n",
    "    print(f\"Loaded train.csv: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: ../data/train.csv not found.\")\n",
    "    df = None\n",
    "except Exception as e:\n",
    "    print(f\"Error loading train.csv: {e}\")\n",
    "    df = None\n",
    "\n",
    "if df is not None:\n",
    "    # Clean string columns\n",
    "    print(\"Cleaning string columns...\")\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        df[col] = df[col].astype(str).str.strip().replace({'': np.nan, 'nan': np.nan, 'NaN': np.nan})\n",
    "\n",
    "    # Normalize Yes/No columns\n",
    "    print(\"Normalizing Yes/No columns...\")\n",
    "    yes_no_cols = ['CrossBorder_Shipping', 'Urgent_Shipping', 'Installation_Service',\n",
    "                   'Fragile_Equipment', 'Rural_Hospital']\n",
    "    for col in df.columns:\n",
    "        if col in yes_no_cols:\n",
    "            df[col] = df[col].replace({ 'YES': 'Yes', 'yes': 'Yes', 'Y': 'Yes', 'y': 'Yes',\n",
    "                                        'NO': 'No', 'no': 'No', 'N': 'No', 'n': 'No' })\n",
    "\n",
    "    # Convert date columns\n",
    "    print(\"Converting date columns...\")\n",
    "    df['Order_Placed_Date'] = pd.to_datetime(df['Order_Placed_Date'], errors='coerce')\n",
    "    df['Delivery_Date'] = pd.to_datetime(df['Delivery_Date'], errors='coerce')\n",
    "    print(\"✅ Basic cleaning complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee7b81f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 3: V25 Feature Engineering & Repair (Minimal) ---\n",
      "[Step 1/4] Repairing negative values using abs()...\n",
      "   ✓ Calculated Prediction Cap (99th percentile): 269,745.26\n",
      "   ✓ Repaired negative costs and durations using abs().\n",
      "\n",
      "[Step 2/4] Defining target variable (y)...\n",
      "   ✓ Target (y) created: (5000,)\n",
      "\n",
      "[Step 3/4] Selecting features (X) - Keeping originals...\n",
      "   ✓ Feature set 'X' created (Minimal): (5000, 15)\n",
      "\n",
      "[Step 4/4] Defining Feature Lists for Pipeline...\n",
      "   ✓ Numeric Features: ['Supplier_Reliability', 'Equipment_Height', 'Equipment_Width', 'Equipment_Weight', 'Equipment_Value', 'Base_Transport_Fee', 'Delivery_Days']\n",
      "   ✓ Categorical Features: ['Equipment_Type', 'CrossBorder_Shipping', 'Urgent_Shipping', 'Installation_Service', 'Transport_Method', 'Fragile_Equipment', 'Hospital_Info', 'Rural_Hospital']\n",
      "\n",
      "Splitting data into Train/Validation (80/20)...\n",
      "   ✓ Data split complete.\n",
      "✅ V25 Minimal Feature Engineering complete.\n"
     ]
    }
   ],
   "source": [
    "# === CELL 3: V25 FEATURE ENGINEERING & DATA REPAIR (Minimal) ===\n",
    "if df is not None:\n",
    "    print(\"\\n--- Cell 3: V25 Feature Engineering & Repair (Minimal) ---\")\n",
    "\n",
    "    print(\"[Step 1/4] Repairing negative values using abs()...\")\n",
    "    # Engineer Delivery_Days first\n",
    "    df['Delivery_Days'] = (df['Delivery_Date'] - df['Order_Placed_Date']).dt.days\n",
    "    df['Delivery_Days'] = df['Delivery_Days'].abs() # Use abs()\n",
    "    # Apply abs() to Transport_Cost and store max for clipping later\n",
    "    df['Transport_Cost'] = df['Transport_Cost'].abs() # Use abs()\n",
    "    # Calculate prediction cap based on 99th percentile of ORIGINAL absolute cost\n",
    "    try:\n",
    "        # Calculate on non-NaN costs only\n",
    "        prediction_cap_value = df['Transport_Cost'].dropna().quantile(0.99)\n",
    "        print(f\"   ✓ Calculated Prediction Cap (99th percentile): {prediction_cap_value:,.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ! Warning: Could not calculate prediction cap: {e}. Using fallback.\")\n",
    "        prediction_cap_value = 1000000 # Fallback high value\n",
    "    print(\"   ✓ Repaired negative costs and durations using abs().\")\n",
    "\n",
    "\n",
    "    print(\"\\n[Step 2/4] Defining target variable (y)...\")\n",
    "    y = np.log1p(df['Transport_Cost']) # Log-transform of abs(cost)\n",
    "    print(f\"   ✓ Target (y) created: {y.shape}\")\n",
    "\n",
    "    print(\"\\n[Step 3/4] Selecting features (X) - Keeping originals...\")\n",
    "    # Keep original numeric features, drop IDs/dates/target\n",
    "    # NO Volume, NO Logs (except target), NO Date Parts, NO Missing Flags (initially)\n",
    "    drop_cols = [\n",
    "        'Transport_Cost',\n",
    "        'Hospital_Id', 'Supplier_Name', 'Hospital_Location',\n",
    "        'Order_Placed_Date', 'Delivery_Date',\n",
    "    ]\n",
    "    drop_cols_present = [col for col in drop_cols if col in df.columns]\n",
    "    X = df.drop(columns=drop_cols_present)\n",
    "    print(f\"   ✓ Feature set 'X' created (Minimal): {X.shape}\")\n",
    "    # print(f\"   ✓ Features: {X.columns.tolist()}\") # Uncomment to verify\n",
    "\n",
    "    print(\"\\n[Step 4/4] Defining Feature Lists for Pipeline...\")\n",
    "    # Define features based on columns NOW in X\n",
    "    numeric_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "    categorical_features = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "    print(f\"   ✓ Numeric Features: {numeric_features}\")\n",
    "    print(f\"   ✓ Categorical Features: {categorical_features}\")\n",
    "\n",
    "    # --- Split for Tuning ---\n",
    "    print(\"\\nSplitting data into Train/Validation (80/20)...\")\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print(f\"   ✓ Data split complete.\")\n",
    "\n",
    "    print(\"✅ V25 Minimal Feature Engineering complete.\")\n",
    "\n",
    "else:\n",
    "    print(\"   ✗ Feature engineering skipped due to data loading error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c9c67da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 4: V25 Preprocessing Pipeline Definition & Fitting ---\n",
      "   ✓ Transformers defined (Median Impute + StandardScaler + OHE).\n",
      "   ✓ Preprocessor assembled.\n",
      "Fitting preprocessor on X_train...\n",
      "✅ V25 Preprocessor fitted.\n"
     ]
    }
   ],
   "source": [
    "# === CELL 4: V25 PREPROCESSING PIPELINE DEFINITION & FITTING ===\n",
    "if 'X_train' in locals():\n",
    "    print(\"\\n--- Cell 4: V25 Preprocessing Pipeline Definition & Fitting ---\")\n",
    "\n",
    "    # --- Define Transformers ---\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()) # Using StandardScaler\n",
    "    ])\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False, drop=None))\n",
    "    ])\n",
    "    print(\"   ✓ Transformers defined (Median Impute + StandardScaler + OHE).\")\n",
    "\n",
    "    # --- Assemble Preprocessor ---\n",
    "    # Ensure feature lists are correctly defined from Cell 3\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "    print(\"   ✓ Preprocessor assembled.\")\n",
    "\n",
    "    # --- Fit Preprocessor ---\n",
    "    print(\"Fitting preprocessor on X_train...\")\n",
    "    preprocessor.fit(X_train) # Fit only on the training part\n",
    "    print(\"✅ V25 Preprocessor fitted.\")\n",
    "else:\n",
    "      print(\"   ✗ Preprocessor fitting skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b36585c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 5: Tuning ElasticNet (V25 Data) ---\n",
      "GridSearchCV Parameter Grid:\n",
      "{'enet__alpha': [0.001, 0.01, 0.1, 1.0, 10.0], 'enet__l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]}\n",
      "Starting GridSearchCV for ElasticNet...\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "\n",
      "✅ ElasticNet Tuning complete!\n",
      "   Best hyperparameters: {'enet__alpha': 0.001, 'enet__l1_ratio': 0.9}\n",
      "   Best CV RMSE (log-space): 0.8294\n",
      "--- Validation Set Performance ---\n",
      "   Validation RMSE (log-space): 0.7478\n",
      "   Validation RMSE (original scale): 41,698.49\n"
     ]
    }
   ],
   "source": [
    "# === CELL 5: HYPERPARAMETER TUNING (ELASTICNET on V25 DATA) ===\n",
    "if 'X_train' in locals() and 'preprocessor' in locals():\n",
    "    print(\"\\n--- Cell 5: Tuning ElasticNet (V25 Data) ---\")\n",
    "\n",
    "    # --- ElasticNet pipeline ---\n",
    "    enet_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor), # Use V25 preprocessor fitted in Cell 4\n",
    "        ('enet', ElasticNet(random_state=42, max_iter=10000)) # Increased max_iter\n",
    "    ])\n",
    "\n",
    "    # --- Hyperparameter grid ---\n",
    "    # Explore alpha and l1_ratio\n",
    "    param_grid = {\n",
    "        'enet__alpha': [0.001, 0.01, 0.1, 1.0, 10.0], # Similar range to Lasso/Ridge\n",
    "        'enet__l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]  # Mix between L1 and L2\n",
    "    }\n",
    "    print(f\"GridSearchCV Parameter Grid:\\n{param_grid}\")\n",
    "\n",
    "    # --- GridSearchCV setup ---\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    grid_search_enet = GridSearchCV(\n",
    "        estimator=enet_pipeline,\n",
    "        param_grid=param_grid,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        cv=kf,\n",
    "        n_jobs=-1, # Use all cores\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(\"Starting GridSearchCV for ElasticNet...\")\n",
    "    grid_search_enet.fit(X_train, y_train) # Fit on the 80% training split\n",
    "\n",
    "    print(\"\\n✅ ElasticNet Tuning complete!\")\n",
    "    best_params = grid_search_enet.best_params_ # Save the best params\n",
    "    best_cv_rmse = -grid_search_enet.best_score_\n",
    "    print(f\"   Best hyperparameters: {best_params}\")\n",
    "    print(f\"   Best CV RMSE (log-space): {best_cv_rmse:.4f}\")\n",
    "\n",
    "    # --- Evaluate on Validation Set ---\n",
    "    best_model_tuned = grid_search_enet.best_estimator_\n",
    "    y_val_pred_log = best_model_tuned.predict(X_val)\n",
    "    val_rmse_log = np.sqrt(mean_squared_error(y_val, y_val_pred_log))\n",
    "    y_val_orig = np.expm1(y_val)\n",
    "    y_val_pred_orig = np.maximum(np.expm1(y_val_pred_log), 0) # Clip preds >= 0\n",
    "    val_rmse_orig = np.sqrt(mean_squared_error(y_val_orig, y_val_pred_orig))\n",
    "    print(f\"--- Validation Set Performance ---\")\n",
    "    print(f\"   Validation RMSE (log-space): {val_rmse_log:.4f}\")\n",
    "    print(f\"   Validation RMSE (original scale): {val_rmse_orig:,.2f}\")\n",
    "\n",
    "else:\n",
    "    print(\"   ✗ Tuning skipped - check previous cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5fdad4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 6: Training Final ElasticNet Model (V25 Data + Best Params) ---\n",
      "   ✓ Using best params from ElasticNet GridSearch: {'enet__alpha': 0.001, 'enet__l1_ratio': 0.9}\n",
      "   Re-fitting V25 preprocessor on full dataset X...\n",
      "   ✓ Preprocessor re-fitted on full X dataset.\n",
      "Fitting final ElasticNet model on the full V25 dataset (X, y)...\n",
      "X shape: (5000, 15)\n",
      "y shape: (5000,)\n",
      "\n",
      "✅ Final (V25 ELASTICNET) model trained.\n",
      "The 'final_pipeline' object is ready for prediction.\n"
     ]
    }
   ],
   "source": [
    "# === CELL 6: TRAIN FINAL ELASTICNET MODEL (V25 DATA + BEST PARAMS) ===\n",
    "\n",
    "if 'best_params' in locals() and 'X' in locals() and 'y' in locals() and 'ElasticNet' in globals():\n",
    "    print(\"\\n--- Cell 6: Training Final ElasticNet Model (V25 Data + Best Params) ---\")\n",
    "\n",
    "    print(f\"   ✓ Using best params from ElasticNet GridSearch: {best_params}\")\n",
    "\n",
    "    # --- Re-fit preprocessor on FULL X data ---\n",
    "    print(\"   Re-fitting V25 preprocessor on full dataset X...\")\n",
    "    # Rebuild the final preprocessor object\n",
    "    numeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])\n",
    "    categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "                                            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False, drop=None))])\n",
    "    final_preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_features),\n",
    "                                                         ('cat', categorical_transformer, categorical_features)],\n",
    "                                           remainder='drop')\n",
    "    final_preprocessor.fit(X) # Fit on ALL X data defined in Cell 3 (V25)\n",
    "    print(\"   ✓ Preprocessor re-fitted on full X dataset.\")\n",
    "\n",
    "    # Map grid search param names\n",
    "    final_model_params = {key.replace('enet__', ''): value for key, value in best_params.items()}\n",
    "\n",
    "    # Create the final pipeline using the RE-FITTED preprocessor and best params\n",
    "    final_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', final_preprocessor), # Use the re-fitted preprocessor\n",
    "        ('enet', ElasticNet(\n",
    "            random_state=42,\n",
    "            max_iter=10000, # Keep increased iterations\n",
    "            **final_model_params # Unpack best alpha and l1_ratio\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # === Fit final model on full V25 dataset ===\n",
    "    print(\"Fitting final ElasticNet model on the full V25 dataset (X, y)...\")\n",
    "    print(\"X shape:\", X.shape)\n",
    "    print(\"y shape:\", y.shape)\n",
    "\n",
    "    final_pipeline.fit(X, y) # Train on full X and y from Cell 3 (V25)\n",
    "\n",
    "    print(\"\\n✅ Final (V25 ELASTICNET) model trained.\")\n",
    "    print(\"The 'final_pipeline' object is ready for prediction.\")\n",
    "\n",
    "else:\n",
    "    print(\"   ✗ Final model training skipped. Required variables not found.\")\n",
    "    if 'best_params' not in locals(): print(\"     - Best parameters not found (Run GridSearchCV - Cell 5).\")\n",
    "    if 'X' not in locals() or 'y' not in locals(): print(\"     - Full dataset 'X' or 'y' not defined (Cell 3).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7430b3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ V25 Test preparation function defined.\n"
     ]
    }
   ],
   "source": [
    "# === CELL 7: V25 TEST DATA PREPARATION FUNCTION ===\n",
    "\n",
    "# This function applies the minimal V25 steps\n",
    "def prepare_features_V25(df_raw, train_cols_expected):\n",
    "    \"\"\"Applies V25 cleaning and minimal FE to raw test data\"\"\"\n",
    "    print(\"Preparing test features (V25 - Bare Bones)...\")\n",
    "    df_test = df_raw.copy()\n",
    "    df_test.columns = df_test.columns.str.strip()\n",
    "\n",
    "    # Basic Cleaning (Strings, Yes/No, Dates)\n",
    "    for col in df_test.select_dtypes(include='object').columns:\n",
    "         df_test[col] = df_test[col].astype(str).str.strip().replace({'': np.nan, 'nan': np.nan, 'NaN': np.nan})\n",
    "    yes_no_cols = ['CrossBorder_Shipping', 'Urgent_Shipping', 'Installation_Service','Fragile_Equipment', 'Rural_Hospital']\n",
    "    for col in df_test.columns:\n",
    "        if col in yes_no_cols:\n",
    "             df_test[col] = df_test[col].replace({ 'YES': 'Yes', 'yes': 'Yes', 'Y': 'Yes', 'y': 'Yes','NO': 'No', 'no': 'No', 'N': 'No', 'n': 'No' })\n",
    "    df_test['Order_Placed_Date'] = pd.to_datetime(df_test['Order_Placed_Date'], errors='coerce')\n",
    "    df_test['Delivery_Date'] = pd.to_datetime(df_test['Delivery_Date'], errors='coerce')\n",
    "\n",
    "    # --- V25 Minimal Feature Engineering ---\n",
    "    # Only Delivery_Days (repaired)\n",
    "    df_test['Delivery_Days'] = (df_test['Delivery_Date'] - df_test['Order_Placed_Date']).dt.days\n",
    "    df_test['Delivery_Days'] = df_test['Delivery_Days'].abs() # Repair negatives\n",
    "\n",
    "    # NO Volume, NO Logs (except target), NO Date Parts, NO Missing Flags\n",
    "\n",
    "    # --- Ensure consistency with training columns ---\n",
    "    missing_cols = set(train_cols_expected) - set(df_test.columns)\n",
    "    if missing_cols:\n",
    "        print(f\"     ! Adding missing columns to test data: {missing_cols}\")\n",
    "        for c in missing_cols:\n",
    "            df_test[c] = np.nan # Add as NaN\n",
    "\n",
    "    # Return only the columns expected by the training pipeline\n",
    "    try:\n",
    "        # Select only the columns that X had when the pipeline was trained\n",
    "        df_test = df_test[train_cols_expected]\n",
    "        print(f\"Test data prepared. Shape: {df_test.shape}\")\n",
    "        return df_test\n",
    "    except KeyError as e:\n",
    "        print(f\"   ✗ Error: Columns mismatch: {e}\")\n",
    "        # Identify discrepancies\n",
    "        expected_set = set(train_cols_expected)\n",
    "        actual_set = set(df_test.columns)\n",
    "        print(f\"     Missing in Test (Required by X): {expected_set - actual_set}\")\n",
    "        print(f\"     Extra in Test (Not in X): {actual_set - expected_set}\")\n",
    "        return None\n",
    "\n",
    "print(\"\\n✅ V25 Test preparation function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "626b350a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 8: Generating Submission File (V25 ElasticNet Pipeline) ---\n",
      "Loaded test.csv: (500, 19)\n",
      "Preparing test features (V25 - Bare Bones)...\n",
      "Test data prepared. Shape: (500, 15)\n",
      "Getting predictions from the final V25 ElasticNet model...\n",
      "Converting predictions from log scale...\n",
      "Applying AGGRESSIVE safety clip to predictions at 269,745.26\n",
      "Creating submission DataFrame...\n",
      "\n",
      "✅ Submission file 'model.csv' saved successfully.\n",
      "Final Predictions Head:\n",
      "            Hospital_Id  Transport_Cost\n",
      "0          fffe33003400      569.086111\n",
      "1  fffe3700330036003600      510.127901\n",
      "2  fffe3300390038003400     6645.958853\n",
      "3      fffe310030003900      216.484956\n",
      "4  fffe3700330031003200     1089.063520\n"
     ]
    }
   ],
   "source": [
    "# === CELL 8: GENERATE SUBMISSION (V25 - ElasticNet) ===\n",
    "\n",
    "# Check if the final model, prepare function, and cap value exist\n",
    "if 'final_pipeline' in locals() and 'prepare_features_V25' in globals() and 'X' in locals() and 'prediction_cap_value' in locals():\n",
    "    print(\"\\n--- Cell 8: Generating Submission File (V25 ElasticNet Pipeline) ---\")\n",
    "    try:\n",
    "        # Load the raw test data\n",
    "        df_test_raw = pd.read_csv('../data/test.csv')\n",
    "        submission_ids = df_test_raw['Hospital_Id']\n",
    "        print(f\"Loaded test.csv: {df_test_raw.shape}\")\n",
    "\n",
    "        # Prepare test features using V25 function, passing training columns\n",
    "        X_test_final_raw = prepare_features_V25(df_test_raw, X.columns) # Pass training X columns\n",
    "\n",
    "        if X_test_final_raw is not None:\n",
    "            print(\"Getting predictions from the final V25 ElasticNet model...\")\n",
    "            # The final_pipeline object (trained in Cell 6) applies the V25 preprocessing\n",
    "            log_predictions = final_pipeline.predict(X_test_final_raw)\n",
    "\n",
    "            print(\"Converting predictions from log scale...\")\n",
    "            final_predictions = np.expm1(log_predictions)\n",
    "\n",
    "            # --- AGGRESSIVE Safety Clip ---\n",
    "            # Use the prediction_cap_value calculated in Cell 3 (99th percentile of original abs cost)\n",
    "            print(f\"Applying AGGRESSIVE safety clip to predictions at {prediction_cap_value:,.2f}\")\n",
    "            final_predictions = np.clip(final_predictions, 0, prediction_cap_value) # Clip hard at 99th percentile\n",
    "\n",
    "            print(\"Creating submission DataFrame...\")\n",
    "            submission_df = pd.DataFrame({\n",
    "                'Hospital_Id': submission_ids,\n",
    "                'Transport_Cost': final_predictions\n",
    "            })\n",
    "\n",
    "            output_filename = 'model.csv'\n",
    "            submission_df.to_csv(output_filename, index=False)\n",
    "            print(f\"\\n✅ Submission file '{output_filename}' saved successfully.\")\n",
    "            print(\"Final Predictions Head:\")\n",
    "            print(submission_df.head())\n",
    "\n",
    "        else:\n",
    "            print(\"   ✗ Submission generation failed: Error preparing test features.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"   ✗ Error: ../data/test.csv not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ✗ An unexpected error occurred during submission generation: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n--- Submission Generation Skipped ---\")\n",
    "    if 'final_pipeline' not in locals(): print(\"   Reason: Final V25 model not trained (Run Cell 6).\")\n",
    "    if 'prepare_features_V25' not in globals(): print(\"   Reason: 'prepare_features_V25' function not defined (Run Cell 7).\")\n",
    "    if 'X' not in locals(): print(\"   Reason: Training features 'X' not defined (Run Cell 3).\")\n",
    "    if 'prediction_cap_value' not in locals(): print(\"   Reason: 'prediction_cap_value' not defined (Run Cell 3).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
